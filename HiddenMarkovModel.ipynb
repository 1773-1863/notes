{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hidden Markov Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import pygraphviz\n",
    "import pyparsing\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from IPython.display import Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4sAAADVCAYAAAD+f2V5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYREAAAAA4MCwCAAAAABw+H/S3XMlsOzHiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1056f6a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def makeDBN(inter, intra, T, labels):\n",
    "    \"\"\"Unfold a graph for T time slices\"\"\"\n",
    "    N = max(max([i for i,j in inter]),max([j for i,j in inter]))+1\n",
    "\n",
    "    G = np.zeros((N*T,N*T))\n",
    "    pos = []\n",
    "    all_labels = []\n",
    "    for n in range(N):\n",
    "        pos.append((0,-n))\n",
    "        all_labels.append('$'+labels[n]+'_{'+str(0+1)+\"}\"+'$')\n",
    "        \n",
    "    for e in inter:\n",
    "        s,d = e\n",
    "        G[s,d] = 1\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for n in range(N):\n",
    "            pos.append((t,-n))\n",
    "            all_labels.append('$'+labels[n]+'_{'+str(t+1)+\"}\"+'$')\n",
    "\n",
    "        for e in inter:\n",
    "            s,d = e\n",
    "            s = s + N*t\n",
    "            d = d + N*t\n",
    "            G[s,d] = 1\n",
    "        \n",
    "        for e in intra:\n",
    "            s,d = e\n",
    "            s = s + N*(t-1)\n",
    "            d = d + N*t\n",
    "            G[s,d] = 1\n",
    "    return G,pos,all_labels\n",
    "\n",
    "#inter = [(0,1),(1,2),(2,3)]\n",
    "#intra = [(0,0),(1,1),(0,1),(0,2)]\n",
    "#variable_names = [\"r\",\"z\",\"x\", \"y\"] \n",
    "inter = [(0,1)]\n",
    "intra = [(0,0)]\n",
    "variable_names = [\"x\", \"y\"] \n",
    "T = 5\n",
    "\n",
    "A, pos, label_list = makeDBN(inter, intra, T, variable_names)\n",
    "\n",
    "G = nx.DiGraph(A)\n",
    "labels = {i: s for i,s in enumerate(label_list)}\n",
    "plt.figure(figsize=(12,2.5))\n",
    "nx.draw(G, pos, node_color=\"white\", node_size=2500, labels=labels, font_size=24, arrows=True)\n",
    "#nx.draw_graphviz(G,node_size=500, labels=labels, font_size=24, arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}) & = & \\sum_{x_{1:K}} p(y_{1:K}|x_{1:K}) p(x_{1:K}) \\\\\n",
    "& = &  \\underbrace{\\sum_{x_K} p(y_K | x_K ) \\sum_{x_{K-1}} p(x_K|x_{K-1})}_{\\alpha_K}  \\dots \\sum_{x_{2}} p(x_3|x_{2})\n",
    "                               \\underbrace{p(y_{2}|x_{2})\\overbrace{ \\sum_{x_{1}} p(x_2|x_{1})}^{\\alpha_{2|1}} }_{\\alpha_2}\n",
    "                                 \\underbrace{p(y_{1}|x_{1})\\overbrace{p(x_1)}^{\\alpha_{1|0}}}_{\\alpha_1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "##### Backward Pass\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}) & = &  \\sum_{x_1} p(x_1) p(y_1 | x_1 )\n",
    "%underbrace{\\sum_{x_2} p(x_2|x_{1}) p(y_2 | x_2 )}_{\\beta_1}\n",
    "\\dots\n",
    "\\underbrace{ \\sum_{x_{K-1}} p(x_{K-1}|x_{K-2}) p(y_{K-1} | x_{K-1} )}_{\\beta_{K-2}}\n",
    "\\underbrace{ \\sum_{x_K} p(x_K|x_{K-1}) p(y_K | x_K )}_{\\beta_{K-1}}\n",
    "\\underbrace{{\\pmb 1}}_{\\beta_{K}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1|0} & \\equiv & p(x_1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{k|k} & \\equiv & p(y_{1:k}, x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{k|k-1}  & \\equiv & p(y_{1:k-1}, x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "For $k=1, 2, \\dots, K$\n",
    "\n",
    "__Predict__\n",
    "\n",
    "$k=1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1|0}(x_1) = p(x_1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "$k>1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\alpha_{k|k-1}(x_k)} & = & p(y_{1:k-1}, x_k) = \\sum_{x_{k-1}} p(x_k| x_{k-1}) p(y_{1:k-1}, x_{k-1}) \\\\\n",
    "& = & \\sum_{x_{k-1}} p(x_k| x_{k-1}) { \\alpha_{k-1|k-1}(x_{k-1}) }\n",
    "\\end{eqnarray}\n",
    "\n",
    "__Update__\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\alpha_{k|k}(x_k) } & = & p(y_{1:k}, x_k) = p(y_k | x_k) p(y_{1:k-1}, x_k) \\\\\n",
    " & = & p(y_k | x_k) {\\alpha_{k|k-1}(x_k)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k+1}(x_k) & \\equiv & p(y_{k+1:K}| x_k) \\\\\n",
    "\\beta_{k|k}(x_k) & \\equiv & p(y_{k:K}| x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "For $k=K, K-1, \\dots, 1$\n",
    "\n",
    "__'Postdict'__ : (Backward Prediction)\n",
    "\n",
    "$k=K$\n",
    "\\begin{eqnarray}\n",
    "{\\beta_{K|K+1}(x_K)} & = & \\mathbf{1} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "$k<K$\n",
    "\\begin{eqnarray}\n",
    "{\\beta_{k|k+1}(x_k)} & = & p(y_{k+1:K}| x_k) = \\sum_{x_{k+1}} p(x_{k+1}| x_{k}) p(y_{k+1:K}| x_{k+1}) \\\\\n",
    "& = & \\sum_{x_{k+1}} p(x_{k+1}| x_{k}) {\\beta_{k+1|k+1}(x_{k+1}) }\n",
    "\\end{eqnarray}\n",
    "\n",
    "__Update__\n",
    "\\begin{eqnarray}\n",
    "{\\beta_{k|k}(x_k) } & = & p(y_{k:K}| x_k) = p(y_k | x_k) p(y_{k+1:K}| x_k) \\\\\n",
    " & = & p(y_k | x_k) {\\beta_{k|k+1}(x_k)}\n",
    "\\end{eqnarray}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically Stable computation of $\\log(\\sum_i \\exp (l_i ) ))$\n",
    "\n",
    "Derivation\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L & = & \\log(\\sum_i \\exp (l_i) ) \n",
    " =   \\log(\\sum_i \\exp (l_i) \\frac{\\exp(l^*)}{\\exp(l^*)} ) \\\\\n",
    "& = &  \\log( \\exp(l^*) \\sum_i \\exp (l_i - l^*) ) \\\\\n",
    "& = &  l^* + \\log( \\sum_i \\exp (l_i - l^*) )\n",
    "\\end{eqnarray}\n",
    "\n",
    "Choose $l^*  =  \\max_i l_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Naive evaluation  :', -inf)\n",
      "('Numerically stable:', array([-1000.]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_sum_exp_naive(l):\n",
    "    return np.log(np.sum(np.exp(l)))\n",
    "\n",
    "def log_sum_exp(l, axis=0):\n",
    "    l_star = np.max(l, axis=axis, keepdims=True)\n",
    "    return l_star + np.log(np.sum(np.exp(l - l_star),axis=axis,keepdims=True)) \n",
    "    \n",
    "    \n",
    "l = np.array([-1000, -10000])\n",
    "\n",
    "print('Naive evaluation  :', log_sum_exp_naive(l))\n",
    "print('Numerically stable:', log_sum_exp(l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An implementation of the forward backward algorithm\n",
    "# For numerical stability, we calculate everything in the log domain\n",
    "\n",
    "# Defined in Sampling.ipynb\n",
    "def normalize(A, axis=None):\n",
    "\n",
    "    Z = np.sum(A, axis=axis,keepdims=True)\n",
    "        \n",
    "    idx = np.where(Z == 0)\n",
    "    Z[idx] = 1\n",
    "    return A/Z\n",
    "\n",
    "def predict(A, lp):\n",
    "    lstar = np.max(lp)\n",
    "    return lstar + np.log(np.dot(A,np.exp(lp-lstar)))\n",
    "\n",
    "def postdict(A, lp):\n",
    "    lstar = np.max(lp)\n",
    "    return lstar + np.log(np.dot(np.exp(lp-lstar), A))\n",
    "\n",
    "def update(y, logB, lp):\n",
    "    return logB[y,:] + lp\n",
    "\n",
    "\n",
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "y = np.array([0, 1, 3, 2, 4])\n",
    "T = y.shape[0]\n",
    "\n",
    "logA = np.log(A)\n",
    "logB = np.log(B)\n",
    "\n",
    "# Forward Pass\n",
    "\n",
    "# Python indexes starting from zero so\n",
    "# log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "# log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "log_alpha  = np.zeros((S, T))\n",
    "log_alpha_pred = np.zeros((S, T))\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred[:,0] = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred[:,k] = predict(A, log_alpha[:,k-1])\n",
    "    \n",
    "    log_alpha[:,k] = update(y[k], logB, log_alpha_pred[:,k])\n",
    "    \n",
    "# Backward Pass\n",
    "log_beta  = np.zeros((S, T))\n",
    "log_beta_post = np.zeros((S, T))\n",
    "\n",
    "for k in range(T-1,-1,-1):\n",
    "    if k==T-1:\n",
    "        log_beta_post[:,k] = np.zeros(S)\n",
    "    else:\n",
    "        log_beta_post[:,k] = postdict(A, log_beta[:,k+1])\n",
    "    \n",
    "    log_beta[:,k] = update(y[k], logB, log_beta_post[:,k])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randgen(pr, N=1): \n",
    "    L = len(pr)\n",
    "    return np.random.choice(range(L), size=N, replace=True, p=pr)\n",
    "\n",
    "#Generate data from an HMM\n",
    "S = 3\n",
    "R = 5\n",
    "\n",
    "pi = normalize(np.random.rand(S),axis=0)\n",
    "A = normalize(np.random.rand(S,S),axis=0)\n",
    "B = normalize(np.random.rand(R,S),axis=0)\n",
    "\n",
    "# Number of steps\n",
    "T = 100\n",
    "\n",
    "x = np.zeros(T)\n",
    "y = np.zeros(T)\n",
    "\n",
    "for t in range(T):\n",
    "    if t==0:\n",
    "        x[t] = randgen(pi)\n",
    "    else:\n",
    "        x[t] = randgen(A[:,x[t-1]])\n",
    "    \n",
    "    y[t] = randgen(B[:,x[t]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing - Forward Backward Algorithm (Two filter formulation)\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}, x_k) & = & p(y_{1:k}, x_k) p(y_{k+1:K} | x_k) \\\\\n",
    "& = & {\\alpha_{k|k}(x_k) } {\\beta_{k|k+1}(x_k)} \\\\\n",
    "& \\equiv & \\gamma_k(x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Smoothing (Forward filtering - Backward smoothing), The Correction Smoother\n",
    "\n",
    "Suppose we have computed the filtered quantities $p(x_t| y_{1:t})$ via the forward pass. The forward-backward algorithm requires us to store all observations. For batch settings, this is OK however when datapoints are arriving indeed sequentially, this may be not desired.  \n",
    "\n",
    "We will derive a recursive algorithm to compute the marginals $p(x_t | y_{1:T})$.\n",
    "\n",
    "Note that if we calculate instead the so-called __pairwise__ marginal $p(x_t, x_{t+1} | y_{1:T} )$, we can get by simple marginalization\n",
    "\n",
    "\\begin{align}\n",
    "p(x_t | y_{1:T}) & =  \\sum_{x_{t+1}} p(x_t, x_{t+1} | y_{1:T} )  & \\text{Definition} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "p(x_t, x_{t+1} | y_{1:T} ) & =   p(x_{t} |x_{t+1}, y_{1:t},y_{t+1:T} ) p(x_{t+1}|y_{1:T} ) & \\text{Factorize} \\\\\n",
    "& =   p(x_{t} |x_{t+1}, y_{1:t} ) p(x_{t+1}|y_{1:T} ) & \\text{Conditional Independence}\\\\\n",
    "  & =  \\frac{p(x_{t}, x_{t+1}| y_{1:t} )}{p(x_{t+1}| y_{1:t} )} p(x_{t+1}|y_{1:T} ) & \\text{Definition of Conditional} \n",
    "\\end{align}\n",
    "\n",
    "This update has the form:\n",
    "\\begin{eqnarray}\n",
    "\\text{New Pairwise Marginal}_{t,t+1} & = & \\frac{\\text{Old Pairwise Marginal}_{t,t+1}}{\\text{Old Marginal}_{t+1}} \\times {\\text{New Marginal}_{t+1}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "The old pairwise marginal can be simply calculated from the filtered marginals as\n",
    "\n",
    "\\begin{align}\n",
    "p(x_{t}, x_{t+1}| y_{1:t} ) & =  p(x_{t+1} | x_t,  y_{1:t} ) p(x_t | y_{1:t}) & \\text{Definition} \\\\\n",
    "& =  p(x_{t+1} | x_t ) p(x_t | y_{1:t}) & \\text{Conditional Independence} \\\\\n",
    "& = \\text{Transition Model} \\times \\text{Filtering distribution} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "The correction smoother calculates a factorisation of the posterior of form\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(x_{1:T}|y_{1:T}) & = & \\frac{\\prod_{t=1}^{T-1} p(x_{t}, x_{t+1} | y_{1:T}) }{ \\prod_{t=2}^{T-1} p(x_{t} | y_{1:T}) }\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.76473315 -7.76473315 -7.76473315 -7.76473315 -7.76473315]]\n"
     ]
    }
   ],
   "source": [
    "# Smoother check\n",
    "# All numbers must be equal to the marginal likelihood p(y_{1:K})\n",
    "\n",
    "log_gamma = log_alpha + log_beta_post\n",
    "print(log_sum_exp(log_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Correction Smoother\n",
    "# For numerical stability, we calculate everything in the log domain\n",
    "log_gamma_corr = np.zeros_like(log_alpha)\n",
    "log_gamma_corr[:,T-1] = log_alpha[:,T-1]\n",
    "\n",
    "for k in range(T-2,-1,-1):\n",
    "    log_old_pairwise_marginal = log_alpha[:,k].reshape(1,S) + logA \n",
    "    log_old_marginal = predict(A, log_alpha[:,k])\n",
    "    log_new_pairwise_marginal = log_old_pairwise_marginal + log_gamma_corr[:,k+1].reshape(S,1) - log_old_marginal.reshape(S,1)\n",
    "    log_gamma_corr[:,k] = log_sum_exp(log_new_pairwise_marginal, axis=0).reshape(S)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -7.81659977 -11.43670606  -8.83277138 -11.04365384 -10.22402554]\n",
      " [-10.74985688  -8.42061397 -11.29064374 -10.11727173  -7.96630137]\n",
      " [-19.15983484  -8.55089731  -8.23171269  -7.90721448 -10.09719242]]\n",
      "[[ -7.81659977 -11.43670606  -8.83277138 -11.04365384 -10.22402554]\n",
      " [-10.74985688  -8.42061397 -11.29064374 -10.11727173  -7.96630137]\n",
      " [-19.15983484  -8.55089731  -8.23171269  -7.90721448 -10.09719242]]\n"
     ]
    }
   ],
   "source": [
    "# Verify that result coincide\n",
    "\n",
    "print(log_gamma)\n",
    "print(log_gamma_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of HMM in Python\n",
    "\n",
    "We will integrate filtering, smoothing and training functionality into an HMM object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randgen(pr, N=1): \n",
    "    L = len(pr)\n",
    "    return np.random.choice(range(L), size=N, replace=True, p=pr)\n",
    "\n",
    "def log_sum_exp(l, axis=0):\n",
    "    l_star = np.max(l, axis=axis, keepdims=True)\n",
    "    return l_star + np.log(np.sum(np.exp(l - l_star),axis=axis,keepdims=True)) \n",
    "\n",
    "def normalize_exp(log_P, axis=None):\n",
    "    a = np.max(log_P, keepdims=True, axis=axis)\n",
    "    P = normalize(np.exp(log_P - a), axis=axis)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self, pi, A, B):\n",
    "        # p(x_0)\n",
    "        self.pi = pi\n",
    "        # p(x_k|x_{k-1})\n",
    "        self.A = A\n",
    "        # p(y_k|x_{k})\n",
    "        self.B = B\n",
    "        # Number of possible latent states at each time\n",
    "        self.S = pi.shape[0]\n",
    "        # Number of possible observations at each time\n",
    "        self.R = B.shape[0]\n",
    "        self.logB = np.log(self.B)\n",
    "        self.logA = np.log(self.A)\n",
    "        self.logpi = np.log(self.pi)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_random_parameters(cls, S=3, R=5):\n",
    "        A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "        B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "        pi = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "        return cls(pi, A, B)\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = \"Prior:\\n\" + str(self.pi) + \"\\nA:\\n\" + str(self.A) + \"\\nB:\\n\" + str(self.B)\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.__str__()\n",
    "        return s\n",
    "\n",
    "    def predict(self, lp):\n",
    "        lstar = np.max(lp)\n",
    "        return lstar + np.log(np.dot(self.A,np.exp(lp-lstar)))\n",
    "\n",
    "    def postdict(self, lp):\n",
    "        lstar = np.max(lp)\n",
    "        return lstar + np.log(np.dot(np.exp(lp-lstar), self.A))\n",
    "\n",
    "    def update(self, y, lp):\n",
    "        return self.logB[y,:] + lp\n",
    "\n",
    "    def generate_sequence(self, T=10):\n",
    "    # T: Number of steps\n",
    "\n",
    "        x = np.zeros(T)\n",
    "        y = np.zeros(T)\n",
    "\n",
    "        for t in range(T):\n",
    "            if t==0:\n",
    "                x[t] = randgen(pi)\n",
    "            else:\n",
    "                x[t] = randgen(A[:,x[t-1]])    \n",
    "            y[t] = randgen(B[:,x[t]])\n",
    "    \n",
    "        return y, x\n",
    "\n",
    "    def forward(self, y):\n",
    "        T = len(y)\n",
    "        \n",
    "        # Forward Pass\n",
    "\n",
    "        # Python indexes starting from zero so\n",
    "        # log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "        # log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "        log_alpha  = np.zeros((self.S, T))\n",
    "        log_alpha_pred = np.zeros((self.S, T))\n",
    "        for k in range(T):\n",
    "            if k==0:\n",
    "                log_alpha_pred[:,0] = self.logpi\n",
    "            else:\n",
    "                log_alpha_pred[:,k] = self.predict(log_alpha[:,k-1])\n",
    "\n",
    "            log_alpha[:,k] = self.update(y[k], log_alpha_pred[:,k])\n",
    "            \n",
    "        return log_alpha, log_alpha_pred\n",
    "            \n",
    "    def backward(self, y):\n",
    "        # Backward Pass\n",
    "        T = len(y)\n",
    "        log_beta  = np.zeros((self.S, T))\n",
    "        log_beta_post = np.zeros((self.S, T))\n",
    "\n",
    "        for k in range(T-1,-1,-1):\n",
    "            if k==T-1:\n",
    "                log_beta_post[:,k] = np.zeros(self.S)\n",
    "            else:\n",
    "                log_beta_post[:,k] = self.postdict(log_beta[:,k+1])\n",
    "\n",
    "            log_beta[:,k] = self.update(y[k], log_beta_post[:,k])\n",
    "\n",
    "        return log_beta, log_beta_post\n",
    "        \n",
    "    def forward_backward_smoother(self, y):\n",
    "        log_alpha, log_alpha_pred = self.forward(y)\n",
    "        log_beta, log_beta_post = self.backward(y)\n",
    "        \n",
    "        log_gamma = log_alpha + log_beta_post\n",
    "        return log_gamma\n",
    "        \n",
    "    def correction_smoother(self, y):\n",
    "        # Correction Smoother\n",
    "\n",
    "        log_alpha, log_alpha_pred = self.forward(y)\n",
    "        T = len(y)\n",
    "        \n",
    "        # For numerical stability, we calculate everything in the log domain\n",
    "        log_gamma_corr = np.zeros_like(log_alpha)\n",
    "        log_gamma_corr[:,T-1] = log_alpha[:,T-1]\n",
    "\n",
    "        C2 = np.zeros((self.S, self.S))\n",
    "        C3 = np.zeros((self.R, self.S))\n",
    "        C3[y[-1],:] = normalize_exp(log_alpha[:,T-1])\n",
    "        for k in range(T-2,-1,-1):\n",
    "            log_old_pairwise_marginal = log_alpha[:,k].reshape(1,self.S) + self.logA \n",
    "            log_old_marginal = self.predict(log_alpha[:,k])\n",
    "            log_new_pairwise_marginal = log_old_pairwise_marginal + log_gamma_corr[:,k+1].reshape(self.S,1) - log_old_marginal.reshape(self.S,1)\n",
    "            log_gamma_corr[:,k] = log_sum_exp(log_new_pairwise_marginal, axis=0).reshape(self.S)\n",
    "            C2 += normalize_exp(log_new_pairwise_marginal)\n",
    "            C3[y[k],:] += normalize_exp(log_gamma_corr[:,k])\n",
    "        C1 = normalize_exp(log_gamma_corr[:,0])\n",
    "        return log_gamma_corr, C1, C2, C3\n",
    "    \n",
    "hm = HMM.from_random_parameters()\n",
    "\n",
    "y,x = hm.generate_sequence(10)\n",
    "\n",
    "log_alpha, log_alpha_pred = hm.forward(y)\n",
    "log_gamma = hm.forward_backward_smoother(y)\n",
    "log_gamma_corr, C1, C2, C3 = hm.correction_smoother(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-20.43376791 -19.63336678 -20.51510014 -19.66336793 -19.45966654\n",
      "  -20.38425476 -19.84295445 -19.97490005 -19.97822383 -19.96644463]\n",
      " [-19.83519429 -18.83491876 -18.74026077 -19.34093034 -18.69834129\n",
      "  -18.94575204 -18.88791988 -18.88865129 -18.87974567 -18.90386717]\n",
      " [-18.32614708 -19.07937893 -18.88881435 -18.65800128 -19.43018833\n",
      "  -18.71653402 -18.91847434 -18.86987934 -18.877589   -18.85794333]]\n",
      "[[-20.43376791 -19.63336678 -20.51510014 -19.66336793 -19.45966654\n",
      "  -20.38425476 -19.84295445 -19.97490005 -19.97822383 -19.96644463]\n",
      " [-19.83519429 -18.83491876 -18.74026077 -19.34093034 -18.69834129\n",
      "  -18.94575204 -18.88791988 -18.88865129 -18.87974567 -18.90386717]\n",
      " [-18.32614708 -19.07937893 -18.88881435 -18.65800128 -19.43018833\n",
      "  -18.71653402 -18.91847434 -18.86987934 -18.877589   -18.85794333]]\n"
     ]
    }
   ],
   "source": [
    "print(log_gamma)\n",
    "\n",
    "print(log_gamma_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating parameters of an homogeneous Markov chain\n",
    "$\\newcommand{\\ind}[1]{\\left[{#1}\\right]}$\n",
    "\n",
    "We have a Markov chain with transition probabilities $p(x_t = i| x_{t-1} = j) =  A_{i,j}$\n",
    "and initial state $p(x_1) = \\pi_i$.\n",
    "\n",
    "The distributions are\n",
    "\\begin{eqnarray}\n",
    "p(x_1 |\\pi)& = &\\prod_{s=1}^{S} \\pi_s^{\\ind{s = x_1}} \\\\\n",
    "p(x_t | x_{t-1}, A) &=& \\prod_{j=1}^{S} \\prod_{s=1}^{S}  A_{s,j}^{{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "The loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A) & = & \\log \\left( p(x_1 | \\pi) \\prod_{t=2}^T p(x_t | x_{t-1}, A) \\right) \\\\\n",
    "& = & \\sum_{s=1}^{S} {\\ind{s = x_1}} \\log \\pi_s + \\sum_{t=2}^T \\sum_{j=1}^{S}\\sum_{s=1}^{S} {{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\log A_{s,j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We have the constraints $\\sum_s \\pi_s = 1$ and $\\sum_i A_{i,j} = 1$ for all $j=1 \\dots S$ so we have $S+1$ constraints. We write the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A) & = & \\sum_{s=1}^{S} {\\ind{s = x_1}} \\log \\pi_s + \\sum_{t=2}^T \\sum_{j=1}^{S} \\sum_{s=1}^{S} {{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\log A_{s,j} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To find $\\pi$ and $A$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A)}{\\partial \\pi_i} & = & {\\ind{i = x_1}} \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)}{\\partial A_{i,j}} & = & \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{A_{i,j}} - \\lambda^A_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "Substitute the constraints $\\sum_s \\pi_s = 1$ and $\\sum_s A_{s,j} = 1, \\; j=1\\dots S$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & {\\ind{i = x_1}} \\frac{1}{\\lambda^\\pi} \\\\\n",
    "\\sum_i \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i {\\ind{i = x_1}} = 1\\\\\n",
    "\\lambda^\\pi & = & 1\\\\\n",
    "\\pi_i & = & {\\ind{i = x_1}}\n",
    "\\end{eqnarray}\n",
    "As we have effectively only a single observation for $x_1$, we have a crisp estimate.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_{t=2}^T \\sum_i  {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\\\\n",
    "A_{i,j} & = & \\frac{\\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}{\\sum_{t=2}^T \\sum_i  {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}\\\\\n",
    "& = & \\frac{\\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}{\\sum_{t=2}^T \\ind{j = x_{t-1}}}\n",
    "\\end{eqnarray}\n",
    "The result is intuitive. The denominator counts the number of times the chain visited state $j$ in the previous state. The numerator counts the number of times we visit $i$ given we were at $j$ the previous time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating parameters of an homogeneous Markov chain when several sequences are observed\n",
    "\n",
    "Suppose we have observed several sequences\n",
    "\\begin{eqnarray}\n",
    "X = \\{x_1^{(n)}, x_2^{(n)}, \\dots, x_{T_n}^{(n)}   \\}\n",
    "\\end{eqnarray}\n",
    "for $n = 1\\dots N$. Here $T_n$ is the length of the $n$'th sequence.\n",
    "\n",
    "The notation becomes slightly more complicated but conceptully the derivation is similar.\n",
    "\n",
    "The joint probability of all sequences is\n",
    "\\begin{eqnarray}\n",
    "p(X | \\pi, A) & = & \\prod_n \\left( p(x_1^{(n)}) \\prod_{t=2}^{T_n} p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right)\n",
    "\\end{eqnarray}\n",
    "The loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A) & = & \\sum_n \\left( \\log p(x_1^{(n)}) + \\sum_{t=2}^{T_n} \\log p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right) \\\\\n",
    "& = & \\sum_n \\left( \\sum_{s=1}^{S} {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_{t=2}^{T_n}\\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S} {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} \\right) \\\\\n",
    "& = & \\sum_{s=1}^{S} \\sum_n  {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_n \\sum_{t=2}^{T_n}\n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S}   {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "We write the Lagrangian\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)\n",
    "& = & \\sum_{s=1}^{S} \\sum_n  {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_n  \\sum_{t=2}^{T_n} \\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S}  {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $\\pi$ and $A$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A)}{\\partial \\pi_i} & = & \\sum_n {\\ind{i = x_1^{(n)}}} \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)}{\\partial A_{i,j}} & = &  \\sum_n \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{A_{i,j}} - \\lambda^A_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Prior\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\sum_n {\\ind{i = x_1^{(n)}}} \\frac{1}{\\lambda^\\pi}\\\\\n",
    "\\sum \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i \\sum_n {\\ind{i = x_1^{(n)}}} = 1 \\\\\n",
    "\\lambda^\\pi & = & \\sum_i \\sum_n {\\ind{i = x_1^{(n)}}} = N \\\\\n",
    "\\pi_i & = & \\frac{1}{N} \\sum_n {\\ind{i = x_1^{(n)}}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Transition Matrix\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & \\sum_n \\sum_{t=2}^{T_n}  {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i \\sum_n \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_i \\sum_n  \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\\\\n",
    "& = &  \\sum_n \\sum_{t=2}^{T_n}  \\ind{j = x_{t-1}^{(n)}} \\\\\n",
    "A_{i,j} & = &  \\frac{\\sum_n \\sum_{t=2}^{T_n}  {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}}}{\\sum_n \\sum_{t=2}^{T_n}  {\\ind{j = x_{t-1}^{(n)}}}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The EM Algorithm\n",
    "\n",
    "$\\newcommand{\\E}[1]{\\left\\langle#1\\right\\rangle}$\n",
    "\n",
    "The EM algorithm is a standart approach for ML estimation, when we have hidden variables.\n",
    "The canonical model is $p(y, x| \\theta)$ where we observe only $y$.\n",
    "\n",
    "The observed data loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & = & \\log p(y| \\theta) = \\log \\sum_x p(y, x| \\theta)\n",
    "\\end{eqnarray}\n",
    "\n",
    "The key to the EM algorithm is the Jensen's inequality, that states for a concave function $f$ we have for $0 \\leq \\lambda \\leq 1$\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f(\\lambda x_1 + (1 - \\lambda) x_2) \\geq \\lambda f( x_1) + (1 - \\lambda) f(x_2)\n",
    "\\end{eqnarray}\n",
    "\n",
    "In words the value of a function evaluated at the convex combination (lhs) is always equal and larger then the convex combination of the function values. As mathematical expectation\n",
    "\\begin{eqnarray}\n",
    "\\E{f(x)} & = & \\sum_x p(x) f(x) \\\\\n",
    "\\sum_x p(x) & = & 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "As $\\log(x)$  is a concave function, we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    " f(\\E{x}) & \\geq & \\E{f(x)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The key idea of the EM algorithm is to lower bound the observed data likelihood an maximize the bound with respect to the parameters. We take any distribution $q(x)$\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & = & \\log \\sum_x p(y, x| \\theta) \\\\\n",
    "& = & \\log \\sum_x p(y, x| \\theta) \\frac{q(x)}{q(x)} \\\\\n",
    "& = & \\log  \\E{\\frac{p(y, x| \\theta)}{q(x)}}_{q(x)}\\\\\n",
    "& \\geq & \\E{\\log {p(y, x| \\theta)}}_{q(x)} -\\E{\\log{q(x)} }_{q(x)}\\\\\n",
    "\\end{eqnarray}\n",
    "For _any_ $q(x)$, we have a lower bound. The natural strategy here is to choose the $q(x)$ that will maximise the lower bound. This is an optimisation problem. To make the notation more familiar, we let $q(x = i) = q_i$. Then, we arrive at the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(q, \\lambda) & = & \\sum_i q_i \\log p(y, x=i| \\theta) - \\sum_i q_i \\log q_i \\\\\n",
    "& & + \\lambda (1 - \\sum_i q_i)\n",
    "\\end{eqnarray}\n",
    "We take the derivative with respect to $q_i$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(q, \\lambda)}{\\partial q_k} & = & \\log p(y, x=k| \\theta) - (\\log q_k + 1) - \\lambda = 0\\\\\n",
    "\\log q_k  & = & \\log p(y, x=k| \\theta) -1 - \\lambda \\\\\n",
    "q_k  & = & p(y, x=k| \\theta) \\exp(-1 - \\lambda) \\\\\n",
    "\\sum_k q_k & = & \\exp(-1 - \\lambda) \\sum_k p(y, x=k| \\theta) = 1\\\\\n",
    "\\exp(1 + \\lambda) & = & p(y | \\theta) \\\\\n",
    "\\exp(-1 - \\lambda) & = & 1/p(y | \\theta) \\\\\n",
    "\\end{eqnarray}\n",
    "hence we have\n",
    "\\begin{eqnarray}\n",
    "q_k  & = & p(y, x=k| \\theta)/p(y | \\theta) = p(x=k| y \\theta)\n",
    "\\end{eqnarray}\n",
    "This result shows that the best we can do is to choose the posterior distribution\n",
    "\\begin{eqnarray}\n",
    "q(x) & = & p(x| y, \\theta)\n",
    "\\end{eqnarray}\n",
    "The EM algorithm is an iterative algorithm that exploits this bound result. Given a particular parameter setting $\\theta^{(\\tau)}$ at iteration $\\tau$, we can compute a lower bound of the true likelihood function.\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & \\geq & \\E{\\log {p(y, x| \\theta)}}_{p(x| y, \\theta^{(\\tau)})} -\\E{\\log p(x| y, \\theta^{(\\tau)}) }_{p(x| y, \\theta^{(\\tau)})}\\\\\n",
    "& \\equiv & {\\cal F}[\\theta; \\theta^{(\\tau)}] +  H[p(x| y, \\theta^{(\\tau)})]\n",
    "\\end{eqnarray}\n",
    "We need to show that\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta^{(\\tau)}) & = & \\E{\\log {p(y, x| \\theta)}}_{p(x| y, \\theta^{(\\tau)})} -\\E{\\log p(x| y, \\theta^{(\\tau)}) }_{p(x| y, \\theta^{(\\tau)})}\\\\\n",
    "& = & {\\cal F}[\\theta^{(\\tau)}; \\theta^{(\\tau)}] +  H[p(x| y, \\theta^{(\\tau)})]\n",
    "\\end{eqnarray}\n",
    "In other words, the bound is tight at $\\theta^{(\\tau)}$, hence maximizing the bound guarantees maximization of the true loglikelihoood.\n",
    "\n",
    "In most cases, where the EM algorithm can be applied, the joint distribution is from an {\\it exponential family}, i.e., it has the generic algebraic form\n",
    "\\begin{eqnarray}\n",
    "p(y, x| \\theta) & = & b(y, x)\\exp( \\sum_l \\phi_l(y, x) \\psi(\\theta_l) - A(\\theta)   )\n",
    "\\end{eqnarray}\n",
    "where $\\phi_l$ are the sufficient statistics and $\\psi(\\theta_l)$ are the {\\it canonical} parameters. The canonical parameters are in one to one relation with a conventional parametrisation. We will give several explicit examples when we cover the HMM's of the next section.\n",
    "\n",
    "In the case when the complete data likelihood is an exponential family, the computation of the bound requires the expectation\n",
    "\\begin{eqnarray}\n",
    "\\E{\\log p(y, x| \\theta)} & = & \\E{\\log b(x, y)} + \\sum_l \\E{\\phi_l(y, x)} \\psi(\\theta_l) - A(\\theta)\n",
    "\\end{eqnarray}\n",
    "where the expectation is taken with respect to the posterior $p(x|y, \\theta^{(\\tau)})$. In other words, we need to compute expectations of form $\\E{\\phi_l(y, x)}$. Once these are available, we have effectively an expression for ${\\cal F}(\\theta; \\theta^{(\\tau)})$. By maximisation of ${\\cal F}$ with respect to $\\theta$,  and arrive at $\\theta^{(\\tau + 1)}$ and complete the iteration.\n",
    "\n",
    "In a rather abstract sense, the EM algorithm proceeds as follows:\n",
    "\n",
    "##### The Expectation/Maximization (EM) algorithm.\n",
    "\n",
    "\\begin{algorithmic}\n",
    "\\STATE Initialise $\\theta^{(0)}$\n",
    "\\FOR{  $\\text{epoch} = 1 \\dots $  MAXITER}\n",
    "\\STATE E-step. Compute the sufficient statistics of the complete data likelihood\n",
    "\\STATE M-step. Maximize with respect to the parameters $\\theta$ to find $\\theta^{(\\tau + 1)}$\n",
    "\\ENDFOR\n",
    "\\end{algorithmic}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning HMM parameters by EM\n",
    "\n",
    "Suppose we have observed several sequences\n",
    "\\begin{eqnarray}\n",
    "Y = \\{y_1^{(n)}, y_2^{(n)}, \\dots, y_{T_n}^{(n)}   \\}\n",
    "\\end{eqnarray}\n",
    "for $n = 1\\dots N$. Here $T_n$ is the length of the $n$'th sequence.\n",
    "Let $Y \\in \\{1,\\dots, R\\}$ and $X \\in \\{1,\\dots, S \\}$.\n",
    "\n",
    "The discrete observation, discrete state space HMM has the following factors:\n",
    "\\begin{eqnarray}\n",
    "p(x_1 = i) & = & \\pi_i \\\\\n",
    "p(y_k = r| x_k = i) & = & B_{r,i} \\\\\n",
    "p(x_k = i| x_{k-1} = j) & = & A_{i,j} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The joint probability of all observed sequences and corresponding hidden sequences are\n",
    "\\begin{eqnarray}\n",
    "p(Y, X | \\pi, A, B) & = & \\prod_n \\left( p(x_1^{(n)}) p(y_1^{(n)} | x_1^{(n)})  \\prod_{t=2}^{T_n} p(y_t^{(n)} | x_t^{(n)}) p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right)\n",
    "\\end{eqnarray}\n",
    "The expected complete data loglikelihood is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A, B) & = & \\E{\\sum_n \\left( \\log p(x_1^{(n)}) + \\sum_{t=1}^{T_n} \\log p(x_t^{(n)}| x_{t-1}^{(n)} ) + \\sum_{t=2}^{T_n} \\log p(y_t^{(n)} | x_t^{(n)}) \\right)} \\\\\n",
    "& = & \\E{\\sum_n \\left( \\sum_{s=1}^{S} {\\ind{s = x_1^{(n)}}} \\log \\pi_s   + \\log \\pi_s \\sum_{t=2}^{T_n}\\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S} {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} + \\sum_{t=1}^{T_n}\\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} {\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}} \\log B_{r,i}\\right)} \\\\\n",
    "& = & \\sum_n \\sum_{s=1}^{S}   \\E{{\\ind{s = x_1^{(n)}}}} \\log \\pi_s  + \\sum_n \\sum_{t=2}^{T_n}\n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S}   \\E{{\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\log A_{s,j} + \\sum_n \\sum_{t=1}^{T_n}\\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} \\E{{\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}}} \\log B_{r,i} \\\\\n",
    "& = & \\sum_{s=1}^{S} \\underbrace{\\left( \\sum_n \\E{ {\\ind{s = x_1^{(n)}}}} \\right)}_{\\equiv C_1} \\log \\pi_s  + \n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S} \\underbrace{\\left(  \\sum_n \\sum_{t=2}^{T_n} \\E{{\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\right)}_{\\equiv C_2} \\log A_{s,j} + \\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} \\underbrace{\\left(  \\sum_n \\sum_{t=1}^{T_n} \\E{{\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}}} \\right)}_{\\equiv C_3} \\log B_{r,i} \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The M-Step\n",
    "We write the Lagrangian to ensure that the columns of $A$ and $B$ are positive and normalized\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A, \\lambda^B)\n",
    "& = & \\sum_{s=1}^{S} C_1(s) \\log \\pi_s + \\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S}  C_2(s,j) \\log A_{s,j} + \\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} C_3(r,i) \\log B_{r,i} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "+ \\sum_i \\lambda^B_i \\left( 1 - \\sum_r B_{r,i} \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To find $\\pi$, $A$ and $B$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial \\pi_i} & = & C_1(i) \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial A_{i,j}} & = &  C_2(i,j) \\frac{1}{A_{i,j}} - \\lambda^A_j = 0 \\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, B, \\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial B_{k,i}} & = &  C_3(k,i) \\frac{1}{B_{k,i}} - \\lambda^B_i = 0 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Prior\n",
    "We set the derivative to zero and solve for $\\pi$\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & C_1(i) \\frac{1}{\\lambda^\\pi}\n",
    "\\end{eqnarray}\n",
    "As we have the normalization constraint for $\\pi$, we also have the following equality from which we can solve for the Lagrange multiplier:\n",
    "\\begin{eqnarray}\n",
    "\\sum_i \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i C_1(i) = 1 \\\\\n",
    "\\lambda^\\pi & = & \\sum_i  C_1(i)  = N \n",
    "\\end{eqnarray}\n",
    "Substituting, we obtain the intuitive answer:\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\frac{1}{N} C_1(i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Transition Matrix\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & C_2(i,j) \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i C_2(i,j) \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_i C_2(i,j) \\\\\n",
    "A_{i,j} & = &  \\frac{C_2(i,j)}{\\sum_i C_2(i,j)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Observation Matrix\n",
    "\\begin{eqnarray}\n",
    "B_{k,i} & = & C_3(k,i) \\frac{1}{\\lambda^B_i} \\\\\n",
    "\\sum_k B_{k,i} & = & \\sum_k C_3(k,i) \\frac{1}{\\lambda^B_i} = 1 \\\\\n",
    "\\lambda^B_i & = & \\sum_k C_3(k,i) \\\\\n",
    "B_{k,i} & = &  \\frac{C_3(k,i)}{\\sum_k C_3(k,i)}\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "y = np.array([0, 1, 3, 2, 4])\n",
    "\n",
    "hm = HMM(p, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward smoothers\n",
    "\n",
    "The EM algorithm requires obtaining the sufficient statistics of an HMM.  \n",
    "\n",
    "The key observation is that the sufficient statistics are additive:\n",
    "\\begin{eqnarray}\n",
    "C_t & = & \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k}) \\right) p(x_{1:t}|y_{1:t}) dx_{1:t}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We will use this observation as the basis of a forward recursion. First we decompose the posterior \n",
    "as a product of the filtering density at time $t$ and a conditional quantity familiar from the correction smoother \n",
    "\\begin{eqnarray}\n",
    "C_t & = & \\int \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k})\\right) p(x_{1:t-1}|y_{1:t},x_t) p(x_{t}|y_{1:t}) dx_{1:t-1} dx_t \\\\\n",
    "& = & \\int \\underbrace{\\left( \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k})\\right) p(x_{1:t-1}|y_{1:t-1},x_t) dx_{1:t-1} \\right)}_{=V_t(x_t)} p(x_{t}|y_{1:t})  dx_t\n",
    "\\end{eqnarray}\n",
    "\n",
    "Due to additivity, we can decompose further\n",
    "\\begin{eqnarray}\n",
    "V_t(x_t) & = & \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-1}|y_{1:t-1}, x_t) dx_{1:t-1} \\\\\n",
    "& = & \\int \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-2}|y_{1:t-1}, x_{t-1}, x_t) p(x_{t-1}|y_{1:t-1}, x_t) dx_{1:t-2} dx_{t-1} \\\\\n",
    "& = & \\int \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-2}|y_{1:t-2}, x_{t-1}) p(x_{t-1}|y_{1:t-1}, x_t) dx_{1:t-2} dx_{t-1} \\\\\n",
    "& = & \\int \\left( s_t(x_{t-1}, x_{t}) + \\int \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k}) p(x_{1:t-2}|y_{1:t-2}, x_{t-1}) dx_{1:t-2}  \\right)  p(x_{t-1}|y_{1:t-1}, x_t)  dx_{t-1} \\\\\n",
    "& = & \\int \\left( s_t(x_{t-1}, x_{t}) + V_{t-1}(x_{t-1})  \\right)  p(x_{t-1}|y_{1:t-1}, x_t)  dx_{t-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "V_1(x_1) & = & 0 \\\\\n",
    "V_2(x_2) & = & \\int  s_2(x_{1}, x_{2})   p(x_{1}|y_{1}, x_2)  dx_{1} \\\\\n",
    "V_3(x_3) & = & \\int \\left( s_3(x_{2}, x_{3}) + V_{2}(x_{2})  \\right)  p(x_{2}|y_{1:2}, x_3)  dx_{2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above recursion when applied to the sufficient statistics of an HMM has the following specific form:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "C_2(a,b) &=& \\sum_{x_4} \\sum_{x_3} \\sum_{x_2} \\left(\\sum_{x_1} {\\ind{a = x_2}}\\ind{b = x_{1}} p(x_1|y_1, x_2) + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_2|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4})\\\\\n",
    "&=& \\sum_{x_4}\\sum_{x_3} \\sum_{x_2} \\left( \\underbrace{{\\ind{a = x_2}} p(x_1=b|y_1, x_2)}_{V_2(x_2)} + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_2|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "&=&  \\sum_{x_4}\\sum_{x_3} \\left( \\underbrace{p(x_1=b|y_1, x_2=a) p(x_2=a|y_{1,2}, x_3) + {\\ind{a = x_3}} p(x_2=b|y_{1,2}, x_3)}_{V_3(x_3)} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "&=&  \\sum_{x_4} \\left(p(x_1=b|y_1, x_2=a)\\sum_{x_3} p(x_2=a|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) + p(x_2=b|y_{1,2}, x_3=a) p(x_3=a|y_{1,3}, x_4) \\\\\n",
    "+ {\\ind{a = x_4}} p(x_3=b |y_{1,3}, x_4) \\right)  p(x_4|y_{1,4}) \\\\\n",
    "&=& p(x_1=b|y_1, x_2=a)\\sum_{x_3} p(x_2=a|y_{1,2}, x_3) \\sum_{x_4} p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "& & + p(x_2=b|y_{1,2}, x_3=a) \\sum_{x_4} p(x_3=a|y_{1,3}, x_4)p(x_4|y_{1,4}) \\\\\n",
    "& & + p(x_3=b |y_{1,3}, x_4=a) p(x_4=a|y_{1,4})  \n",
    "\\end{eqnarray}\n",
    "\n",
    "One can verify, that the last line is indeed equal to the required sufficient statistics given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_2(a,b) & = &  \\sum_{x_{1:T}} \\sum_{t=2}^T \\ind{a = x_t}\\ind{b = x_{t-1}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = & \\sum_{x_{2:T}} \\left(\\sum_{x_{1}} \\ind{a = x_2}\\ind{b = x_{1}} p(x_{1}|y_{1},x_{2}) + \\sum_{t=3}^T \\ind{a = x_t}\\ind{b = x_{t-1}}  \\right) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "V_{2,2}(a, b, x_2) & = & \\ind{a = x_2} p(x_{1} = b |y_{1},x_{2}) \\\\\n",
    "C_2(a,b) & = & \\sum_{x_{3:T}} \\left( \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\sum_{x_2} \\ind{a = x_3}\\ind{b = x_{2}} p(x_{2}|y_{1:2},x_{3})  + \\sum_{t=4}^T \\ind{a = x_t}\\ind{b = x_{t-1}}  \\right) \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "V_{2,3}(a, b, x_3) & = & \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\sum_{x_2} \\ind{a = x_3}\\ind{b = x_{2}} p(x_{2}|y_{1:2},x_{3}) \\\\\n",
    "& = & \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = x_3} p(x_{2}=b|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update rule is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "V_{2,t}(a, b, x_t) & = & \\sum_{x_{t-1}} V_{2,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\sum_{x_{t-1}} \\ind{a = x_t}\\ind{b = x_{t-1}} p(x_{t-1}|y_{1:t-1},x_{t}) \\\\\n",
    "& = & \\sum_{x_{t-1}} V_{2,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\ind{a = x_t} p(x_{t-1}=b|y_{1:t-1},x_{t})\n",
    "\\end{eqnarray}\n",
    "\n",
    "The advantage of this algorithm is that it is entirely forward and has attractive space properties, requiring only $S^3 + 2S$ space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_3(a,b) & = &  \\sum_{x_{1:T}} \\sum_{t=1}^T \\ind{a = y_t}\\ind{b = x_{t}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = &  \\sum_{x_{2:T}} \\left(\\ind{a = y_1} p(x_{1}=b|y_{1},x_{2}) + \\sum_{t=2}^T \\ind{a = y_t}\\ind{b = x_{t}} \\right) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{3,2}(a, b, x_2)  & = & \\ind{a = y_1} p(x_{1}=b|y_{1},x_{2}) \\\\\n",
    "C_3(a,b) & = &  \\sum_{x_{3:T}} \\left(\\sum_{x_2} V_{3,2}(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = y_2} p(x_{2}=b|y_{1:2},x_{3}) + \\sum_{t=3}^T \\ind{a = y_t}\\ind{b = x_{t}} \\right) \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{3,3}(a, b, x_3)  & = & \\sum_{x_2} V_{3,2}(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = y_2} p(x_{2}=b|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update rule is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "V_{3,t}(a, b, x_t) & = & \\sum_{x_{t-1}} V_{3,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\ind{a = y_t} p(x_{t-1}=b|y_{1:t-1},x_{t})\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_1(a) & = &  \\sum_{x_{1:T}} \\ind{a = x_{1}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = &  \\sum_{x_{2:T}} p(x_{1}=a|y_{1},x_{2}) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{1,2}(a, x_2)  & = & p(x_{1}=a|y_{1},x_{2}) \\\\\n",
    "C_1(a) & = &  \\sum_{x_{3:T}} \\sum_{x_2} V_{1,2}(a, x_2) p(x_{2}|y_{1:2},x_{3})  \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{1,3}(a, x_3)  & = & \\sum_{x_2} V_{1,2}(a, x_2) p(x_{2}|y_{1:2},x_{3})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " An implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55766819]\n",
      " [ 0.41471831]\n",
      " [ 0.0276135 ]]\n",
      "1.0\n",
      "[[ 0.25945937  0.00608506  0.6788905 ]\n",
      " [ 0.61402584  3.54249044  3.36225566]\n",
      " [ 0.27474163  3.89013632  0.37191517]]\n",
      "13.0\n",
      "[[ 0.06156212  1.20205124  1.73638664]\n",
      " [ 1.06898349  2.33689306  0.59412344]\n",
      " [ 0.01327825  1.00608493  1.98063682]\n",
      " [ 0.00440298  2.89368259  0.10191443]\n",
      " [ 0.35387629  0.49477843  0.15134528]]\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "logA = np.log(A)\n",
    "logB = np.log(B)\n",
    "\n",
    "hm = HMM(p, A, B)\n",
    "\n",
    "y = np.array([1, 1, 1, 3, 2, 0, 3, 2, 1, 0, 0, 3, 2, 4])\n",
    "T = y.shape[0]\n",
    "\n",
    "# Forward only estimation of sufficient statistics\n",
    "V1  = np.eye((S))\n",
    "V2  = np.zeros((S,S,S))\n",
    "V3  = np.zeros((R,S,S))\n",
    "I_S1S = np.eye(S).reshape((S,1,S))\n",
    "I_RR = np.eye(R)\n",
    "\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred = predict(A, log_alpha)\n",
    "    \n",
    "    if k>0:\n",
    "        # Calculate p(x_{k-1}|y_{1:k-1}, x_k) \n",
    "        lp = np.log(normalize_exp(log_alpha)).reshape(S,1) + logA.T    \n",
    "        P = normalize_exp(lp, axis=0)\n",
    "        \n",
    "        # Update\n",
    "        V1 = np.dot(V1, P)             \n",
    "        V2 = np.dot(V2, P) + I_S1S*P.reshape((1,S,S))    \n",
    "        V3 = np.dot(V3, P) + I_RR[:,y[k-1]].reshape((R,1,1))*P.reshape((1,S,S))    \n",
    "        \n",
    "    log_alpha = update(y[k], logB, log_alpha_pred)    \n",
    "    p_xT = normalize_exp(log_alpha)    \n",
    "    \n",
    "C1 = np.dot(V1, p_xT.reshape(S,1))\n",
    "C2 = np.dot(V2, p_xT.reshape(1,S,1)).reshape((S,S))\n",
    "C3 = np.dot(V3, p_xT.reshape(1,S,1)).reshape((R,S))\n",
    "C3[y[-1],:] +=  p_xT\n",
    "    \n",
    "\n",
    "print(C1)\n",
    "print(np.sum(C1))\n",
    "\n",
    "print(C2)\n",
    "print(np.sum(C2))\n",
    "\n",
    "print(C3)\n",
    "print(np.sum(C3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "V_1(x_1) & = & 0 \\\\\n",
    "V_2(x_2) & = & \\int  s_2(x_{1}, x_{2})   p(x_{1}|y_{1}, x_2)  dx_{1} \\\\\n",
    "V_3(x_3) & = & \\int \\left( s_3(x_{2}, x_{3}) + V_{2}(x_{2})  \\right)  p(x_{2}|y_{1:2}, x_3)  dx_{2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.55766819  0.41471831  0.0276135 ]\n",
      "1.0\n",
      "[[ 0.25945937  0.00608506  0.6788905 ]\n",
      " [ 0.61402584  3.54249044  3.36225566]\n",
      " [ 0.27474163  3.89013632  0.37191517]]\n",
      "13.0\n",
      "[[ 0.06156212  1.20205124  1.73638664]\n",
      " [ 1.06898349  2.33689306  0.59412344]\n",
      " [ 0.01327825  1.00608493  1.98063682]\n",
      " [ 0.00440298  2.89368259  0.10191443]\n",
      " [ 0.35387629  0.49477843  0.15134528]]\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "lg, C1_corr, C2_corr, C3_corr = hm.correction_smoother(y)\n",
    "\n",
    "print(C1_corr)\n",
    "print(np.sum(C1_corr))\n",
    "\n",
    "print(C2_corr)\n",
    "print(np.sum(C2_corr))\n",
    "\n",
    "print(C3_corr)\n",
    "print(np.sum(C3_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"stdin_port\": 52422, \n",
      "  \"ip\": \"127.0.0.1\", \n",
      "  \"control_port\": 52423, \n",
      "  \"hb_port\": 52424, \n",
      "  \"signature_scheme\": \"hmac-sha256\", \n",
      "  \"key\": \"b381430f-be59-444c-beb0-e75c64b6c970\", \n",
      "  \"shell_port\": 52420, \n",
      "  \"transport\": \"tcp\", \n",
      "  \"iopub_port\": 52421\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> ipython <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> ipython <app> --existing kernel-8739c958-47db-4b42-a280-a39582b02bdd.json \n",
      "or even just:\n",
      "    $> ipython <app> --existing \n",
      "if this is the most recent IPython session you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
