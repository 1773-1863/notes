{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hidden Markov Models\n",
    "### Ali Taylan Cemgil, Bogazici University\n",
    "\n",
    "The latex equations on the Github version do not render well. Please work on a clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import pygraphviz\n",
    "import pyparsing\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from IPython.display import Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4sAAADVCAYAAAD+f2V5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X9MVXee//HXRbGC0EBFM0g7THWYGUREB5vdCSo4Q5Fu\nt7NNHVdGsmmwa7bWTlZsN61NVGptQdNaO+sM225XnBmqpWhmsq6GndJo9WKTKln5YX9shLVVKiPX\nxkK5KFQ/3z+MfKWnWryfyz336vORmMiVez4fk2fOvW/uvQePMcYIAAAAAICrRLm9AQAAAABA+GFY\nBAAAAAA4MCwCAAAAABwYFgEAAAAADgyLAAAAAAAHhkUAAAAAgAPDIgAAAADAgWERAAAAAODAsAgA\nAAAAcGBYBAAAAAA4MCwCAAAAABxGu70B4Eb5/X4dPXpULS0t6unp0fnz5yVJY8eOVXx8vDIzMzVj\nxgzFxsa6vFOEI/qBDfqBDfqBDfqBGzzGGOP2JoDr6enpUU1NjQ4ePKjGxka1t7dr6tSpysrKUkJC\ngmJiYiRJfX19OnfunJqamvTBBx9o8uTJys7O1pw5c7Ro0SLFx8e7/D+BG+gHNugHNugHNugH4YBh\nEWGrtbVVlZWV2rFjh+bNm6fCwkJlZ2dr2rRpGjNmzHXv29/fr9bWVjU2Nqqurk779u3T4sWLtWzZ\nMmVkZITofwA30Q9s0A9s0A9s0A/CigHCjNfrNXPnzjWTJk0ya9euNadOnbI+5qlTp8zatWtNcnKy\nyc3NNV6vNwg7RTiiH9igH9igH9igH4QjhkWEjd7eXlNaWmqSk5NNdXW16e/vD/oa/f39prq62iQn\nJ5vS0lLT29sb9DXgDvqBDfqBDfqBDfpBOONqqAgLDQ0NmjFjhjo7O9XS0qLi4mJFR0cHfZ3o6GgV\nFxerublZp0+f1owZM9TQ0BD0dRBa9AMb9AMb9AMb9IOw5/a0ClRUVJjk5GSza9eukK+9a9cu853v\nfMdUVFSEfG0EB/3ABv3ABv3ABv0gEnCBG7jGGKNVq1Zp9+7d+vOf/6yUlBRX9tHR0aGCggI98MAD\nKi8vl8fjcWUfuDH0Axv0Axv0Axv0g0jC71mEa1atWqX6+nodOHBA48ePd20fKSkpevfdd1VYWCiP\nx6Py8nLX9oLhox/YoB/YoB/YoB9EEoZFuGLDhg3avXu36yfKK5KSklRXV6fc3FwlJCToqaeecntL\nuA76gQ36gQ36gQ36QaThbagIOa/Xq4ULF+rIkSOuvfXiWjo6OjRr1izt3LlTOTk5bm8H34B+YIN+\nYIN+YIN+EIm4GipCyu/3a8mSJfrNb34TdidK6fJbMrZs2aKSkhL5/X63t4OvoR/YoB/YoB/YoB9E\nKl5ZREitXLlSp0+f1o4dO9zeynUVFRUpJSVFL730kttbwVXoBzboBzboBzboB5GKYREh09DQoIUL\nF6q5uVlJSUlub+e6fD6fMjMzeTtGGKEf2KAf2KAf2KAfRDKGRYRMXl6eli5dquLiYre3MizV1dV6\n/fXXtX//fre3AtEP7NAPbNAPbNAPIhnDIkKitbVV8+fP14kTJxQdHe32doZlYGBAqampevvtt5WR\nkeH2dm5p9AMb9AMb9AMb9INIxwVuEBKVlZVaunRpxJwoJSk6OlpLly5VZWWl21u55dEPbNAPbNAP\nbNAPIh2vLGLE9fT0KDU1VS0tLWF5BbDrOXXqlKZPn65PPvlE8fHxbm/nlkQ/sEE/sEE/sEE/uBnw\nyiJGXE1NjebNm2d1ovz973+vn/70p/rJT36iv/u7v1NXV5c+/fRTPfDAA8rPz9eCBQv02WefBXHX\nl915553Ky8tTTU1N0I+N4aEf2AhGP26hH/dx/oENzj+4GTAsYsQdPHhQhYWFAd9/06ZNOn78uOrr\n6/Xee+9p9OjRWrRokR577DG99tprevbZZ9XQ0KCNGzcGcdf/X2Fhobxe74gcG9+OfmDDth/JvSf7\nEv24jfMPbHD+wc2AYREjrrGxUdnZ2QHd9//+7/90+PBhrVu3TlFRl3PNyMjQ/v37tXDhQiUnJ6u2\ntlZdXV2aMWNGMLc9KDs7W42NjSNybHw7+oENm34k95/s04+7OP/ABucf3BQMMIK+/PJLExMTYy5c\nuBDQ/Z999lnT1NQ05LaHHnrIjB492nzxxRfGGGP6+vpMc3Oz9V6v5fz58yYmJsb09vaO2Br4ZvQD\nG7b9tLe3m6KioiG3rV692ng8HrNt2zZjjDH//M//bKKiokxVVZXtdr8R/biH8w9scP7BzYJhESOq\noaHBZGdnB3z/S5cuOb6eMGGCueeee2y3dkN+/OMfm0OHDoV0TdAP7Nj2Ew5P9o2hH7dw/oENzj+4\nWfA2VIyolpYWZWVlBXx/j8cz5Ovm5mb5fD7NmzfvW+978OBBTZ06NeC1r5aVlaWWlpagHAvD50Y/\nfX19evbZZ7V8+XLl5eWpqKhIn376acB7kOjHLbb9rF69WtOnTx/82hijgwcPaubMmbr99tslSWPH\njlVmZqb1Xq+Hftzh5uPXFY888ogOHToU8B4k+nEL5x/cLEa7vQHc3Hp6epSQkBC0473zzjuSdN0H\n2+3bt6u+vl49PT366KOPgrJuQkKCenp6gnIsOJ9EXc/jjz8etHWH089zzz2nZcuW6a677pIklZSU\nKCcnR01NTbrjjjsCWpd+gitU/VzryX5JSck17+P3+7V161adOXNGfr9fbW1tev75561+cEU/wRXO\n55+r1dfXq6qqSg8//LDVuvQTXOF8/vnDH/6gl19+WbNmzVJcXJw6OjoUExOjbdu2BbwP+gGvLGJE\nnT9/XjExMQHfv7Ozc8irOu+8845GjRql2bNnD/m+n//854N/X7x4sbZu3ar7778/4HW/LiYmRn19\nfUE7HobP5vc73Wg/58+f169//Wtt3bp18N+eeeYZdXR0qKqqKuB90I97gvn7wYbzZP/pp5/W3r17\ntW7dOr344otKTU3VvHnz1N3dHfC69OOeUJ5/rub3+7Vv376A174a/bgn1Oefixcvyufz6Y033tCf\n/vQnpaen67XXXrNal37AsIiw9fnnn2vq1KmaOXOmJOmLL77Qvn37dNdddykuLm7w+2prazVnzpwR\n3cvFixe1evVqeTwe/gThTygE0s/FixeVlJQ05IHxu9/9riSpra0t4L3QT+T1IwX2ZH/UqFE6c+bM\n4Nc//OEP1dXVpY8//jjgfdBP5PVj+/j1yiuvaMWKFUHZC/1EXj9SYOcfj8ej6upq9fb2qr29XWVl\nZRozZkzI9oybE8MiRtTYsWMD/onUiRMn1N3draVLl+rSpUtauXKllixZor/85S/y+XySpH379mnb\ntm164okngrlth/7+fr344osyly8KxR/LPzci0Le/BNLPuHHjdOLECVVUVAwep729XZI0efLkgPYh\n0U8k9hPok/2XX35ZR44cGfz6+PHjiouLU3p6ekD7kOgnEvuxefw6cuSIvve972nChAkBrf119BN5\n/dj8sOFG9/ht+vr6rN4hhsjHZxYxouLj4/Xhhx8GdN8f//jHeuaZZ+T1epWXl6fHH39cf//3f687\n77xTP/vZzxQbG6tp06bpzTffHPwdViPl3Llz+tGPfjSia9xKhvtg9uqrr+r9998PaI1g9bN9+3ZN\nnDhRjzzySED7kOgn2ELRz5Un+08++eSQJ/tVVVXy+XxKSkoafLK/e/fubzxGd3e3amtr9eqrrw55\ngnej6Ce4wvn889VXX6m2tlYbNmwIaN1vQj/BFe7nn/fee0979+5VXFycPvzwQ1VUVAy+QyYQ9AMZ\nYATZXjraRlVVlfF4PEE5FpeOdoeb/RhjzCeffGLGjx9v9uzZY3Uc+nGHbT+rV6828+bNM3PmzDE1\nNTXGGGPKy8vN9OnTzV//9V+bf/zHfzTd3d2O+/X19ZmKigrz4IMPmo0bNzp+hcKNoh93uHH+eeWV\nV8wnn3wy+LXH4zHvvvuu1THpxx1unH+2bdtm/uVf/mXw69raWvOjH/3I9Pf3B7wP+oHHmCC/Xg1c\nxe/3KykpSefOnQv5++a3bdumJUuW6NKlS1bHuXDhghITE+Xz+RQbGxuk3WE43OxnYGBA9913n5Yt\nW6YFCxYEfBz6cY+b/VxRWlqq//mf/1FdXZ3Gjh17w/enH/eEup+PP/5Y+/fv1z/90z8N3hYVFaV9\n+/YpNzc3oGPSj3vcOP+cOXNGiYmJio6OliT19vYqPj5e27dvV1FR0Q0fj34g8ZlFjLDY2FhNnjxZ\nra2tbm8lYK2trZoyZQonShe42U9paamefPLJwUEx0Avc0I97wuH8s3z5ch04cEAbN24M6P70455Q\n97N3714dOnRIJSUlKikpUXFxsSSpoqIi4Ivd0I973Dj/TJw4cXBQlC5/Dl+6/NbUQNAPJIZFhEB2\ndrYaGxvd3kbAGhsblZ2d7fY2bllu9LN582bdf//9KiwslHT5VcY333wzoGPRj7tC2U9nZ6cmTZqk\ndevWDd525bNCgX52iX7cFcp+SktL9bvf/U5VVVWqqqrSCy+8IElatWqVNm/eHNAx6cddoeynp6dH\nqamp2rRp05DbJGn06MAuUUI/kBgWEQJz5sxRXV1dyNe98vZT23da19XVOS5VjdAJdT9//OMftXv3\nbjU3N6uiokIVFRVauXKl7r777oCORz/uCmU/nZ2d6uzs1Llz5wZv6+rqkhT41XTpx11uPX5Jly92\nI13+1ReBoh93hbKfKxdKuvqx6vjx45KkvLy8gI5JP5DEBW4w8rq7u01iYqI5depUSNbbs2ePWbBg\ngZk4caKJiooyM2fONMXFxeaLL7644WOdPHnSJCYmfuNFLBAaoezH5/OZcePGmaioKOPxeAb/REVF\nmcOHD9/w8ejHfaHs59KlSyY/P9+0tbUN3rZp0yaTmJho2tvbb/h49OO+UD9+XbFu3TqTmZlpoqKi\nzPe//33z2GOP3fAx6Md9oe6nrKzMnDlzZvDrJ554wuTn5wd0LPrBFVzgBiGxfPlyTZgwQWVlZW5v\n5YasXbtWZ8+e1ZYtW9zeyi2NfmAjlP2cPXtW69ev18DAgAYGBuTz+bR+/fqAfs8i/YQHzj+wEcp+\n/H6/1q9fr+7ubg0MDCguLk7PP/98QBfXoh9cwbCIkDh27JgKCgp04sSJIR++DmcDAwNKTU3V22+/\nrYyMDLe3c0ujH9igH9igH9igH0Q6PrOIkMjIyFBaWpreeustt7cybDU1NfrBD37AiTIM0A9s0A9s\n0A9s0A8iHa8sImQaGhq0cOFCNTc3Kykpye3tXJfP51NmZqZ27typnJwct7cD0Q/s0A9s0A9s0A8i\nGcMiQmrlypU6ffq0duzY4fZWrquoqEgpKSl66aWX3N4KrkI/sEE/sEE/sEE/iFjuXVsHt6Le3l6T\nlpZmdu3a5fZWrmnnzp0mLS3N9Pb2ur0VfA39wAb9wAb9wAb9IFLxyiJCrqGhQb/4xS905MgRpaSk\nuL2dITo6OjRr1izefhHG6Ac26Ac26Ac26AeRiAvcIORycnK0YsUKFRQU6OzZs25vZ5DP59O9996r\n0tJSTpRhjH5gg35gg35gg34QiXhlEa55+umnVV9fr7q6Otc/8O3z+TR//nwVFBSovLzc1b1geOgH\nNugHNugHNugHkYRXFuGa8vJy5efnKzc3Vx0dHa7to6OjQ3PnzlVBQYFeeOEF1/aBG0M/sEE/sEE/\nsEE/iCSjysrKytzeBG5NHo9H+fn5+vLLL7VkyRJNnjxZ6enpId3Drl279OCDD+rRRx/V2rVr5fF4\nQro+Akc/sEE/sEE/sEE/iCjuXl8HuMzr9Zq0tDRTVFRkurq6Rny9rq4us2jRIpOWlma8Xu+Ir4eR\nRT+wQT+wQT+wQT8Id7wNFWEhJydHR48e1aRJkzR9+nS98cYbGhgYCPo6AwMDqq6uVmZmplJSUnT0\n6FE+zH0ToB/YoB/YoB/YoB+EPbenVeDrvF6vyc3NNcnJyWbNmjXm5MmT1sc8efKkWbNmjUlOTja5\nubn8NO0mRj+wQT+wQT+wQT8IR1wNFWHr2LFjqqys1Pbt25WXl6fCwkJlZ2dr2rRpuu2226573wsX\nLqi1tVWNjY2qq6vT/v37VVxcrEcffVQZGRkh+h/ATfQDG/QDG/QDG/SDcMKwiLDX09Ojmpoaeb1e\nNTY2qq2tTenp6crKylJCQoJiYmJ08eJF9ff369y5c2pqatKHH36oKVOmKDs7W7Nnz9aiRYsUHx/v\n9n8FLqAf2KAf2KAf2KAfhAOGRUQcv9+vpqYmtbS0qKenR319fVq9erVefPFFxcfHKzMzU1lZWYqN\njXV7qwhD9AMb9AMb9AMb9AM3MCzipuDxeETKCBT9wAb9wAb9wAb9YKRxNVQAAAAAgAPDIgAAAADA\ngWERAAAAAODAsAgAAAAAcGBYBAAAAAA4MCwCAAAAABwYFgEAAAAADgyLAAAAAAAHhkUAAAAAgAPD\nIgAAAADAgWERAAAAAODAsAgAAAAAcGBYBAAAAAA4MCwCAAAAABwYFgEAAAAADgyLAAAAAAAHhkUA\nAAAAgAPDIgAAAADAgWERAAAAAODAsAgAAAAAcGBYBAAAAAA4MCwCAAAAABwYFgEAAAAADgyLAAAA\nAAAHhkUAAAAAgAPDIgAAAADAgWERAAAAAODAsAgAAAAAcGBYBAAAAAA4MCwCAAAAABwYFgEAAAAA\nDgyLAAAAAAAHhkUAAAAAgAPDIgAAAADAgWERAAAAAODAsAgAAAAAcGBYBAAAAAA4MCwCAAAAABwY\nFgEAAAAADgyLAAAAAAAHhkUAAAAAgAPDIgAAAADAgWERAAAAAODAsAgAAAAAcGBYBAAAAAA4MCwC\nAAAAABwYFgEAAAAADgyLAAAAAAAHhkUAAAAAgAPDIgAAAADAgWERAAAAAODAsAgAAAAAcGBYBAAA\nAAA4MCwCAAAAABwYFgEAAAAADgyLAAAAAAAHhkUAAAAAgAPDIgAAAADAgWERAAAAAODAsAgAAAAA\ncGBYBAAAAAA4MCwCAAAAABwYFgEAAAAADgyLAAAAAAAHhkUAAAAAgAPDIgAAAADAgWERAAAAAODA\nsAgAAAAAcGBYBAAAAAA4eIwxxu1NAIHyeDxDviZn3Aj6gQ36gQ36gQ36QajwyiIAAAAAwIFhEQAA\nAADgwLAIAAAAAHBgWAQAAAAAODAsAgAAAAAcRpWVlZW5vQngRvj9fh0+fFh79+7Vf/3Xfw35t+Tk\nZPX392v8+PGKjo52aYcIZ/QDG/QDG/QDG/QDN/CrMxD2enp6VFNTo4MHD6qxsVHt7e2aOnWqsrKy\nFBsbq/j4+MHv8/v9ampq0gcffKDJkycrOztbc+bM0aJFiwa/D7cW+oEN+oEN+oEN+kE4YFhE2Gpt\nbVVlZaV27NihefPmqbCwUNnZ2Zo2bZrGjBlz3fv29/ertbVVjY2Nqqur0759+7R48WItW7ZMGRkZ\nIfofwE30Axv0Axv0Axv0g7BigDDj9XrN3LlzzaRJk8zatWvNqVOnrI956tQps3btWpOcnGxyc3ON\n1+sNwk4RjugHNugHNugHNugH4YhhEWGjt7fXlJaWmuTkZFNdXW36+/uDvkZ/f7+prq42ycnJprS0\n1PT29gZ9DbiDfmCDfmCDfmCDfhDOuBoqwkJDQ4NmzJihzs5OtbS0qLi4eEQ+oB0dHa3i4mI1Nzfr\n9OnTmjFjhhoaGoK+DkKLfmCDfmCDfmCDfhD23J5WgYqKCpOcnGx27doV8rV37dplvvOd75iKioqQ\nr43goB/YoB/YoB/YoB9EAi5wA9cYY7Rq1Srt3r1bf/7zn5WSkuLKPjo6OlRQUKAHHnhA5eXl8ng8\nruwDN4Z+YIN+YIN+YIN+EElGu70B3LpWrVql+vp6HThwQOPHj3dtHykpKXr33XdVWFgoj8ej8vJy\n1/aC4aMf2KAf2KAf2KAfRBKGRbhiw4YN2r17t+snyiuSkpJUV1en3NxcJSQk6KmnnnJ7S7gO+oEN\n+oEN+oEN+kGk4W2oCDmv16uFCxfqyJEjrr314lo6Ojo0a9Ys7dy5Uzk5OW5vB9+AfmCDfmCDfmCD\nfhCJuBoqQsrv92vJkiX6zW9+E3YnSunyWzK2bNmikpIS+f1+t7eDr6Ef2KAf2KAf2KAfRCpeWURI\nrVy5UqdPn9aOHTvc3sp1FRUVKSUlRS+99JLbW8FV6Ac26Ac26Ac26AeRimERIdPQ0KCFCxequblZ\nSUlJbm/nunw+nzIzM3k7RhihH9igH9igH9igH0QyhkWETF5enpYuXari4mK3tzIs1dXVev3117V/\n/363twLRD+zQD2zQD2zQDyIZwyJCorW1VfPnz9eJEycUHR3t9naGZWBgQKmpqXr77beVkZHh9nZu\nafQDG/QDG/QDG/SDSMcFbhASlZWVWrp0acScKCUpOjpaS5cuVWVlpdtbueXRD2zQD2zQD2zQDyId\nryxixPX09Cg1NVUtLS1heQWw6zl16pSmT5+uTz75RPHx8W5v55ZEP7BBP7BBP7BBP7gZ8MoiRlxN\nTY3mzZtndaJctWqV7rnnHs2YMUPvv/++49/vu+8+LV261Gab3+jOO+9UXl6eampqgn5sDA/9wEYw\n+nEL/biP8w9scP7BzYBhESPu4MGDKiwsDPj+r7/+um6//XYdPnxYaWlpWrly5ZB/b2tr03//93/L\n4/HYbvUbFRYWyuv1jsix8e3oBzZs+5Hce7Iv0Y/bOP/ABucf3AwYFjHiGhsblZ2dHdB9v/rqK731\n1ltatWqVJOmjjz7SqFGjhnzPwYMHJUlz5syx2+g1ZGdnq7GxcUSOjW9HP7Bh04/k/pN9+nEX5x/Y\n4PyDmwHDIkZUb2+v2tvbNW3atIDuf+DAAT3wwAOSpKNHj+rYsWN68MEHHd8jSXPnzrXb7DVMmzZN\nbW1t8vv9I3J8XBv9wIZtP+HwZJ9+3MP5BzY4/+BmwbCIEdXU1KSpU6dqzJgxAd3/pz/9qX71q19J\nkv793/9d0dHR+od/+Ich33Pw4EHdeeedSk1Ntd7vN7ntttuUnp6upqamETk+ro1+YMO2n3B4sk8/\n7uH8Axucf3CzYFjEiGppaVFWVpb1ca78hO3ee+9VUlLS4O2dnZ1qa2tz/FTt4MGDmjp1qvW6V2Rl\nZamlpSVox8PwhLqfvr4+Pfvss1q+fLny8vJUVFSkTz/91Hp9+nGHbT/h8GRfoh+3uPX4dcUjjzyi\nQ4cOWa9PP+7g/IObxWi3N4CbW09PjxISEqyP4/V6dfbsWf3t3/7tkNu//haM7du3q76+Xj09Pfro\no4+s170iISFBPT09QTsehifU/Tz33HNatmyZ7rrrLklSSUmJcnJy1NTUpDvuuCPg9enHHcHq59ue\n7P/yl7+UJPn9fm3dulVnzpyR3+9XW1ubnn/+eesfXNGPO0J9/rlafX29qqqq9PDDD1uvTz/uCPX5\n5w9/+INefvllzZo1S3Fxcero6FBMTIy2bdtmtT79gFcWMaLOnz+vmJgY6+N88MEHkqR77rlnyO1X\n3oJx5cF28eLF2rp1q+6//37rNa8WExOjvr6+oB4T3y6U/Zw/f16//vWvtXXr1sF/f+aZZ9TR0aGq\nqiqr9enHHcHqZ7hP9p9++mnt3btX69at04svvqjU1FTNmzdP3d3dVuvTjztC/fh1hd/v1759+6zX\nvYJ+3BHq88/Fixfl8/n0xhtv6E9/+pPS09P12muvWa9PP2BYRETo7e2VJN1+++2Dt/n9fu3Zs0fj\nx49XRkaGW1tDBBhOPxcvXlRSUtKQB8Xvfve7ki5fcQ63ruE+2R81apTOnDkz+O8//OEP1dXVpY8/\n/jhEO0U4utHHr1deeUUrVqwI6R4RvoZ7/vF4PKqurh68sE5ZWVnAn5cErsawiBE1duzYoPxEKj8/\nXx6PR++8844kqbu7W7/85S914sQJ5eTkWB//2/T19QXlJ4S4MaHsZ9y4cTpx4oQqKioG79fe3i5J\nmjx5stX69OOOYPUz3Cf7L7/8so4cOTL4PcePH1dcXJzS09Ot1qcfd7jx+HXkyBF973vf04QJE6zX\nvYJ+3BHq848kGWOs1/s6+gHDIkZUfHy8zp07Z32cmTNn6ve//722bNmi2bNn66GHHtKUKVMkjdwl\no6927tw5xcfHj/g6GMrtfrZv366JEyfqkUcesVqfftwRrH4C+WFVd3e3amtr9eqrryouLs5qffpx\nR6jPP1999ZVqa2sHP4MWLPTjDjfOP++9956eeuopPffcc1q8eHFQLtBGP2BYxIjKzMwM2iWXi4uL\n1draKq/Xq/r6en322WeSpJ///OdBOf71NDU1KTMzc8TXwVBu9vPpp5+qsrJSW7duVWJiotXa9OOO\nYPVzIz9sOH/+vDZs2KCHH35Yv/rVr1RUVGS9Pv24I9Tnn9/+9rdavnx5UNa7Gv24w43zz+eff64N\nGzZo9erVeuihhzR//nwNDAxYrU8/kAFGUG9vr4mJiTEXLlwI+Bivv/66ueOOO0xtbe3gbWfPnjVx\ncXHmvvvu+8b7VFVVGY/HE/CaVzt//ryJiYkxvb29QTkehs+tfvr7+83PfvYzs3PnzoDXvYJ+3BOM\nfq5l0aJFxuPxmP/93/+95vesWLHC5Obmmr6+voDXoR/3hPL889FHH5l/+7d/G3Jfj8dj9u/fH/Da\nxtCPm0J9/vnLX/5i+vv7B7/+8ssvjcfjMTt27Ah4HfqBMcbwyiJGVGxsrCZPnqzW1taAj7F582b1\n9fUpOTl58LY1a9ZozJgx+td//ddgbPO6WltbNWXKFMXGxo74WhjKrX5KS0v15JNPasGCBZLsLnBD\nP+4JRj//8R//ofHjx2vnzp2Dt33++efas2ePCgsLlZaWds37Ll++XAcOHNDGjRsDXp9+3BPK88/e\nvXt16NAhlZSUqKSkRMXFxZKkiooKq4vd0I97Qn3+mThxoqKjowe/HjdunKTLb00NFP1A4m2oCIHs\n7Gw1NjYGfP+7775bb775pnJycmSM0caNG1VbW6v//M//HHwrxkhqbGxUdnb2iK+DbxbqfjZv3qz7\n779fhYXfVf6vAAAGeUlEQVSFkqSBgQG9+eabAa9PP+6y7We4T/Y7Ozs1adIkrVu3bvC2K1fTff/9\n9wNen37cFarzT2lpqX73u9+pqqpKVVVVeuGFFyRJq1at0ubNmwNen37cFarzT09Pj1JTU7Vp06Yh\nt0nS6NGB/0p1+oHEsIgQmDNnjurq6gK+/29/+1u9+uqrys3N1U9+8hOdPHlSR48eve5VUC9duiQp\nOFcGq6ur0+zZs62Pg8CEsp8//vGP2r17t5qbm1VRUaGKigqtXLlSd999d8Dr04+7bPsZ7pP9zs5O\ndXZ2DrmgRVdXlyS7q+nSj7vcePySLl/sRrr8u/Ns0I+7QnX+iYqKGvz+K44fPy5JysvLC3h9+oEk\nPrOIkdfd3W0SExPNqVOnRnytPXv2mAULFpiJEyeaqKgoM3PmTFNcXGy++OKLgI538uRJk5iYaLq7\nu4O8UwxXqPrx+Xxm3LhxJioqyng8nsE/UVFR5vDhwwEdk37cZ9vPyZMnzd/8zd+YuXPnmr/6q78y\njz/+uPnss88c33fp0iWTn59v2traBm/btGmTSUxMNO3t7QGvTT/uCuXj1xXr1q0zmZmZJioqynz/\n+983jz32WEDHoR/3her8Y4wxZWVl5syZM4NfP/HEEyY/Pz+gda+sTT8wxhiPMSPwS1mAr1m+fLkm\nTJigsrIyt7dyQ9auXauzZ89qy5Ytbm/llkY/sBGqfs6ePav169drYGBAAwMD8vl8Wr9+fcC/Z5F+\nwgPnH9gIVT9+v1/r169Xd3e3BgYGFBcXp+eff15jx44N6Hj0gysYFhESx44dU0FBgU6cODHkA9jh\nbGBgQKmpqXr77beH/NJbhB79wAb9wAb9wAb9INLxmUWEREZGhtLS0vTWW2+5vZVhq6mp0Q9+8ANO\nlGGAfmCDfmCDfmCDfhDpeGURIdPQ0KCFCxequblZSUlJbm/nunw+nzIzM7Vz585vvRABQoN+YIN+\nYIN+YIN+EMkYFhFSK1eu1OnTp7Vjxw63t3JdRUVFSklJ0UsvveT2VnAV+oEN+oEN+oEN+kHEcu/a\nOrgV9fb2mrS0NLNr1y63t3JNO3fuNGlpaaa3t9ftreBr6Ac26Ac26Ac26AeRilcWEXINDQ36xS9+\noSNHjiglJcXt7QzR0dGhWbNm8faLMEY/sEE/sEE/sEE/iERc4AYhl5OToxUrVqigoEBnz551ezuD\nfD6f7r33XpWWlnKiDGP0Axv0Axv0Axv0g0jEK4twzdNPP636+nrV1dW5/oFvn8+n+fPnq6CgQOXl\n5a7uBcNDP7BBP7BBP7BBP4gkvLII15SXlys/P1+5ubnq6OhwbR8dHR2aO3euCgoK9MILL7i2D9wY\n+oEN+oEN+oEN+kEkGVVWVlbm9iZwa/J4PMrPz9eXX36pJUuWaPLkyUpPTw/pHnbt2qUHH3xQjz76\nqNauXSuPxxPS9RE4+oEN+oEN+oEN+kFEcff6OsBlXq/XpKWlmaKiItPV1TXi63V1dZlFixaZtLQ0\n4/V6R3w9jCz6gQ36gQ36gQ36QbjjbagICzk5OTp69KgmTZqk6dOn64033tDAwEDQ1xkYGFB1dbUy\nMzOVkpKio0eP8mHumwD9wAb9wAb9wAb9IOy5Pa0CX+f1ek1ubq5JTk42a9asMSdPnrQ+5smTJ82a\nNWtMcnKyyc3N5adpNzH6gQ36gQ36gQ36QTjiaqgIW8eOHVNlZaW2b9+uvLw8FRYWKjs7W9OmTdNt\nt9123fteuHBBra2tamxsVF1dnfbv36/i4mI9+uijysjICNH/AG6iH9igH9igH9igH4QThkWEvZ6e\nHtXU1Mjr9aqxsVFtbW1KT09XVlaWEhISFBMTI0nq6+vTuXPn1NTUpA8//FBTpkxRdna2Zs+erUWL\nFik+Pt7l/wncQD+wQT+wQT+wQT8IBwyLiDh+v19NTU1qaWlRT0+P+vr6JEkxMTGKj49XZmamsrKy\nFBsb6/JOEY7oBzboBzboBzboB25gWAQAAAAAOHA1VAAAAACAA8MiAAAAAMCBYREAAAAA4MCwCAAA\nAABwYFgEAAAAADgwLAIAAAAAHBgWAQAAAAAODIsAAAAAAAeGRQAAAACAA8MiAAAAAMCBYREAAAAA\n4MCwCAAAAABwYFgEAAAAADgwLAIAAAAAHBgWAQAAAAAODIsAAAAAAAeGRQAAAACAA8MiAAAAAMCB\nYREAAAAA4MCwCAAAAABw+H/S3XMlsOzHiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1056f6a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def makeDBN(inter, intra, T, labels):\n",
    "    \"\"\"Unfold a graph for T time slices\"\"\"\n",
    "    N = max(max([i for i,j in inter]),max([j for i,j in inter]))+1\n",
    "\n",
    "    G = np.zeros((N*T,N*T))\n",
    "    pos = []\n",
    "    all_labels = []\n",
    "    for n in range(N):\n",
    "        pos.append((0,-n))\n",
    "        all_labels.append('$'+labels[n]+'_{'+str(0+1)+\"}\"+'$')\n",
    "        \n",
    "    for e in inter:\n",
    "        s,d = e\n",
    "        G[s,d] = 1\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for n in range(N):\n",
    "            pos.append((t,-n))\n",
    "            all_labels.append('$'+labels[n]+'_{'+str(t+1)+\"}\"+'$')\n",
    "\n",
    "        for e in inter:\n",
    "            s,d = e\n",
    "            s = s + N*t\n",
    "            d = d + N*t\n",
    "            G[s,d] = 1\n",
    "        \n",
    "        for e in intra:\n",
    "            s,d = e\n",
    "            s = s + N*(t-1)\n",
    "            d = d + N*t\n",
    "            G[s,d] = 1\n",
    "    return G,pos,all_labels\n",
    "\n",
    "#inter = [(0,1),(1,2),(2,3)]\n",
    "#intra = [(0,0),(1,1),(0,1),(0,2)]\n",
    "#variable_names = [\"r\",\"z\",\"x\", \"y\"] \n",
    "inter = [(0,1)]\n",
    "intra = [(0,0)]\n",
    "variable_names = [\"x\", \"y\"] \n",
    "T = 5\n",
    "\n",
    "A, pos, label_list = makeDBN(inter, intra, T, variable_names)\n",
    "\n",
    "G = nx.DiGraph(A)\n",
    "labels = {i: s for i,s in enumerate(label_list)}\n",
    "plt.figure(figsize=(12,2.5))\n",
    "nx.draw(G, pos, node_color=\"white\", node_size=2500, labels=labels, font_size=24, arrows=True)\n",
    "#nx.draw_graphviz(G,node_size=500, labels=labels, font_size=24, arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}) & = & \\sum_{x_{1:K}} p(y_{1:K}|x_{1:K}) p(x_{1:K}) \\\\\n",
    "& = &  \\underbrace{\\sum_{x_K} p(y_K | x_K ) \\sum_{x_{K-1}} p(x_K|x_{K-1})}_{\\alpha_K}  \\dots \\sum_{x_{2}} p(x_3|x_{2})\n",
    "                               \\underbrace{p(y_{2}|x_{2})\\overbrace{ \\sum_{x_{1}} p(x_2|x_{1})}^{\\alpha_{2|1}} }_{\\alpha_2}\n",
    "                                 \\underbrace{p(y_{1}|x_{1})\\overbrace{p(x_1)}^{\\alpha_{1|0}}}_{\\alpha_1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "##### Backward Pass\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}) & = &  \\sum_{x_1} p(x_1) p(y_1 | x_1 )\n",
    "%underbrace{\\sum_{x_2} p(x_2|x_{1}) p(y_2 | x_2 )}_{\\beta_1}\n",
    "\\dots\n",
    "\\underbrace{ \\sum_{x_{K-1}} p(x_{K-1}|x_{K-2}) p(y_{K-1} | x_{K-1} )}_{\\beta_{K-2}}\n",
    "\\underbrace{ \\sum_{x_K} p(x_K|x_{K-1}) p(y_K | x_K )}_{\\beta_{K-1}}\n",
    "\\underbrace{{\\pmb 1}}_{\\beta_{K}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1|0} & \\equiv & p(x_1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{k|k} & \\equiv & p(y_{1:k}, x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{k|k-1}  & \\equiv & p(y_{1:k-1}, x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "For $k=1, 2, \\dots, K$\n",
    "\n",
    "__Predict__\n",
    "\n",
    "$k=1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1|0}(x_1) = p(x_1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "$k>1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\alpha_{k|k-1}(x_k)} & = & p(y_{1:k-1}, x_k) = \\sum_{x_{k-1}} p(x_k| x_{k-1}) p(y_{1:k-1}, x_{k-1}) \\\\\n",
    "& = & \\sum_{x_{k-1}} p(x_k| x_{k-1}) { \\alpha_{k-1|k-1}(x_{k-1}) }\n",
    "\\end{eqnarray}\n",
    "\n",
    "__Update__\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\alpha_{k|k}(x_k) } & = & p(y_{1:k}, x_k) = p(y_k | x_k) p(y_{1:k-1}, x_k) \\\\\n",
    " & = & p(y_k | x_k) {\\alpha_{k|k-1}(x_k)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k+1}(x_k) & \\equiv & p(y_{k+1:K}| x_k) \\\\\n",
    "\\beta_{k|k}(x_k) & \\equiv & p(y_{k:K}| x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "For $k=K, K-1, \\dots, 1$\n",
    "\n",
    "__'Postdict'__ : (Backward Prediction)\n",
    "\n",
    "$k=K$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_{K|K+1}(x_K) & = & \\mathbf{1} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "$k<K$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k+1}(x_k) & = & p(y_{k+1:K}| x_k) = \\sum_{x_{k+1}} p(x_{k+1}| x_{k}) p(y_{k+1:K}| x_{k+1}) \\\\\n",
    "& = & \\sum_{x_{k+1}} p(x_{k+1}| x_{k}) {\\beta_{k+1|k+1}(x_{k+1}) }\n",
    "\\end{eqnarray}\n",
    "\n",
    "__Update__\n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k}(x_k)  & = & p(y_{k:K}| x_k) = p(y_k | x_k) p(y_{k+1:K}| x_k) \\\\\n",
    " & = & p(y_k | x_k) {\\beta_{k|k+1}(x_k)}\n",
    "\\end{eqnarray}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically Stable computation of $\\log(\\sum_i \\exp (l_i ) ))$\n",
    "\n",
    "Derivation\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L & = & \\log(\\sum_i \\exp (l_i) ) \n",
    " =   \\log(\\sum_i \\exp (l_i) \\frac{\\exp(l^*)}{\\exp(l^*)} ) \\\\\n",
    "& = &  \\log( \\exp(l^*) \\sum_i \\exp (l_i - l^*) ) \\\\\n",
    "& = &  l^* + \\log( \\sum_i \\exp (l_i - l^*) )\n",
    "\\end{eqnarray}\n",
    "\n",
    "Choose $l^*  =  \\max_i l_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Naive evaluation  :', -inf)\n",
      "('Numerically stable:', array([-1000.]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_sum_exp_naive(l):\n",
    "    return np.log(np.sum(np.exp(l)))\n",
    "\n",
    "def log_sum_exp(l, axis=0):\n",
    "    l_star = np.max(l, axis=axis, keepdims=True)\n",
    "    return l_star + np.log(np.sum(np.exp(l - l_star),axis=axis,keepdims=True)) \n",
    "    \n",
    "    \n",
    "l = np.array([-1000, -10000])\n",
    "\n",
    "print('Naive evaluation  :', log_sum_exp_naive(l))\n",
    "print('Numerically stable:', log_sum_exp(l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An implementation of the forward backward algorithm\n",
    "# For numerical stability, we calculate everything in the log domain\n",
    "\n",
    "# Defined in Sampling.ipynb\n",
    "def normalize(A, axis=None):\n",
    "\n",
    "    Z = np.sum(A, axis=axis,keepdims=True)\n",
    "        \n",
    "    idx = np.where(Z == 0)\n",
    "    Z[idx] = 1\n",
    "    return A/Z\n",
    "\n",
    "def predict(A, lp):\n",
    "    lstar = np.max(lp)\n",
    "    return lstar + np.log(np.dot(A,np.exp(lp-lstar)))\n",
    "\n",
    "def postdict(A, lp):\n",
    "    lstar = np.max(lp)\n",
    "    return lstar + np.log(np.dot(np.exp(lp-lstar), A))\n",
    "\n",
    "def update(y, logB, lp):\n",
    "    return logB[y,:] + lp\n",
    "\n",
    "\n",
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "y = np.array([0, 1, 3, 2, 4])\n",
    "T = y.shape[0]\n",
    "\n",
    "logA = np.log(A)\n",
    "logB = np.log(B)\n",
    "\n",
    "# Forward Pass\n",
    "\n",
    "# Python indexes starting from zero so\n",
    "# log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "# log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "log_alpha  = np.zeros((S, T))\n",
    "log_alpha_pred = np.zeros((S, T))\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred[:,0] = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred[:,k] = predict(A, log_alpha[:,k-1])\n",
    "    \n",
    "    log_alpha[:,k] = update(y[k], logB, log_alpha_pred[:,k])\n",
    "    \n",
    "# Backward Pass\n",
    "log_beta  = np.zeros((S, T))\n",
    "log_beta_post = np.zeros((S, T))\n",
    "\n",
    "for k in range(T-1,-1,-1):\n",
    "    if k==T-1:\n",
    "        log_beta_post[:,k] = np.zeros(S)\n",
    "    else:\n",
    "        log_beta_post[:,k] = postdict(A, log_beta[:,k+1])\n",
    "    \n",
    "    log_beta[:,k] = update(y[k], logB, log_beta_post[:,k])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randgen(pr, N=1): \n",
    "    L = len(pr)\n",
    "    return np.random.choice(range(L), size=N, replace=True, p=pr)\n",
    "\n",
    "#Generate data from an HMM\n",
    "S = 3\n",
    "R = 5\n",
    "\n",
    "pi = normalize(np.random.rand(S),axis=0)\n",
    "A = normalize(np.random.rand(S,S),axis=0)\n",
    "B = normalize(np.random.rand(R,S),axis=0)\n",
    "\n",
    "# Number of steps\n",
    "T = 100\n",
    "\n",
    "x = np.zeros(T)\n",
    "y = np.zeros(T)\n",
    "\n",
    "for t in range(T):\n",
    "    if t==0:\n",
    "        x[t] = randgen(pi)\n",
    "    else:\n",
    "        x[t] = randgen(A[:,x[t-1]])\n",
    "    \n",
    "    y[t] = randgen(B[:,x[t]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing - Forward Backward Algorithm (Two filter formulation)\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}, x_k) & = & p(y_{1:k}, x_k) p(y_{k+1:K} | x_k) \\\\\n",
    "& = & {\\alpha_{k|k}(x_k) } {\\beta_{k|k+1}(x_k)} \\\\\n",
    "& \\equiv & \\gamma_k(x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Smoothing (Forward filtering - Backward smoothing), The Correction Smoother\n",
    "\n",
    "Suppose we have computed the filtered quantities $p(x_t| y_{1:t})$ via the forward pass. The forward-backward algorithm requires us to store all observations. For batch settings, this is OK however when datapoints are arriving indeed sequentially, this may be not desired.  \n",
    "\n",
    "We will derive a recursive algorithm to compute the marginals $p(x_t | y_{1:T})$.\n",
    "\n",
    "Note that if we calculate instead the so-called __pairwise__ marginal $p(x_t, x_{t+1} | y_{1:T} )$, we can get by simple marginalization\n",
    "\n",
    "\\begin{align}\n",
    "p(x_t | y_{1:T}) & =  \\sum_{x_{t+1}} p(x_t, x_{t+1} | y_{1:T} )  & \\text{Definition} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "p(x_t, x_{t+1} | y_{1:T} ) & =   p(x_{t} |x_{t+1}, y_{1:t},y_{t+1:T} ) p(x_{t+1}|y_{1:T} ) & \\text{Factorize} \\\\\n",
    "& =   p(x_{t} |x_{t+1}, y_{1:t} ) p(x_{t+1}|y_{1:T} ) & \\text{Conditional Independence}\\\\\n",
    "  & =  \\frac{p(x_{t}, x_{t+1}| y_{1:t} )}{p(x_{t+1}| y_{1:t} )} p(x_{t+1}|y_{1:T} ) & \\text{Definition of Conditional} \n",
    "\\end{align}\n",
    "\n",
    "This update has the form:\n",
    "\\begin{eqnarray}\n",
    "\\text{New Pairwise Marginal}_{t,t+1} & = & \\frac{\\text{Old Pairwise Marginal}_{t,t+1}}{\\text{Old Marginal}_{t+1}} \\times {\\text{New Marginal}_{t+1}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "The old pairwise marginal can be simply calculated from the filtered marginals as\n",
    "\n",
    "\\begin{align}\n",
    "p(x_{t}, x_{t+1}| y_{1:t} ) & =  p(x_{t+1} | x_t,  y_{1:t} ) p(x_t | y_{1:t}) & \\text{Definition} \\\\\n",
    "& =  p(x_{t+1} | x_t ) p(x_t | y_{1:t}) & \\text{Conditional Independence} \\\\\n",
    "& = \\text{Transition Model} \\times \\text{Filtering distribution} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "The correction smoother calculates a factorisation of the posterior of form\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(x_{1:T}|y_{1:T}) & = & \\frac{\\prod_{t=1}^{T-1} p(x_{t}, x_{t+1} | y_{1:T}) }{ \\prod_{t=2}^{T-1} p(x_{t} | y_{1:T}) }\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.76473315 -7.76473315 -7.76473315 -7.76473315 -7.76473315]]\n"
     ]
    }
   ],
   "source": [
    "# Smoother check\n",
    "# All numbers must be equal to the marginal likelihood p(y_{1:K})\n",
    "\n",
    "log_gamma = log_alpha + log_beta_post\n",
    "print(log_sum_exp(log_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Correction Smoother\n",
    "# For numerical stability, we calculate everything in the log domain\n",
    "log_gamma_corr = np.zeros_like(log_alpha)\n",
    "log_gamma_corr[:,T-1] = log_alpha[:,T-1]\n",
    "\n",
    "for k in range(T-2,-1,-1):\n",
    "    log_old_pairwise_marginal = log_alpha[:,k].reshape(1,S) + logA \n",
    "    log_old_marginal = predict(A, log_alpha[:,k])\n",
    "    log_new_pairwise_marginal = log_old_pairwise_marginal + log_gamma_corr[:,k+1].reshape(S,1) - log_old_marginal.reshape(S,1)\n",
    "    log_gamma_corr[:,k] = log_sum_exp(log_new_pairwise_marginal, axis=0).reshape(S)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -7.81659977 -11.43670606  -8.83277138 -11.04365384 -10.22402554]\n",
      " [-10.74985688  -8.42061397 -11.29064374 -10.11727173  -7.96630137]\n",
      " [-19.15983484  -8.55089731  -8.23171269  -7.90721448 -10.09719242]]\n",
      "[[ -7.81659977 -11.43670606  -8.83277138 -11.04365384 -10.22402554]\n",
      " [-10.74985688  -8.42061397 -11.29064374 -10.11727173  -7.96630137]\n",
      " [-19.15983484  -8.55089731  -8.23171269  -7.90721448 -10.09719242]]\n"
     ]
    }
   ],
   "source": [
    "# Verify that result coincide\n",
    "\n",
    "print(log_gamma)\n",
    "print(log_gamma_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of HMM in Python\n",
    "\n",
    "We will integrate filtering, smoothing and training functionality into an HMM object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randgen(pr, N=1): \n",
    "    L = len(pr)\n",
    "    return np.random.choice(range(L), size=N, replace=True, p=pr)\n",
    "\n",
    "def log_sum_exp(l, axis=0):\n",
    "    l_star = np.max(l, axis=axis, keepdims=True)\n",
    "    return l_star + np.log(np.sum(np.exp(l - l_star),axis=axis,keepdims=True)) \n",
    "\n",
    "def normalize_exp(log_P, axis=None):\n",
    "    a = np.max(log_P, keepdims=True, axis=axis)\n",
    "    P = normalize(np.exp(log_P - a), axis=axis)\n",
    "    return P\n",
    "\n",
    "def normalize(A, axis=None):\n",
    "\n",
    "    Z = np.sum(A, axis=axis,keepdims=True)\n",
    "        \n",
    "    idx = np.where(Z == 0)\n",
    "    Z[idx] = 1\n",
    "    return A/Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 15.88596022  43.53889168  66.7431929 ]\n",
      " [ 60.90427912   1.7644211   17.08158815]\n",
      " [ 49.29195672  34.37190379   9.41780631]]\n",
      "[[ 15.88596022  43.53889168  66.7431929 ]\n",
      " [ 60.90427912   1.7644211   17.08158815]\n",
      " [ 49.29195672  34.37190379   9.41780631]]\n"
     ]
    }
   ],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self, pi, A, B):\n",
    "        # p(x_0)\n",
    "        self.pi = pi\n",
    "        # p(x_k|x_{k-1})\n",
    "        self.A = A\n",
    "        # p(y_k|x_{k})\n",
    "        self.B = B\n",
    "        # Number of possible latent states at each time\n",
    "        self.S = pi.shape[0]\n",
    "        # Number of possible observations at each time\n",
    "        self.R = B.shape[0]\n",
    "        self.logB = np.log(self.B)\n",
    "        self.logA = np.log(self.A)\n",
    "        self.logpi = np.log(self.pi)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_random_parameters(cls, S=3, R=5):\n",
    "        A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "        B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "        pi = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "        return cls(pi, A, B)\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = \"Prior:\\n\" + str(self.pi) + \"\\nA:\\n\" + str(self.A) + \"\\nB:\\n\" + str(self.B)\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.__str__()\n",
    "        return s\n",
    "\n",
    "    def predict(self, lp):\n",
    "        lstar = np.max(lp)\n",
    "        return lstar + np.log(np.dot(self.A,np.exp(lp-lstar)))\n",
    "\n",
    "    def postdict(self, lp):\n",
    "        lstar = np.max(lp)\n",
    "        return lstar + np.log(np.dot(np.exp(lp-lstar), self.A))\n",
    "\n",
    "    def update(self, y, lp):\n",
    "        return self.logB[y,:] + lp\n",
    "\n",
    "    def generate_sequence(self, T=10):\n",
    "    # T: Number of steps\n",
    "\n",
    "        x = np.zeros(T)\n",
    "        y = np.zeros(T)\n",
    "\n",
    "        for t in range(T):\n",
    "            if t==0:\n",
    "                x[t] = randgen(pi)\n",
    "            else:\n",
    "                x[t] = randgen(A[:,x[t-1]])    \n",
    "            y[t] = randgen(B[:,x[t]])\n",
    "    \n",
    "        return y, x\n",
    "\n",
    "    def forward(self, y):\n",
    "        T = len(y)\n",
    "        \n",
    "        # Forward Pass\n",
    "\n",
    "        # Python indexes starting from zero so\n",
    "        # log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "        # log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "        log_alpha  = np.zeros((self.S, T))\n",
    "        log_alpha_pred = np.zeros((self.S, T))\n",
    "        for k in range(T):\n",
    "            if k==0:\n",
    "                log_alpha_pred[:,0] = self.logpi\n",
    "            else:\n",
    "                log_alpha_pred[:,k] = self.predict(log_alpha[:,k-1])\n",
    "\n",
    "            log_alpha[:,k] = self.update(y[k], log_alpha_pred[:,k])\n",
    "            \n",
    "        return log_alpha, log_alpha_pred\n",
    "            \n",
    "    def backward(self, y):\n",
    "        # Backward Pass\n",
    "        T = len(y)\n",
    "        log_beta  = np.zeros((self.S, T))\n",
    "        log_beta_post = np.zeros((self.S, T))\n",
    "\n",
    "        for k in range(T-1,-1,-1):\n",
    "            if k==T-1:\n",
    "                log_beta_post[:,k] = np.zeros(self.S)\n",
    "            else:\n",
    "                log_beta_post[:,k] = self.postdict(log_beta[:,k+1])\n",
    "\n",
    "            log_beta[:,k] = self.update(y[k], log_beta_post[:,k])\n",
    "\n",
    "        return log_beta, log_beta_post\n",
    "        \n",
    "    def forward_backward_smoother(self, y):\n",
    "        log_alpha, log_alpha_pred = self.forward(y)\n",
    "        log_beta, log_beta_post = self.backward(y)\n",
    "        \n",
    "        log_gamma = log_alpha + log_beta_post\n",
    "        return log_gamma\n",
    "        \n",
    "    def correction_smoother(self, y):\n",
    "        # Correction Smoother\n",
    "\n",
    "        log_alpha, log_alpha_pred = self.forward(y)\n",
    "        T = len(y)\n",
    "        \n",
    "        # For numerical stability, we calculate everything in the log domain\n",
    "        log_gamma_corr = np.zeros_like(log_alpha)\n",
    "        log_gamma_corr[:,T-1] = log_alpha[:,T-1]\n",
    "\n",
    "        C2 = np.zeros((self.S, self.S))\n",
    "        C3 = np.zeros((self.R, self.S))\n",
    "        C3[y[-1],:] = normalize_exp(log_alpha[:,T-1])\n",
    "        for k in range(T-2,-1,-1):\n",
    "            log_old_pairwise_marginal = log_alpha[:,k].reshape(1,self.S) + self.logA \n",
    "            log_old_marginal = self.predict(log_alpha[:,k])\n",
    "            log_new_pairwise_marginal = log_old_pairwise_marginal + log_gamma_corr[:,k+1].reshape(self.S,1) - log_old_marginal.reshape(self.S,1)\n",
    "            log_gamma_corr[:,k] = log_sum_exp(log_new_pairwise_marginal, axis=0).reshape(self.S)\n",
    "            C2 += normalize_exp(log_new_pairwise_marginal)\n",
    "            C3[y[k],:] += normalize_exp(log_gamma_corr[:,k])\n",
    "        C1 = normalize_exp(log_gamma_corr[:,0])\n",
    "        return log_gamma_corr, C1, C2, C3\n",
    "    \n",
    "    def forward_only_SS(self, y, V=None):\n",
    "        # Forward only estimation of expected sufficient statistics\n",
    "        T = len(y)\n",
    "        \n",
    "        if V is None:\n",
    "            V1  = np.eye((self.S))\n",
    "            V2  = np.zeros((self.S,self.S,self.S))\n",
    "            V3  = np.zeros((self.R,self.S,self.S))\n",
    "        else:\n",
    "            V1, V2, V3 = V\n",
    "            \n",
    "        I_S1S = np.eye(self.S).reshape((self.S,1,self.S))\n",
    "        I_RR = np.eye(self.R)\n",
    "        \n",
    "        for k in range(T):\n",
    "            if k==0:\n",
    "                log_alpha_pred = self.logpi\n",
    "            else:\n",
    "                log_alpha_pred = self.predict(log_alpha)\n",
    "\n",
    "            if k>0:\n",
    "                #print(self.S, self.R)\n",
    "                #print(log_alpha)\n",
    "                # Calculate p(x_{k-1}|y_{1:k-1}, x_k) \n",
    "                lp = np.log(normalize_exp(log_alpha)).reshape(self.S,1) + self.logA.T    \n",
    "                P = normalize_exp(lp, axis=0)\n",
    "\n",
    "                # Update\n",
    "                V1 = np.dot(V1, P)             \n",
    "                V2 = np.dot(V2, P) + I_S1S*P.reshape((1,self.S,self.S))    \n",
    "                V3 = np.dot(V3, P) + I_RR[:,y[k-1]].reshape((self.R,1,1))*P.reshape((1,self.S,self.S))    \n",
    "\n",
    "            log_alpha = self.update(y[k], log_alpha_pred)    \n",
    "            p_xT = normalize_exp(log_alpha)    \n",
    "\n",
    "        C1 = np.dot(V1, p_xT.reshape(self.S,1))\n",
    "        C2 = np.dot(V2, p_xT.reshape(1,self.S,1)).reshape((self.S,self.S))\n",
    "        C3 = np.dot(V3, p_xT.reshape(1,self.S,1)).reshape((self.R,self.S))\n",
    "        C3[y[-1],:] +=  p_xT\n",
    "        \n",
    "        ll = log_sum_exp(log_alpha)\n",
    "        \n",
    "        return C1, C2, C3, ll, (V1, V2, V3)\n",
    "\n",
    "    def train_EM(self, y, EPOCH=10):\n",
    "        \n",
    "        LL = np.zeros(EPOCH)\n",
    "        for e in range(EPOCH):\n",
    "            C1, C2, C3, ll, V = self.forward_only_SS(y)\n",
    "            LL[e] = ll\n",
    "            p = normalize(C1 + 0.1, axis=0).reshape(S)\n",
    "            #print(p,np.size(p))            \n",
    "            A = normalize(C2, axis=0)\n",
    "            #print(A)\n",
    "            B = normalize(C3, axis=0)\n",
    "            #print(B)\n",
    "            self.__init__(p, A, B)\n",
    "            print(ll)\n",
    "            \n",
    "        return LL\n",
    "            \n",
    "    \n",
    "    \n",
    "hm = HMM.from_random_parameters()\n",
    "\n",
    "y,x = hm.generate_sequence(300)\n",
    "\n",
    "log_alpha, log_alpha_pred = hm.forward(y)\n",
    "log_gamma = hm.forward_backward_smoother(y)\n",
    "log_gamma_corr, C1_corr, C2_corr, C3_corr = hm.correction_smoother(y)\n",
    "C1, C2, C3, ll, V = hm.forward_only_SS(y)\n",
    "\n",
    "print(C2)\n",
    "print(C2_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-426.58701925]\n",
      "[-426.50875094]\n",
      "[-426.43820233]\n",
      "[-426.37320209]\n",
      "[-426.31236722]\n",
      "[-426.25485919]\n",
      "[-426.20019912]\n",
      "[-426.1481339]\n",
      "[-426.09854186]\n",
      "[-426.05136786]\n",
      "[-426.0065801]\n",
      "[-425.96414281]\n",
      "[-425.92400082]\n",
      "[-425.88607251]\n",
      "[-425.85024882]\n",
      "[-425.81639596]\n",
      "[-425.78436028]\n",
      "[-425.75397395]\n",
      "[-425.72506052]\n",
      "[-425.6974399]\n",
      "[-425.67093218]\n",
      "[-425.64536059]\n",
      "[-425.62055323]\n",
      "[-425.59634408]\n",
      "[-425.57257315]\n",
      "[-425.54908621]\n",
      "[-425.5257341]\n",
      "[-425.50237182]\n",
      "[-425.47885755]\n",
      "[-425.45505165]\n",
      "[-425.43081579]\n",
      "[-425.40601216]\n",
      "[-425.38050294]\n",
      "[-425.35414999]\n",
      "[-425.32681488]\n",
      "[-425.29835919]\n",
      "[-425.26864528]\n",
      "[-425.23753737]\n",
      "[-425.20490301]\n",
      "[-425.17061495]\n",
      "[-425.13455309]\n",
      "[-425.09660655]\n",
      "[-425.0566755]\n",
      "[-425.01467232]\n",
      "[-424.97052201]\n",
      "[-424.92416123]\n",
      "[-424.87553596]\n",
      "[-424.82459766]\n",
      "[-424.77129818]\n",
      "[-424.71558399]\n",
      "[-424.65739059]\n",
      "[-424.59663808]\n",
      "[-424.53322934]\n",
      "[-424.46705178]\n",
      "[-424.39798384]\n",
      "[-424.32590704]\n",
      "[-424.25072388]\n",
      "[-424.17238137]\n",
      "[-424.09089967]\n",
      "[-424.00640385]\n",
      "[-423.91915612]\n",
      "[-423.82958436]\n",
      "[-423.73830154]\n",
      "[-423.6461096]\n",
      "[-423.55398202]\n",
      "[-423.46302096]\n",
      "[-423.37438957]\n",
      "[-423.28922555]\n",
      "[-423.20854827]\n",
      "[-423.13317501]\n",
      "[-423.06366192]\n",
      "[-423.00027942]\n",
      "[-422.94302419]\n",
      "[-422.89166129]\n",
      "[-422.84578443]\n",
      "[-422.80488154]\n",
      "[-422.76839442]\n",
      "[-422.73576615]\n",
      "[-422.70647379]\n",
      "[-422.68004743]\n",
      "[-422.65607821]\n",
      "[-422.63421873]\n",
      "[-422.61417867]\n",
      "[-422.59571798]\n",
      "[-422.57863938]\n",
      "[-422.56278085]\n",
      "[-422.54800902]\n",
      "[-422.53421334]\n",
      "[-422.52130135]\n",
      "[-422.50919479]\n",
      "[-422.49782649]\n",
      "[-422.487138]\n",
      "[-422.47707772]\n",
      "[-422.46759947]\n",
      "[-422.45866145]\n",
      "[-422.45022541]\n",
      "[-422.44225609]\n",
      "[-422.43472076]\n",
      "[-422.42758891]\n",
      "[-422.42083203]\n"
     ]
    }
   ],
   "source": [
    "LL = hm.train_EM(y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108b3a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(LL)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-16313.90614625 -16312.4853005  -16313.85474093 ..., -16314.22026662\n",
      "  -16311.45681055 -16311.54475846]\n",
      " [-16311.73271192 -16311.95712853 -16311.51508649 ..., -16311.4639602\n",
      "  -16317.12007238 -16313.58930946]\n",
      " [-16313.00790242 -16313.82798753 -16315.2308311  ..., -16318.09774604\n",
      "  -16314.379592   -16315.23317637]]\n",
      "[[-16313.90614625 -16312.4853005  -16313.85474094 ..., -16314.22026662\n",
      "  -16311.45681055 -16311.54475846]\n",
      " [-16311.73271192 -16311.95712853 -16311.51508649 ..., -16311.4639602\n",
      "  -16317.12007238 -16313.58930946]\n",
      " [-16313.00790242 -16313.82798753 -16315.2308311  ..., -16318.09774604\n",
      "  -16314.379592   -16315.23317637]]\n"
     ]
    }
   ],
   "source": [
    "print(log_gamma)\n",
    "\n",
    "print(log_gamma_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating parameters of an homogeneous Markov chain\n",
    "$\\newcommand{\\ind}[1]{\\left[{#1}\\right]}$\n",
    "\n",
    "We have a Markov chain with transition probabilities $p(x_t = i| x_{t-1} = j) =  A_{i,j}$\n",
    "and initial state $p(x_1) = \\pi_i$.\n",
    "\n",
    "The distributions are\n",
    "\\begin{eqnarray}\n",
    "p(x_1 |\\pi)& = &\\prod_{s=1}^{S} \\pi_s^{\\ind{s = x_1}} \\\\\n",
    "p(x_t | x_{t-1}, A) &=& \\prod_{j=1}^{S} \\prod_{s=1}^{S}  A_{s,j}^{{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "The loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A) & = & \\log \\left( p(x_1 | \\pi) \\prod_{t=2}^T p(x_t | x_{t-1}, A) \\right) \\\\\n",
    "& = & \\sum_{s=1}^{S} {\\ind{s = x_1}} \\log \\pi_s + \\sum_{t=2}^T \\sum_{j=1}^{S}\\sum_{s=1}^{S} {{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\log A_{s,j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We have the constraints $\\sum_s \\pi_s = 1$ and $\\sum_i A_{i,j} = 1$ for all $j=1 \\dots S$ so we have $S+1$ constraints. We write the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A) & = & \\sum_{s=1}^{S} {\\ind{s = x_1}} \\log \\pi_s + \\sum_{t=2}^T \\sum_{j=1}^{S} \\sum_{s=1}^{S} {{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\log A_{s,j} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To find $\\pi$ and $A$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A)}{\\partial \\pi_i} & = & {\\ind{i = x_1}} \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)}{\\partial A_{i,j}} & = & \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{A_{i,j}} - \\lambda^A_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "Substitute the constraints $\\sum_s \\pi_s = 1$ and $\\sum_s A_{s,j} = 1, \\; j=1\\dots S$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & {\\ind{i = x_1}} \\frac{1}{\\lambda^\\pi} \\\\\n",
    "\\sum_i \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i {\\ind{i = x_1}} = 1\\\\\n",
    "\\lambda^\\pi & = & 1\\\\\n",
    "\\pi_i & = & {\\ind{i = x_1}}\n",
    "\\end{eqnarray}\n",
    "As we have effectively only a single observation for $x_1$, we have a crisp estimate.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_{t=2}^T \\sum_i  {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\\\\n",
    "A_{i,j} & = & \\frac{\\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}{\\sum_{t=2}^T \\sum_i  {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}\\\\\n",
    "& = & \\frac{\\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}{\\sum_{t=2}^T \\ind{j = x_{t-1}}}\n",
    "\\end{eqnarray}\n",
    "The result is intuitive. The denominator counts the number of times the chain visited state $j$ in the previous state. The numerator counts the number of times we visit $i$ given we were at $j$ the previous time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating parameters of an homogeneous Markov chain when several sequences are observed\n",
    "\n",
    "Suppose we have observed several sequences\n",
    "\\begin{eqnarray}\n",
    "X = \\{x_1^{(n)}, x_2^{(n)}, \\dots, x_{T_n}^{(n)}   \\}\n",
    "\\end{eqnarray}\n",
    "for $n = 1\\dots N$. Here $T_n$ is the length of the $n$'th sequence.\n",
    "\n",
    "The notation becomes slightly more complicated but conceptully the derivation is similar.\n",
    "\n",
    "The joint probability of all sequences is\n",
    "\\begin{eqnarray}\n",
    "p(X | \\pi, A) & = & \\prod_n \\left( p(x_1^{(n)}) \\prod_{t=2}^{T_n} p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right)\n",
    "\\end{eqnarray}\n",
    "The loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A) & = & \\sum_n \\left( \\log p(x_1^{(n)}) + \\sum_{t=2}^{T_n} \\log p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right) \\\\\n",
    "& = & \\sum_n \\left( \\sum_{s=1}^{S} {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_{t=2}^{T_n}\\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S} {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} \\right) \\\\\n",
    "& = & \\sum_{s=1}^{S} \\sum_n  {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_n \\sum_{t=2}^{T_n}\n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S}   {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "We write the Lagrangian\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)\n",
    "& = & \\sum_{s=1}^{S} \\sum_n  {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_n  \\sum_{t=2}^{T_n} \\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S}  {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $\\pi$ and $A$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A)}{\\partial \\pi_i} & = & \\sum_n {\\ind{i = x_1^{(n)}}} \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)}{\\partial A_{i,j}} & = &  \\sum_n \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{A_{i,j}} - \\lambda^A_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Prior\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\sum_n {\\ind{i = x_1^{(n)}}} \\frac{1}{\\lambda^\\pi}\\\\\n",
    "\\sum \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i \\sum_n {\\ind{i = x_1^{(n)}}} = 1 \\\\\n",
    "\\lambda^\\pi & = & \\sum_i \\sum_n {\\ind{i = x_1^{(n)}}} = N \\\\\n",
    "\\pi_i & = & \\frac{1}{N} \\sum_n {\\ind{i = x_1^{(n)}}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Transition Matrix\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & \\sum_n \\sum_{t=2}^{T_n}  {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i \\sum_n \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_i \\sum_n  \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\\\\n",
    "& = &  \\sum_n \\sum_{t=2}^{T_n}  \\ind{j = x_{t-1}^{(n)}} \\\\\n",
    "A_{i,j} & = &  \\frac{\\sum_n \\sum_{t=2}^{T_n}  {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}}}{\\sum_n \\sum_{t=2}^{T_n}  {\\ind{j = x_{t-1}^{(n)}}}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The EM Algorithm\n",
    "\n",
    "$\\newcommand{\\E}[1]{\\left\\langle#1\\right\\rangle}$\n",
    "\n",
    "The EM algorithm is a standart approach for ML estimation, when we have hidden variables.\n",
    "The canonical model is $p(y, x| \\theta)$ where we observe only $y$.\n",
    "\n",
    "The observed data loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & = & \\log p(y| \\theta) = \\log \\sum_x p(y, x| \\theta)\n",
    "\\end{eqnarray}\n",
    "\n",
    "The key to the EM algorithm is the Jensen's inequality, that states for a concave function $f$ we have for $0 \\leq \\lambda \\leq 1$\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f(\\lambda x_1 + (1 - \\lambda) x_2) \\geq \\lambda f( x_1) + (1 - \\lambda) f(x_2)\n",
    "\\end{eqnarray}\n",
    "\n",
    "In words the value of a function evaluated at the convex combination (lhs) is always equal and larger then the convex combination of the function values. As mathematical expectation\n",
    "\\begin{eqnarray}\n",
    "\\E{f(x)} & = & \\sum_x p(x) f(x) \\\\\n",
    "\\sum_x p(x) & = & 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "As $\\log(x)$  is a concave function, we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    " f(\\E{x}) & \\geq & \\E{f(x)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The key idea of the EM algorithm is to lower bound the observed data likelihood an maximize the bound with respect to the parameters. We take any distribution $q(x)$\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & = & \\log \\sum_x p(y, x| \\theta) \\\\\n",
    "& = & \\log \\sum_x p(y, x| \\theta) \\frac{q(x)}{q(x)} \\\\\n",
    "& = & \\log  \\E{\\frac{p(y, x| \\theta)}{q(x)}}_{q(x)}\\\\\n",
    "& \\geq & \\E{\\log {p(y, x| \\theta)}}_{q(x)} -\\E{\\log{q(x)} }_{q(x)}\\\\\n",
    "\\end{eqnarray}\n",
    "For _any_ $q(x)$, we have a lower bound. The natural strategy here is to choose the $q(x)$ that will maximise the lower bound. This is an optimisation problem. To make the notation more familiar, we let $q(x = i) = q_i$. Then, we arrive at the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(q, \\lambda) & = & \\sum_i q_i \\log p(y, x=i| \\theta) - \\sum_i q_i \\log q_i \\\\\n",
    "& & + \\lambda (1 - \\sum_i q_i)\n",
    "\\end{eqnarray}\n",
    "We take the derivative with respect to $q_i$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(q, \\lambda)}{\\partial q_k} & = & \\log p(y, x=k| \\theta) - (\\log q_k + 1) - \\lambda = 0\\\\\n",
    "\\log q_k  & = & \\log p(y, x=k| \\theta) -1 - \\lambda \\\\\n",
    "q_k  & = & p(y, x=k| \\theta) \\exp(-1 - \\lambda) \\\\\n",
    "\\sum_k q_k & = & \\exp(-1 - \\lambda) \\sum_k p(y, x=k| \\theta) = 1\\\\\n",
    "\\exp(1 + \\lambda) & = & p(y | \\theta) \\\\\n",
    "\\exp(-1 - \\lambda) & = & 1/p(y | \\theta) \\\\\n",
    "\\end{eqnarray}\n",
    "hence we have\n",
    "\\begin{eqnarray}\n",
    "q_k  & = & p(y, x=k| \\theta)/p(y | \\theta) = p(x=k| y \\theta)\n",
    "\\end{eqnarray}\n",
    "This result shows that the best we can do is to choose the posterior distribution\n",
    "\\begin{eqnarray}\n",
    "q(x) & = & p(x| y, \\theta)\n",
    "\\end{eqnarray}\n",
    "The EM algorithm is an iterative algorithm that exploits this bound result. Given a particular parameter setting $\\theta^{(\\tau)}$ at iteration $\\tau$, we can compute a lower bound of the true likelihood function.\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & \\geq & \\E{\\log {p(y, x| \\theta)}}_{p(x| y, \\theta^{(\\tau)})} -\\E{\\log p(x| y, \\theta^{(\\tau)}) }_{p(x| y, \\theta^{(\\tau)})}\\\\\n",
    "& \\equiv & {\\cal F}[\\theta; \\theta^{(\\tau)}] +  H[p(x| y, \\theta^{(\\tau)})]\n",
    "\\end{eqnarray}\n",
    "We need to show that\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta^{(\\tau)}) & = & \\E{\\log {p(y, x| \\theta)}}_{p(x| y, \\theta^{(\\tau)})} -\\E{\\log p(x| y, \\theta^{(\\tau)}) }_{p(x| y, \\theta^{(\\tau)})}\\\\\n",
    "& = & {\\cal F}[\\theta^{(\\tau)}; \\theta^{(\\tau)}] +  H[p(x| y, \\theta^{(\\tau)})]\n",
    "\\end{eqnarray}\n",
    "In other words, the bound is tight at $\\theta^{(\\tau)}$, hence maximizing the bound guarantees maximization of the true loglikelihoood.\n",
    "\n",
    "In most cases, where the EM algorithm can be applied, the joint distribution is from an {\\it exponential family}, i.e., it has the generic algebraic form\n",
    "\\begin{eqnarray}\n",
    "p(y, x| \\theta) & = & b(y, x)\\exp( \\sum_l \\phi_l(y, x) \\psi(\\theta_l) - A(\\theta)   )\n",
    "\\end{eqnarray}\n",
    "where $\\phi_l$ are the sufficient statistics and $\\psi(\\theta_l)$ are the {\\it canonical} parameters. The canonical parameters are in one to one relation with a conventional parametrisation. We will give several explicit examples when we cover the HMM's of the next section.\n",
    "\n",
    "In the case when the complete data likelihood is an exponential family, the computation of the bound requires the expectation\n",
    "\\begin{eqnarray}\n",
    "\\E{\\log p(y, x| \\theta)} & = & \\E{\\log b(x, y)} + \\sum_l \\E{\\phi_l(y, x)} \\psi(\\theta_l) - A(\\theta)\n",
    "\\end{eqnarray}\n",
    "where the expectation is taken with respect to the posterior $p(x|y, \\theta^{(\\tau)})$. In other words, we need to compute expectations of form $\\E{\\phi_l(y, x)}$. Once these are available, we have effectively an expression for ${\\cal F}(\\theta; \\theta^{(\\tau)})$. By maximisation of ${\\cal F}$ with respect to $\\theta$,  and arrive at $\\theta^{(\\tau + 1)}$ and complete the iteration.\n",
    "\n",
    "In a rather abstract sense, the EM algorithm proceeds as follows:\n",
    "\n",
    "##### The Expectation/Maximization (EM) algorithm.\n",
    "\n",
    "\\begin{algorithmic}\n",
    "\\STATE Initialise $\\theta^{(0)}$\n",
    "\\FOR{  $\\text{epoch} = 1 \\dots $  MAXITER}\n",
    "\\STATE E-step. Compute the sufficient statistics of the complete data likelihood\n",
    "\\STATE M-step. Maximize with respect to the parameters $\\theta$ to find $\\theta^{(\\tau + 1)}$\n",
    "\\ENDFOR\n",
    "\\end{algorithmic}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning HMM parameters by EM\n",
    "\n",
    "Suppose we have observed several sequences\n",
    "\\begin{eqnarray}\n",
    "Y = \\{y_1^{(n)}, y_2^{(n)}, \\dots, y_{T_n}^{(n)}   \\}\n",
    "\\end{eqnarray}\n",
    "for $n = 1\\dots N$. Here $T_n$ is the length of the $n$'th sequence.\n",
    "Let $Y \\in \\{1,\\dots, R\\}$ and $X \\in \\{1,\\dots, S \\}$.\n",
    "\n",
    "The discrete observation, discrete state space HMM has the following factors:\n",
    "\\begin{eqnarray}\n",
    "p(x_1 = i) & = & \\pi_i \\\\\n",
    "p(y_k = r| x_k = i) & = & B_{r,i} \\\\\n",
    "p(x_k = i| x_{k-1} = j) & = & A_{i,j} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The joint probability of all observed sequences and corresponding hidden sequences are\n",
    "\\begin{eqnarray}\n",
    "p(Y, X | \\pi, A, B) & = & \\prod_n \\left( p(x_1^{(n)}) p(y_1^{(n)} | x_1^{(n)})  \\prod_{t=2}^{T_n} p(y_t^{(n)} | x_t^{(n)}) p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right)\n",
    "\\end{eqnarray}\n",
    "The expected complete data loglikelihood is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A, B) & = & \\E{\\sum_n \\left( \\log p(x_1^{(n)}) + \\sum_{t=1}^{T_n} \\log p(x_t^{(n)}| x_{t-1}^{(n)} ) + \\sum_{t=2}^{T_n} \\log p(y_t^{(n)} | x_t^{(n)}) \\right)} \\\\\n",
    "& = & \\E{\\sum_n \\left( \\sum_{s=1}^{S} {\\ind{s = x_1^{(n)}}} \\log \\pi_s   + \\log \\pi_s \\sum_{t=2}^{T_n}\\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S} {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} + \\sum_{t=1}^{T_n}\\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} {\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}} \\log B_{r,i}\\right)} \\\\\n",
    "& = & \\sum_n \\sum_{s=1}^{S}   \\E{{\\ind{s = x_1^{(n)}}}} \\log \\pi_s  + \\sum_n \\sum_{t=2}^{T_n}\n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S}   \\E{{\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\log A_{s,j} + \\sum_n \\sum_{t=1}^{T_n}\\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} \\E{{\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}}} \\log B_{r,i} \\\\\n",
    "& = & \\sum_{s=1}^{S} \\underbrace{\\left( \\sum_n \\E{ {\\ind{s = x_1^{(n)}}}} \\right)}_{\\equiv C_1} \\log \\pi_s  + \n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S} \\underbrace{\\left(  \\sum_n \\sum_{t=2}^{T_n} \\E{{\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\right)}_{\\equiv C_2} \\log A_{s,j} + \\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} \\underbrace{\\left(  \\sum_n \\sum_{t=1}^{T_n} \\E{{\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}}} \\right)}_{\\equiv C_3} \\log B_{r,i} \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The M-Step\n",
    "We write the Lagrangian to ensure that the columns of $A$ and $B$ are positive and normalized\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A, \\lambda^B)\n",
    "& = & \\sum_{s=1}^{S} C_1(s) \\log \\pi_s + \\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S}  C_2(s,j) \\log A_{s,j} + \\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} C_3(r,i) \\log B_{r,i} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "+ \\sum_i \\lambda^B_i \\left( 1 - \\sum_r B_{r,i} \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To find $\\pi$, $A$ and $B$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial \\pi_i} & = & C_1(i) \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial A_{i,j}} & = &  C_2(i,j) \\frac{1}{A_{i,j}} - \\lambda^A_j = 0 \\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, B, \\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial B_{k,i}} & = &  C_3(k,i) \\frac{1}{B_{k,i}} - \\lambda^B_i = 0 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Prior\n",
    "We set the derivative to zero and solve for $\\pi$\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & C_1(i) \\frac{1}{\\lambda^\\pi}\n",
    "\\end{eqnarray}\n",
    "As we have the normalization constraint for $\\pi$, we also have the following equality from which we can solve for the Lagrange multiplier:\n",
    "\\begin{eqnarray}\n",
    "\\sum_i \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i C_1(i) = 1 \\\\\n",
    "\\lambda^\\pi & = & \\sum_i  C_1(i)  = N \n",
    "\\end{eqnarray}\n",
    "Substituting, we obtain the intuitive answer:\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\frac{1}{N} C_1(i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Transition Matrix\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & C_2(i,j) \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i C_2(i,j) \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_i C_2(i,j) \\\\\n",
    "A_{i,j} & = &  \\frac{C_2(i,j)}{\\sum_i C_2(i,j)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Observation Matrix\n",
    "\\begin{eqnarray}\n",
    "B_{k,i} & = & C_3(k,i) \\frac{1}{\\lambda^B_i} \\\\\n",
    "\\sum_k B_{k,i} & = & \\sum_k C_3(k,i) \\frac{1}{\\lambda^B_i} = 1 \\\\\n",
    "\\lambda^B_i & = & \\sum_k C_3(k,i) \\\\\n",
    "B_{k,i} & = &  \\frac{C_3(k,i)}{\\sum_k C_3(k,i)}\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "y = np.array([0, 1, 3, 2, 4])\n",
    "\n",
    "hm = HMM(p, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward smoothers\n",
    "\n",
    "The EM algorithm requires obtaining the sufficient statistics of an HMM.  \n",
    "\n",
    "The key observation is that the sufficient statistics are additive:\n",
    "\\begin{eqnarray}\n",
    "C_t & = & \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k}) \\right) p(x_{1:t}|y_{1:t}) dx_{1:t}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We will use this observation as the basis of a forward recursion. First we decompose the posterior \n",
    "as a product of the filtering density at time $t$ and a conditional quantity familiar from the correction smoother \n",
    "\\begin{eqnarray}\n",
    "C_t & = & \\int \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k})\\right) p(x_{1:t-1}|y_{1:t},x_t) p(x_{t}|y_{1:t}) dx_{1:t-1} dx_t \\\\\n",
    "& = & \\int \\underbrace{\\left( \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k})\\right) p(x_{1:t-1}|y_{1:t-1},x_t) dx_{1:t-1} \\right)}_{=V_t(x_t)} p(x_{t}|y_{1:t})  dx_t\n",
    "\\end{eqnarray}\n",
    "\n",
    "Due to additivity, we can decompose further\n",
    "\\begin{eqnarray}\n",
    "V_t(x_t) & = & \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-1}|y_{1:t-1}, x_t) dx_{1:t-1} \\\\\n",
    "& = & \\int \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-2}|y_{1:t-1}, x_{t-1}, x_t) p(x_{t-1}|y_{1:t-1}, x_t) dx_{1:t-2} dx_{t-1} \\\\\n",
    "& = & \\int \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-2}|y_{1:t-2}, x_{t-1}) p(x_{t-1}|y_{1:t-1}, x_t) dx_{1:t-2} dx_{t-1} \\\\\n",
    "& = & \\int \\left( s_t(x_{t-1}, x_{t}) + \\int \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k}) p(x_{1:t-2}|y_{1:t-2}, x_{t-1}) dx_{1:t-2}  \\right)  p(x_{t-1}|y_{1:t-1}, x_t)  dx_{t-1} \\\\\n",
    "& = & \\int \\left( s_t(x_{t-1}, x_{t}) + V_{t-1}(x_{t-1})  \\right)  p(x_{t-1}|y_{1:t-1}, x_t)  dx_{t-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "V_1(x_1) & = & 0 \\\\\n",
    "V_2(x_2) & = & \\int  s_2(x_{1}, x_{2})   p(x_{1}|y_{1}, x_2)  dx_{1} \\\\\n",
    "V_3(x_3) & = & \\int \\left( s_3(x_{2}, x_{3}) + V_{2}(x_{2})  \\right)  p(x_{2}|y_{1:2}, x_3)  dx_{2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above recursion when applied to the sufficient statistics of an HMM has the following specific form:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "C_2(a,b) &=& \\sum_{x_4} \\sum_{x_3} \\sum_{x_2} \\left(\\sum_{x_1} {\\ind{a = x_2}}\\ind{b = x_{1}} p(x_1|y_1, x_2) + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_2|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4})\\\\\n",
    "&=& \\sum_{x_4}\\sum_{x_3} \\sum_{x_2} \\left( \\underbrace{{\\ind{a = x_2}} p(x_1=b|y_1, x_2)}_{V_2(x_2)} + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_2|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "&=&  \\sum_{x_4}\\sum_{x_3} \\left( \\underbrace{p(x_1=b|y_1, x_2=a) p(x_2=a|y_{1,2}, x_3) + {\\ind{a = x_3}} p(x_2=b|y_{1,2}, x_3)}_{V_3(x_3)} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "&=&  \\sum_{x_4} \\left(p(x_1=b|y_1, x_2=a)\\sum_{x_3} p(x_2=a|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) + p(x_2=b|y_{1,2}, x_3=a) p(x_3=a|y_{1,3}, x_4) \\\\\n",
    "+ {\\ind{a = x_4}} p(x_3=b |y_{1,3}, x_4) \\right)  p(x_4|y_{1,4}) \\\\\n",
    "&=& p(x_1=b|y_1, x_2=a)\\sum_{x_3} p(x_2=a|y_{1,2}, x_3) \\sum_{x_4} p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "& & + p(x_2=b|y_{1,2}, x_3=a) \\sum_{x_4} p(x_3=a|y_{1,3}, x_4)p(x_4|y_{1,4}) \\\\\n",
    "& & + p(x_3=b |y_{1,3}, x_4=a) p(x_4=a|y_{1,4})  \n",
    "\\end{eqnarray}\n",
    "\n",
    "One can verify, that the last line is indeed equal to the required sufficient statistics given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_2(a,b) & = &  \\sum_{x_{1:T}} \\sum_{t=2}^T \\ind{a = x_t}\\ind{b = x_{t-1}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = & \\sum_{x_{2:T}} \\left(\\sum_{x_{1}} \\ind{a = x_2}\\ind{b = x_{1}} p(x_{1}|y_{1},x_{2}) + \\sum_{t=3}^T \\ind{a = x_t}\\ind{b = x_{t-1}}  \\right) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "V_{2,2}(a, b, x_2) & = & \\ind{a = x_2} p(x_{1} = b |y_{1},x_{2}) \\\\\n",
    "C_2(a,b) & = & \\sum_{x_{3:T}} \\left( \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\sum_{x_2} \\ind{a = x_3}\\ind{b = x_{2}} p(x_{2}|y_{1:2},x_{3})  + \\sum_{t=4}^T \\ind{a = x_t}\\ind{b = x_{t-1}}  \\right) \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "V_{2,3}(a, b, x_3) & = & \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\sum_{x_2} \\ind{a = x_3}\\ind{b = x_{2}} p(x_{2}|y_{1:2},x_{3}) \\\\\n",
    "& = & \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = x_3} p(x_{2}=b|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update rule is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "V_{2,t}(a, b, x_t) & = & \\sum_{x_{t-1}} V_{2,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\sum_{x_{t-1}} \\ind{a = x_t}\\ind{b = x_{t-1}} p(x_{t-1}|y_{1:t-1},x_{t}) \\\\\n",
    "& = & \\sum_{x_{t-1}} V_{2,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\ind{a = x_t} p(x_{t-1}=b|y_{1:t-1},x_{t})\n",
    "\\end{eqnarray}\n",
    "\n",
    "The advantage of this algorithm is that it is entirely forward and has attractive space properties, requiring only $S^3 + 2S$ space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_3(a,b) & = &  \\sum_{x_{1:T}} \\sum_{t=1}^T \\ind{a = y_t}\\ind{b = x_{t}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = &  \\sum_{x_{2:T}} \\left(\\ind{a = y_1} p(x_{1}=b|y_{1},x_{2}) + \\sum_{t=2}^T \\ind{a = y_t}\\ind{b = x_{t}} \\right) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{3,2}(a, b, x_2)  & = & \\ind{a = y_1} p(x_{1}=b|y_{1},x_{2}) \\\\\n",
    "C_3(a,b) & = &  \\sum_{x_{3:T}} \\left(\\sum_{x_2} V_{3,2}(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = y_2} p(x_{2}=b|y_{1:2},x_{3}) + \\sum_{t=3}^T \\ind{a = y_t}\\ind{b = x_{t}} \\right) \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{3,3}(a, b, x_3)  & = & \\sum_{x_2} V_{3,2}(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = y_2} p(x_{2}=b|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update rule is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "V_{3,t}(a, b, x_t) & = & \\sum_{x_{t-1}} V_{3,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\ind{a = y_t} p(x_{t-1}=b|y_{1:t-1},x_{t})\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_1(a) & = &  \\sum_{x_{1:T}} \\ind{a = x_{1}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = &  \\sum_{x_{2:T}} p(x_{1}=a|y_{1},x_{2}) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{1,2}(a, x_2)  & = & p(x_{1}=a|y_{1},x_{2}) \\\\\n",
    "C_1(a) & = &  \\sum_{x_{3:T}} \\sum_{x_2} V_{1,2}(a, x_2) p(x_{2}|y_{1:2},x_{3})  \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{1,3}(a, x_3)  & = & \\sum_{x_2} V_{1,2}(a, x_2) p(x_{2}|y_{1:2},x_{3})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " An implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55766819]\n",
      " [ 0.41471831]\n",
      " [ 0.0276135 ]]\n",
      "1.0\n",
      "[[ 0.25945937  0.00608506  0.6788905 ]\n",
      " [ 0.61402584  3.54249044  3.36225566]\n",
      " [ 0.27474163  3.89013632  0.37191517]]\n",
      "13.0\n",
      "[[ 0.06156212  1.20205124  1.73638664]\n",
      " [ 1.06898349  2.33689306  0.59412344]\n",
      " [ 0.01327825  1.00608493  1.98063682]\n",
      " [ 0.00440298  2.89368259  0.10191443]\n",
      " [ 0.35387629  0.49477843  0.15134528]]\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "logA = np.log(A)\n",
    "logB = np.log(B)\n",
    "\n",
    "hm = HMM(p, A, B)\n",
    "\n",
    "y = np.array([1, 1, 1, 3, 2, 0, 3, 2, 1, 0, 0, 3, 2, 4])\n",
    "T = y.shape[0]\n",
    "\n",
    "# Forward only estimation of sufficient statistics\n",
    "V1  = np.eye((S))\n",
    "V2  = np.zeros((S,S,S))\n",
    "V3  = np.zeros((R,S,S))\n",
    "I_S1S = np.eye(S).reshape((S,1,S))\n",
    "I_RR = np.eye(R)\n",
    "\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred = predict(A, log_alpha)\n",
    "    \n",
    "    if k>0:\n",
    "        # Calculate p(x_{k-1}|y_{1:k-1}, x_k) \n",
    "        lp = np.log(normalize_exp(log_alpha)).reshape(S,1) + logA.T    \n",
    "        P = normalize_exp(lp, axis=0)\n",
    "        \n",
    "        # Update\n",
    "        V1 = np.dot(V1, P)             \n",
    "        V2 = np.dot(V2, P) + I_S1S*P.reshape((1,S,S))    \n",
    "        V3 = np.dot(V3, P) + I_RR[:,y[k-1]].reshape((R,1,1))*P.reshape((1,S,S))    \n",
    "        \n",
    "    log_alpha = update(y[k], logB, log_alpha_pred)    \n",
    "    p_xT = normalize_exp(log_alpha)    \n",
    "    \n",
    "C1 = np.dot(V1, p_xT.reshape(S,1))\n",
    "C2 = np.dot(V2, p_xT.reshape(1,S,1)).reshape((S,S))\n",
    "C3 = np.dot(V3, p_xT.reshape(1,S,1)).reshape((R,S))\n",
    "C3[y[-1],:] +=  p_xT\n",
    "    \n",
    "\n",
    "print(C1)\n",
    "print(np.sum(C1))\n",
    "\n",
    "print(C2)\n",
    "print(np.sum(C2))\n",
    "\n",
    "print(C3)\n",
    "print(np.sum(C3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "V_1(x_1) & = & 0 \\\\\n",
    "V_2(x_2) & = & \\int  s_2(x_{1}, x_{2})   p(x_{1}|y_{1}, x_2)  dx_{1} \\\\\n",
    "V_3(x_3) & = & \\int \\left( s_3(x_{2}, x_{3}) + V_{2}(x_{2})  \\right)  p(x_{2}|y_{1:2}, x_3)  dx_{2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.55766819  0.41471831  0.0276135 ]\n",
      "1.0\n",
      "[[ 0.25945937  0.00608506  0.6788905 ]\n",
      " [ 0.61402584  3.54249044  3.36225566]\n",
      " [ 0.27474163  3.89013632  0.37191517]]\n",
      "13.0\n",
      "[[ 0.06156212  1.20205124  1.73638664]\n",
      " [ 1.06898349  2.33689306  0.59412344]\n",
      " [ 0.01327825  1.00608493  1.98063682]\n",
      " [ 0.00440298  2.89368259  0.10191443]\n",
      " [ 0.35387629  0.49477843  0.15134528]]\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "lg, C1_corr, C2_corr, C3_corr = hm.correction_smoother(y)\n",
    "\n",
    "print(C1_corr)\n",
    "print(np.sum(C1_corr))\n",
    "\n",
    "print(C2_corr)\n",
    "print(np.sum(C2_corr))\n",
    "\n",
    "print(C3_corr)\n",
    "print(np.sum(C3_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"stdin_port\": 52422, \n",
      "  \"ip\": \"127.0.0.1\", \n",
      "  \"control_port\": 52423, \n",
      "  \"hb_port\": 52424, \n",
      "  \"signature_scheme\": \"hmac-sha256\", \n",
      "  \"key\": \"b381430f-be59-444c-beb0-e75c64b6c970\", \n",
      "  \"shell_port\": 52420, \n",
      "  \"transport\": \"tcp\", \n",
      "  \"iopub_port\": 52421\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> ipython <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> ipython <app> --existing kernel-8739c958-47db-4b42-a280-a39582b02bdd.json \n",
      "or even just:\n",
      "    $> ipython <app> --existing \n",
      "if this is the most recent IPython session you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
