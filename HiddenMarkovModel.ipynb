{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "### Ali Taylan Cemgil, Bogazici University\n",
    "\n",
    "The latex equations on the Github version do not render well. Please work on a clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import pygraphviz\n",
    "import pyparsing\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from IPython.display import Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4sAAADVCAYAAAD+f2V5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYREAAAAA4MCwCAAAAABw+H/S3XMlsOzHiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a6af750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def makeDBN(inter, intra, T, labels):\n",
    "    \"\"\"Unfold a graph for T time slices\"\"\"\n",
    "    N = max(max([i for i,j in inter]),max([j for i,j in inter]))+1\n",
    "\n",
    "    G = np.zeros((N*T,N*T))\n",
    "    pos = []\n",
    "    all_labels = []\n",
    "    for n in range(N):\n",
    "        pos.append((0,-n))\n",
    "        all_labels.append('$'+labels[n]+'_{'+str(0+1)+\"}\"+'$')\n",
    "        \n",
    "    for e in inter:\n",
    "        s,d = e\n",
    "        G[s,d] = 1\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for n in range(N):\n",
    "            pos.append((t,-n))\n",
    "            all_labels.append('$'+labels[n]+'_{'+str(t+1)+\"}\"+'$')\n",
    "\n",
    "        for e in inter:\n",
    "            s,d = e\n",
    "            s = s + N*t\n",
    "            d = d + N*t\n",
    "            G[s,d] = 1\n",
    "        \n",
    "        for e in intra:\n",
    "            s,d = e\n",
    "            s = s + N*(t-1)\n",
    "            d = d + N*t\n",
    "            G[s,d] = 1\n",
    "    return G,pos,all_labels\n",
    "\n",
    "#inter = [(0,1),(1,2),(2,3)]\n",
    "#intra = [(0,0),(1,1),(0,1),(0,2)]\n",
    "#variable_names = [\"r\",\"z\",\"x\", \"y\"] \n",
    "inter = [(0,1)]\n",
    "intra = [(0,0)]\n",
    "variable_names = [\"x\", \"y\"] \n",
    "T = 5\n",
    "\n",
    "A, pos, label_list = makeDBN(inter, intra, T, variable_names)\n",
    "\n",
    "G = nx.DiGraph(A)\n",
    "labels = {i: s for i,s in enumerate(label_list)}\n",
    "plt.figure(figsize=(12,2.5))\n",
    "nx.draw(G, pos, node_color=\"white\", node_size=2500, labels=labels, font_size=24, arrows=True)\n",
    "#nx.draw_graphviz(G,node_size=500, labels=labels, font_size=24, arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}) & = & \\sum_{x_{1:K}} p(y_{1:K}|x_{1:K}) p(x_{1:K}) \\\\\n",
    "& = &  \\underbrace{\\sum_{x_K} p(y_K | x_K ) \\sum_{x_{K-1}} p(x_K|x_{K-1})  \\dots \\sum_{x_{2}} p(x_3|x_{2})\n",
    "\\underbrace{ p(y_{2}|x_{2}) \\overbrace{ \\sum_{x_{1}} p(x_2|x_{1})\n",
    "\\underbrace{ p(y_{1}|x_{1}) \\overbrace{ p(x_1)}^{\\alpha_{1|0}}}_{\\alpha_{1|1}}\n",
    "}^{\\alpha_{2|1}} }_{\\alpha_{2|2}}}_{\\alpha_{K|K}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1|0} & \\equiv & p(x_1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{k|k} & \\equiv & p(y_{1:k}, x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{k|k-1}  & \\equiv & p(y_{1:k-1}, x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "For $k=1, 2, \\dots, K$\n",
    "\n",
    "__Predict__\n",
    "\n",
    "$k=1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1|0}(x_1) = p(x_1)\n",
    "\\end{eqnarray}\n",
    "\n",
    "$k>1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\alpha_{k|k-1}(x_k)} & = & p(y_{1:k-1}, x_k) = \\sum_{x_{k-1}} p(x_k| x_{k-1}) p(y_{1:k-1}, x_{k-1}) \\\\\n",
    "& = & \\sum_{x_{k-1}} p(x_k| x_{k-1}) { \\alpha_{k-1|k-1}(x_{k-1}) }\n",
    "\\end{eqnarray}\n",
    "\n",
    "__Update__\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\alpha_{k|k}(x_k) } & = & p(y_{1:k}, x_k) = p(y_k | x_k) p(y_{1:k-1}, x_k) \\\\\n",
    " & = & p(y_k | x_k) {\\alpha_{k|k-1}(x_k)}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}) & = &  \\sum_{x_1} p(x_1) p(y_1 | x_1 )\n",
    "%underbrace{\\sum_{x_2} p(x_2|x_{1}) p(y_2 | x_2 )}_{\\beta_1}\n",
    "\\dots\n",
    "\\underbrace{ \\sum_{x_{K-1}} p(x_{K-1}|x_{K-2}) p(y_{K-1} | x_{K-1} )\n",
    "\\underbrace{ \\sum_{x_K} p(x_K|x_{K-1}) p(y_K | x_K )\n",
    "\\underbrace{{\\pmb 1}}_{\\beta_{K|K+1}}}_{\\beta_{K-1|K}}}_{\\beta_{K-2|K-1}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k+1}(x_k) & \\equiv & p(y_{k+1:K}| x_k) \\\\\n",
    "\\beta_{k|k}(x_k) & \\equiv & p(y_{k:K}| x_k)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "For $k=K, K-1, \\dots, 1$\n",
    "\n",
    "'Postdict' : (Backward Prediction)\n",
    "\n",
    "$k=K$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_{K|K+1}(x_K) & = & \\mathbf{1} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "$k<K$ \n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k+1}(x_k) & = & p(y_{k+1:K}| x_k) = \\sum_{x_{k+1}} p(x_{k+1}| x_{k}) p(y_{k+1:K}| x_{k+1}) \\\\\n",
    "& = & \\sum_{x_{k+1}} p(x_{k+1}| x_{k}) \\beta_{k+1|k+1}(x_{k+1}) \n",
    "\\end{eqnarray}\n",
    "\n",
    "Update\n",
    "\\begin{eqnarray}\n",
    "\\beta_{k|k}(x_k)  & = & p(y_{k:K}| x_k) = p(y_k | x_k) p(y_{k+1:K}| x_k) \\\\\n",
    " & = & p(y_k | x_k) {\\beta_{k|k+1}(x_k)}\n",
    "\\end{eqnarray}\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically Stable computation of $\\log(\\sum_i \\exp (l_i ) ))$\n",
    "\n",
    "Derivation\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L & = & \\log(\\sum_i \\exp (l_i) ) \n",
    " =   \\log(\\sum_i \\exp (l_i) \\frac{\\exp(l^*)}{\\exp(l^*)} ) \\\\\n",
    "& = &  \\log( \\exp(l^*) \\sum_i \\exp (l_i - l^*) ) \\\\\n",
    "& = &  l^* + \\log( \\sum_i \\exp (l_i - l^*) )\n",
    "\\end{eqnarray}\n",
    "\n",
    "Choose $l^*  =  \\max_i l_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Naive evaluation  :', -inf)\n",
      "('Numerically stable:', array([-1000.]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_sum_exp_naive(l):\n",
    "    return np.log(np.sum(np.exp(l)))\n",
    "\n",
    "def log_sum_exp(l, axis=0):\n",
    "    l_star = np.max(l, axis=axis, keepdims=True)\n",
    "    return l_star + np.log(np.sum(np.exp(l - l_star),axis=axis,keepdims=True)) \n",
    "    \n",
    "    \n",
    "l = np.array([-1000, -10000])\n",
    "\n",
    "print('Naive evaluation  :', log_sum_exp_naive(l))\n",
    "print('Numerically stable:', log_sum_exp(l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An implementation of the forward backward algorithm\n",
    "# For numerical stability, we calculate everything in the log domain\n",
    "\n",
    "def normalize(A, axis=None):\n",
    "    Z = np.sum(A, axis=axis,keepdims=True)\n",
    "    idx = np.where(Z == 0)\n",
    "    Z[idx] = 1\n",
    "    return A/Z\n",
    "\n",
    "def randgen(pr, N=1): \n",
    "    L = len(pr)\n",
    "    return np.random.choice(range(L), size=N, replace=True, p=pr)\n",
    "\n",
    "def predict(A, lp):\n",
    "    lstar = np.max(lp)\n",
    "    return lstar + np.log(np.dot(A,np.exp(lp-lstar)))\n",
    "\n",
    "def postdict(A, lp):\n",
    "    lstar = np.max(lp)\n",
    "    return lstar + np.log(np.dot(np.exp(lp-lstar), A))\n",
    "\n",
    "def update(y, logB, lp):\n",
    "    return logB[y,:] + lp\n",
    "\n",
    "# Generate Parameter\n",
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "logA = np.log(A)\n",
    "logB = np.log(B)\n",
    "\n",
    "# Generate Data\n",
    "\n",
    "# Number of steps\n",
    "T = 100\n",
    "\n",
    "x = np.zeros(T,int)\n",
    "y = np.zeros(T,int)\n",
    "for t in range(T):\n",
    "    if t==0:\n",
    "        x[t] = randgen(p)\n",
    "    else:\n",
    "        x[t] = randgen(A[:,x[t-1]])\n",
    "    \n",
    "    y[t] = randgen(B[:,x[t]])\n",
    "    \n",
    "# Forward Pass\n",
    "\n",
    "# Python indexes starting from zero so\n",
    "# log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "# log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "log_alpha  = np.zeros((S, T))\n",
    "log_alpha_pred = np.zeros((S, T))\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred[:,0] = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred[:,k] = predict(A, log_alpha[:,k-1])\n",
    "    \n",
    "    log_alpha[:,k] = update(y[k], logB, log_alpha_pred[:,k])\n",
    "    \n",
    "# Backward Pass\n",
    "log_beta  = np.zeros((S, T))\n",
    "log_beta_post = np.zeros((S, T))\n",
    "\n",
    "for k in range(T-1,-1,-1):\n",
    "    if k==T-1:\n",
    "        log_beta_post[:,k] = np.zeros(S)\n",
    "    else:\n",
    "        log_beta_post[:,k] = postdict(A, log_beta[:,k+1])\n",
    "    \n",
    "    log_beta[:,k] = update(y[k], logB, log_beta_post[:,k])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing - Forward Backward Algorithm (Two filter formulation)\n",
    "\\begin{eqnarray}\n",
    "p(y_{1:K}, x_k) & = & p(y_{1:k}, x_k) p(y_{k+1:K} | x_k) \\\\\n",
    "& = & {\\alpha_{k|k}(x_k) } {\\beta_{k|k+1}(x_k)} \\\\\n",
    "& \\equiv & \\gamma_k(x_k)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069\n",
      "  -124.23863069 -124.23863069 -124.23863069 -124.23863069 -124.23863069]]\n"
     ]
    }
   ],
   "source": [
    "# Smoother check\n",
    "# All numbers must be equal to the marginal likelihood p(y_{1:K})\n",
    "\n",
    "log_gamma = log_alpha + log_beta_post\n",
    "print(log_sum_exp(log_gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing (Forward filtering - Backward smoothing), The Correction Smoother\n",
    "\n",
    "Suppose we have computed the filtered quantities $p(x_t| y_{1:t})$ via the forward pass. The forward-backward algorithm requires us to store all observations. For batch settings, this is OK however when datapoints are arriving indeed sequentially, this may be not desired.  \n",
    "\n",
    "We will derive a recursive algorithm to compute the marginals $p(x_t | y_{1:T})$.\n",
    "\n",
    "Note that if we calculate instead the so-called __pairwise__ marginal $p(x_t, x_{t+1} | y_{1:T} )$, we can get by simple marginalization\n",
    "\n",
    "\\begin{align}\n",
    "p(x_t | y_{1:T}) & =  \\sum_{x_{t+1}} p(x_t, x_{t+1} | y_{1:T} )  & \\text{Definition} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "p(x_t, x_{t+1} | y_{1:T} ) & =   p(x_{t} |x_{t+1}, y_{1:t},y_{t+1:T} ) p(x_{t+1}|y_{1:T} ) & \\text{Factorize} \\\\\n",
    "& =   p(x_{t} |x_{t+1}, y_{1:t} ) p(x_{t+1}|y_{1:T} ) & \\text{Conditional Independence}\\\\\n",
    "  & =  \\frac{p(x_{t}, x_{t+1}| y_{1:t} )}{p(x_{t+1}| y_{1:t} )} p(x_{t+1}|y_{1:T} ) & \\text{Definition of Conditional} \n",
    "\\end{align}\n",
    "\n",
    "This update has the form:\n",
    "\\begin{eqnarray}\n",
    "\\text{New Pairwise Marginal}_{t,t+1} & = & \\frac{\\text{Old Pairwise Marginal}_{t,t+1}}{\\text{Old Marginal}_{t+1}} \\times {\\text{New Marginal}_{t+1}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "The old pairwise marginal can be simply calculated from the filtered marginals as\n",
    "\n",
    "\\begin{align}\n",
    "p(x_{t}, x_{t+1}| y_{1:t} ) & =  p(x_{t+1} | x_t,  y_{1:t} ) p(x_t | y_{1:t}) & \\text{Definition} \\\\\n",
    "& =  p(x_{t+1} | x_t ) p(x_t | y_{1:t}) & \\text{Conditional Independence} \\\\\n",
    "& = \\text{Transition Model} \\times \\text{Filtering distribution} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "The correction smoother calculates a factorisation of the posterior of form\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(x_{1:T}|y_{1:T}) & = & \\frac{\\prod_{t=1}^{T-1} p(x_{t}, x_{t+1} | y_{1:T}) }{ \\prod_{t=2}^{T-1} p(x_{t} | y_{1:T}) }\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAADbCAYAAAAcRUoHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b28c6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correction Smoother\n",
    "# For numerical stability, we calculate everything in the log domain\n",
    "log_gamma_corr = np.zeros_like(log_alpha)\n",
    "log_gamma_corr[:,T-1] = log_alpha[:,T-1]\n",
    "\n",
    "for k in range(T-2,-1,-1):\n",
    "    log_old_pairwise_marginal = log_alpha[:,k].reshape(1,S) + logA \n",
    "    log_old_marginal = predict(A, log_alpha[:,k])\n",
    "    log_new_pairwise_marginal = log_old_pairwise_marginal + log_gamma_corr[:,k+1].reshape(S,1) - log_old_marginal.reshape(S,1)\n",
    "    log_gamma_corr[:,k] = log_sum_exp(log_new_pairwise_marginal, axis=0).reshape(S)\n",
    "    \n",
    "\n",
    "# Verify that result coincide\n",
    "\n",
    "gam = normalize_exp(log_gamma, axis=0)\n",
    "gam_corr = normalize_exp(log_gamma_corr, axis=0)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(gam, interpolation='nearest')\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(gam_corr, interpolation='nearest')\n",
    "\n",
    "plt.show()\n",
    "#print(log_gamma)\n",
    "#print(log_gamma_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of the HMM in Python\n",
    "\n",
    "We will integrate filtering, smoothing and training functionality into an HMM object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "\n",
    "def randgen(pr, N=1): \n",
    "    L = len(pr)\n",
    "    return int(np.random.choice(range(L), size=N, replace=True, p=pr))\n",
    "\n",
    "def log_sum_exp(l, axis=0):\n",
    "    l_star = np.max(l, axis=axis, keepdims=True)\n",
    "    return l_star + np.log(np.sum(np.exp(l - l_star),axis=axis,keepdims=True)) \n",
    "\n",
    "def normalize_exp(log_P, axis=None):\n",
    "    a = np.max(log_P, keepdims=True, axis=axis)\n",
    "    P = normalize(np.exp(log_P - a), axis=axis)\n",
    "    return P\n",
    "\n",
    "def normalize(A, axis=None):\n",
    "    Z = np.sum(A, axis=axis,keepdims=True)\n",
    "    idx = np.where(Z == 0)\n",
    "    Z[idx] = 1\n",
    "    return A/Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 57.13529116  34.39155663  66.77777528]\n",
      " [ 35.48915946  25.11185721   1.85407212]\n",
      " [ 65.41369155   3.63554079   9.19105581]]\n",
      "[[ 57.13529116  34.39155663  66.77777528]\n",
      " [ 35.48915946  25.11185721   1.85407212]\n",
      " [ 65.41369155   3.63554079   9.19105581]]\n"
     ]
    }
   ],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self, pi, A, B):\n",
    "        # p(x_0)\n",
    "        self.pi = pi\n",
    "        # p(x_k|x_{k-1})\n",
    "        self.A = A\n",
    "        # p(y_k|x_{k})\n",
    "        self.B = B\n",
    "        # Number of possible latent states at each time\n",
    "        self.S = pi.shape[0]\n",
    "        # Number of possible observations at each time\n",
    "        self.R = B.shape[0]\n",
    "        self.logB = np.log(self.B)\n",
    "        self.logA = np.log(self.A)\n",
    "        self.logpi = np.log(self.pi)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_random_parameters(cls, S=3, R=5):\n",
    "        A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "        B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "        pi = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "        return cls(pi, A, B)\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = \"Prior:\\n\" + str(self.pi) + \"\\nA:\\n\" + str(self.A) + \"\\nB:\\n\" + str(self.B)\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.__str__()\n",
    "        return s\n",
    "\n",
    "    def predict(self, lp):\n",
    "        lstar = np.max(lp)\n",
    "        return lstar + np.log(np.dot(self.A,np.exp(lp-lstar)))\n",
    "\n",
    "    def postdict(self, lp):\n",
    "        lstar = np.max(lp)\n",
    "        return lstar + np.log(np.dot(np.exp(lp-lstar), self.A))\n",
    "\n",
    "    def update(self, y, lp):\n",
    "        return self.logB[y,:] + lp\n",
    "\n",
    "    def generate_sequence(self, T=10):\n",
    "    # T: Number of steps\n",
    "\n",
    "        x = np.zeros(T, int)\n",
    "        y = np.zeros(T, int)\n",
    "\n",
    "        for t in range(T):\n",
    "            if t==0:\n",
    "                x[t] = randgen(self.pi)\n",
    "            else:\n",
    "                x[t] = randgen(self.A[:,x[t-1]])    \n",
    "            y[t] = randgen(self.B[:,x[t]])\n",
    "    \n",
    "        return y, x\n",
    "\n",
    "    def forward(self, y):\n",
    "        T = len(y)\n",
    "        \n",
    "        # Forward Pass\n",
    "\n",
    "        # Python indexes starting from zero so\n",
    "        # log \\alpha_{k|k} will be in log_alpha[:,k-1]\n",
    "        # log \\alpha_{k|k-1} will be in log_alpha_pred[:,k-1]\n",
    "        log_alpha  = np.zeros((self.S, T))\n",
    "        log_alpha_pred = np.zeros((self.S, T))\n",
    "        for k in range(T):\n",
    "            if k==0:\n",
    "                log_alpha_pred[:,0] = self.logpi\n",
    "            else:\n",
    "                log_alpha_pred[:,k] = self.predict(log_alpha[:,k-1])\n",
    "\n",
    "            log_alpha[:,k] = self.update(y[k], log_alpha_pred[:,k])\n",
    "            \n",
    "        return log_alpha, log_alpha_pred\n",
    "            \n",
    "    def backward(self, y):\n",
    "        # Backward Pass\n",
    "        T = len(y)\n",
    "        log_beta  = np.zeros((self.S, T))\n",
    "        log_beta_post = np.zeros((self.S, T))\n",
    "\n",
    "        for k in range(T-1,-1,-1):\n",
    "            if k==T-1:\n",
    "                log_beta_post[:,k] = np.zeros(self.S)\n",
    "            else:\n",
    "                log_beta_post[:,k] = self.postdict(log_beta[:,k+1])\n",
    "\n",
    "            log_beta[:,k] = self.update(y[k], log_beta_post[:,k])\n",
    "\n",
    "        return log_beta, log_beta_post\n",
    "        \n",
    "    def forward_backward_smoother(self, y):\n",
    "        log_alpha, log_alpha_pred = self.forward(y)\n",
    "        log_beta, log_beta_post = self.backward(y)\n",
    "        \n",
    "        log_gamma = log_alpha + log_beta_post\n",
    "        return log_gamma\n",
    "        \n",
    "    def correction_smoother(self, y):\n",
    "        # Correction Smoother\n",
    "\n",
    "        log_alpha, log_alpha_pred = self.forward(y)\n",
    "        T = len(y)\n",
    "        \n",
    "        # For numerical stability, we calculate everything in the log domain\n",
    "        log_gamma_corr = np.zeros_like(log_alpha)\n",
    "        log_gamma_corr[:,T-1] = log_alpha[:,T-1]\n",
    "\n",
    "        C2 = np.zeros((self.S, self.S))\n",
    "        C3 = np.zeros((self.R, self.S))\n",
    "        C3[y[-1],:] = normalize_exp(log_alpha[:,T-1])\n",
    "        for k in range(T-2,-1,-1):\n",
    "            log_old_pairwise_marginal = log_alpha[:,k].reshape(1,self.S) + self.logA \n",
    "            log_old_marginal = self.predict(log_alpha[:,k])\n",
    "            log_new_pairwise_marginal = log_old_pairwise_marginal + log_gamma_corr[:,k+1].reshape(self.S,1) - log_old_marginal.reshape(self.S,1)\n",
    "            log_gamma_corr[:,k] = log_sum_exp(log_new_pairwise_marginal, axis=0).reshape(self.S)\n",
    "            C2 += normalize_exp(log_new_pairwise_marginal)\n",
    "            C3[y[k],:] += normalize_exp(log_gamma_corr[:,k])\n",
    "        C1 = normalize_exp(log_gamma_corr[:,0])\n",
    "        return log_gamma_corr, C1, C2, C3\n",
    "    \n",
    "    def forward_only_SS(self, y, V=None):\n",
    "        # Forward only estimation of expected sufficient statistics\n",
    "        T = len(y)\n",
    "        \n",
    "        if V is None:\n",
    "            V1  = np.eye((self.S))\n",
    "            V2  = np.zeros((self.S,self.S,self.S))\n",
    "            V3  = np.zeros((self.R,self.S,self.S))\n",
    "        else:\n",
    "            V1, V2, V3 = V\n",
    "            \n",
    "        I_S1S = np.eye(self.S).reshape((self.S,1,self.S))\n",
    "        I_RR = np.eye(self.R)\n",
    "        \n",
    "        for k in range(T):\n",
    "            if k==0:\n",
    "                log_alpha_pred = self.logpi\n",
    "            else:\n",
    "                log_alpha_pred = self.predict(log_alpha)\n",
    "\n",
    "            if k>0:\n",
    "                #print(self.S, self.R)\n",
    "                #print(log_alpha)\n",
    "                # Calculate p(x_{k-1}|y_{1:k-1}, x_k) \n",
    "                lp = np.log(normalize_exp(log_alpha)).reshape(self.S,1) + self.logA.T    \n",
    "                P = normalize_exp(lp, axis=0)\n",
    "\n",
    "                # Update\n",
    "                V1 = np.dot(V1, P)             \n",
    "                V2 = np.dot(V2, P) + I_S1S*P.reshape((1,self.S,self.S))    \n",
    "                V3 = np.dot(V3, P) + I_RR[:,y[k-1]].reshape((self.R,1,1))*P.reshape((1,self.S,self.S))    \n",
    "\n",
    "            log_alpha = self.update(y[k], log_alpha_pred)    \n",
    "            p_xT = normalize_exp(log_alpha)    \n",
    "\n",
    "        C1 = np.dot(V1, p_xT.reshape(self.S,1))\n",
    "        C2 = np.dot(V2, p_xT.reshape(1,self.S,1)).reshape((self.S,self.S))\n",
    "        C3 = np.dot(V3, p_xT.reshape(1,self.S,1)).reshape((self.R,self.S))\n",
    "        C3[y[-1],:] +=  p_xT\n",
    "        \n",
    "        ll = log_sum_exp(log_alpha)\n",
    "        \n",
    "        return C1, C2, C3, ll, (V1, V2, V3)\n",
    "\n",
    "    def train_EM(self, y, EPOCH=10):\n",
    "        \n",
    "        LL = np.zeros(EPOCH)\n",
    "        for e in range(EPOCH):\n",
    "            C1, C2, C3, ll, V = self.forward_only_SS(y)\n",
    "            LL[e] = ll\n",
    "            p = normalize(C1 + 0.1, axis=0).reshape(S)\n",
    "            #print(p,np.size(p))            \n",
    "            A = normalize(C2, axis=0)\n",
    "            #print(A)\n",
    "            B = normalize(C3, axis=0)\n",
    "            #print(B)\n",
    "            self.__init__(p, A, B)\n",
    "            print(ll)\n",
    "            \n",
    "        return LL\n",
    "            \n",
    "    \n",
    "    \n",
    "hm = HMM.from_random_parameters()\n",
    "\n",
    "y,x = hm.generate_sequence(300)\n",
    "\n",
    "log_alpha, log_alpha_pred = hm.forward(y)\n",
    "log_gamma = hm.forward_backward_smoother(y)\n",
    "log_gamma_corr, C1_corr, C2_corr, C3_corr = hm.correction_smoother(y)\n",
    "C1, C2, C3, ll, V = hm.forward_only_SS(y)\n",
    "\n",
    "print(C2)\n",
    "print(C2_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-510.59454094]\n",
      "[-426.6249263]\n",
      "[-421.7946712]\n",
      "[-419.92804551]\n",
      "[-418.89060041]\n",
      "[-418.1976513]\n",
      "[-417.67430715]\n",
      "[-417.24274461]\n",
      "[-416.86420116]\n",
      "[-416.51821331]\n",
      "[-416.19381263]\n",
      "[-415.88536099]\n",
      "[-415.59043003]\n",
      "[-415.30861391]\n",
      "[-415.04073546]\n",
      "[-414.7882066]\n",
      "[-414.55249513]\n",
      "[-414.33474007]\n",
      "[-414.13555106]\n",
      "[-413.95497319]\n",
      "[-413.7925566]\n",
      "[-413.64746734]\n",
      "[-413.51859877]\n",
      "[-413.40466733]\n",
      "[-413.30429136]\n",
      "[-413.21605594]\n",
      "[-413.13856598]\n",
      "[-413.07048816]\n",
      "[-413.01058168]\n",
      "[-412.95771793]\n",
      "[-412.91089021]\n",
      "[-412.86921493]\n",
      "[-412.83192666]\n",
      "[-412.79836899]\n",
      "[-412.76798304]\n",
      "[-412.74029519]\n",
      "[-412.71490499]\n",
      "[-412.69147391]\n",
      "[-412.66971529]\n",
      "[-412.64938562]\n",
      "[-412.63027712]\n",
      "[-412.6122115]\n",
      "[-412.59503478]\n",
      "[-412.57861305]\n",
      "[-412.56282894]\n",
      "[-412.54757873]\n",
      "[-412.53276997]\n",
      "[-412.51831949]\n",
      "[-412.5041518]\n",
      "[-412.49019765]\n",
      "[-412.47639289]\n",
      "[-412.4626775]\n",
      "[-412.44899469]\n",
      "[-412.43529019]\n",
      "[-412.42151157]\n",
      "[-412.40760772]\n",
      "[-412.39352828]\n",
      "[-412.37922323]\n",
      "[-412.36464249]\n",
      "[-412.34973555]\n",
      "[-412.33445118]\n",
      "[-412.31873713]\n",
      "[-412.30253999]\n",
      "[-412.28580494]\n",
      "[-412.26847575]\n",
      "[-412.25049466]\n",
      "[-412.23180253]\n",
      "[-412.2123389]\n",
      "[-412.19204232]\n",
      "[-412.1708507]\n",
      "[-412.14870186]\n",
      "[-412.12553419]\n",
      "[-412.10128749]\n",
      "[-412.07590401]\n",
      "[-412.04932964]\n",
      "[-412.02151523]\n",
      "[-411.99241808]\n",
      "[-411.96200355]\n",
      "[-411.93024669]\n",
      "[-411.89713381]\n",
      "[-411.86266409]\n",
      "[-411.82685081]\n",
      "[-411.78972248]\n",
      "[-411.75132346]\n",
      "[-411.71171419]\n",
      "[-411.67097092]\n",
      "[-411.62918482]\n",
      "[-411.58646069]\n",
      "[-411.54291504]\n",
      "[-411.49867386]\n",
      "[-411.45387012]\n",
      "[-411.408641]\n",
      "[-411.3631252]\n",
      "[-411.31746035]\n",
      "[-411.27178056]\n",
      "[-411.22621439]\n",
      "[-411.18088312]\n",
      "[-411.13589945]\n",
      "[-411.09136658]\n",
      "[-411.04737768]\n"
     ]
    }
   ],
   "source": [
    "LL = hm.train_EM(y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b4e8550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(LL)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(log_gamma)\n",
    "\n",
    "print(log_gamma_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating parameters of an homogeneous Markov chain\n",
    "$\\newcommand{\\ind}[1]{\\left[{#1}\\right]}$\n",
    "\n",
    "We have a Markov chain with transition probabilities $p(x_t = i| x_{t-1} = j) =  A_{i,j}$\n",
    "and initial state $p(x_1) = \\pi_i$.\n",
    "\n",
    "The distributions are\n",
    "\\begin{eqnarray}\n",
    "p(x_1 |\\pi)& = &\\prod_{s=1}^{S} \\pi_s^{\\ind{s = x_1}} \\\\\n",
    "p(x_t | x_{t-1}, A) &=& \\prod_{j=1}^{S} \\prod_{s=1}^{S}  A_{s,j}^{{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "The loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A) & = & \\log \\left( p(x_1 | \\pi) \\prod_{t=2}^T p(x_t | x_{t-1}, A) \\right) \\\\\n",
    "& = & \\sum_{s=1}^{S} {\\ind{s = x_1}} \\log \\pi_s + \\sum_{t=2}^T \\sum_{j=1}^{S}\\sum_{s=1}^{S} {{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\log A_{s,j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We have the constraints $\\sum_s \\pi_s = 1$ and $\\sum_i A_{i,j} = 1$ for all $j=1 \\dots S$ so we have $S+1$ constraints. We write the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A) & = & \\sum_{s=1}^{S} {\\ind{s = x_1}} \\log \\pi_s + \\sum_{t=2}^T \\sum_{j=1}^{S} \\sum_{s=1}^{S} {{\\ind{s = x_t}}\\ind{j = x_{t-1}}} \\log A_{s,j} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To find $\\pi$ and $A$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A)}{\\partial \\pi_i} & = & {\\ind{i = x_1}} \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)}{\\partial A_{i,j}} & = & \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{A_{i,j}} - \\lambda^A_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "Substitute the constraints $\\sum_s \\pi_s = 1$ and $\\sum_s A_{s,j} = 1, \\; j=1\\dots S$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & {\\ind{i = x_1}} \\frac{1}{\\lambda^\\pi} \\\\\n",
    "\\sum_i \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i {\\ind{i = x_1}} = 1\\\\\n",
    "\\lambda^\\pi & = & 1\\\\\n",
    "\\pi_i & = & {\\ind{i = x_1}}\n",
    "\\end{eqnarray}\n",
    "As we have effectively only a single observation for $x_1$, we have a crisp estimate.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i \\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_{t=2}^T \\sum_i  {{\\ind{i = x_t}}\\ind{j = x_{t-1}}} \\\\\n",
    "A_{i,j} & = & \\frac{\\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}{\\sum_{t=2}^T \\sum_i  {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}\\\\\n",
    "& = & \\frac{\\sum_{t=2}^T {{\\ind{i = x_t}}\\ind{j = x_{t-1}}}}{\\sum_{t=2}^T \\ind{j = x_{t-1}}}\n",
    "\\end{eqnarray}\n",
    "The result is intuitive. The denominator counts the number of times the chain visited state $j$ in the previous state. The numerator counts the number of times we visit $i$ given we were at $j$ the previous time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating parameters of an homogeneous Markov chain when several sequences are observed\n",
    "\n",
    "Suppose we have observed several sequences\n",
    "\\begin{eqnarray}\n",
    "X = \\{x_1^{(n)}, x_2^{(n)}, \\dots, x_{T_n}^{(n)}   \\}\n",
    "\\end{eqnarray}\n",
    "for $n = 1\\dots N$. Here $T_n$ is the length of the $n$'th sequence.\n",
    "\n",
    "The notation becomes slightly more complicated but conceptully the derivation is similar.\n",
    "\n",
    "The joint probability of all sequences is\n",
    "\\begin{eqnarray}\n",
    "p(X | \\pi, A) & = & \\prod_n \\left( p(x_1^{(n)}) \\prod_{t=2}^{T_n} p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right)\n",
    "\\end{eqnarray}\n",
    "The loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A) & = & \\sum_n \\left( \\log p(x_1^{(n)}) + \\sum_{t=2}^{T_n} \\log p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right) \\\\\n",
    "& = & \\sum_n \\left( \\sum_{s=1}^{S} {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_{t=2}^{T_n}\\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S} {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} \\right) \\\\\n",
    "& = & \\sum_{s=1}^{S} \\sum_n  {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_n \\sum_{t=2}^{T_n}\n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S}   {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "We write the Lagrangian\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)\n",
    "& = & \\sum_{s=1}^{S} \\sum_n  {\\ind{s = x_1^{(n)}}} \\log \\pi_s + \\sum_n  \\sum_{t=2}^{T_n} \\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S}  {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $\\pi$ and $A$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A)}{\\partial \\pi_i} & = & \\sum_n {\\ind{i = x_1^{(n)}}} \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A)}{\\partial A_{i,j}} & = &  \\sum_n \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{A_{i,j}} - \\lambda^A_j = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Prior\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\sum_n {\\ind{i = x_1^{(n)}}} \\frac{1}{\\lambda^\\pi}\\\\\n",
    "\\sum \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i \\sum_n {\\ind{i = x_1^{(n)}}} = 1 \\\\\n",
    "\\lambda^\\pi & = & \\sum_i \\sum_n {\\ind{i = x_1^{(n)}}} = N \\\\\n",
    "\\pi_i & = & \\frac{1}{N} \\sum_n {\\ind{i = x_1^{(n)}}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Transition Matrix\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & \\sum_n \\sum_{t=2}^{T_n}  {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i \\sum_n \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_i \\sum_n  \\sum_{t=2}^{T_n} {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\\\\n",
    "& = &  \\sum_n \\sum_{t=2}^{T_n}  \\ind{j = x_{t-1}^{(n)}} \\\\\n",
    "A_{i,j} & = &  \\frac{\\sum_n \\sum_{t=2}^{T_n}  {{\\ind{i = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}}}{\\sum_n \\sum_{t=2}^{T_n}  {\\ind{j = x_{t-1}^{(n)}}}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The EM Algorithm\n",
    "\n",
    "$\\newcommand{\\E}[1]{\\left\\langle#1\\right\\rangle}$\n",
    "\n",
    "The EM algorithm is a standart approach for ML estimation, when we have hidden variables.\n",
    "The canonical model is $p(y, x| \\theta)$ where we observe only $y$.\n",
    "\n",
    "The observed data loglikelihood is\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & = & \\log p(y| \\theta) = \\log \\sum_x p(y, x| \\theta)\n",
    "\\end{eqnarray}\n",
    "\n",
    "The key to the EM algorithm is the Jensen's inequality, that states for a concave function $f$ we have for $0 \\leq \\lambda \\leq 1$\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f(\\lambda x_1 + (1 - \\lambda) x_2) \\geq \\lambda f( x_1) + (1 - \\lambda) f(x_2)\n",
    "\\end{eqnarray}\n",
    "\n",
    "In words the value of a function evaluated at the convex combination (lhs) is always equal and larger then the convex combination of the function values. As mathematical expectation\n",
    "\\begin{eqnarray}\n",
    "\\E{f(x)} & = & \\sum_x p(x) f(x) \\\\\n",
    "\\sum_x p(x) & = & 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "As $\\log(x)$  is a concave function, we have\n",
    "\n",
    "\\begin{eqnarray}\n",
    " f(\\E{x}) & \\geq & \\E{f(x)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The key idea of the EM algorithm is to lower bound the observed data likelihood an maximize the bound with respect to the parameters. We take any distribution $q(x)$\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & = & \\log \\sum_x p(y, x| \\theta) \\\\\n",
    "& = & \\log \\sum_x p(y, x| \\theta) \\frac{q(x)}{q(x)} \\\\\n",
    "& = & \\log  \\E{\\frac{p(y, x| \\theta)}{q(x)}}_{q(x)}\\\\\n",
    "& \\geq & \\E{\\log {p(y, x| \\theta)}}_{q(x)} -\\E{\\log{q(x)} }_{q(x)}\\\\\n",
    "\\end{eqnarray}\n",
    "For _any_ $q(x)$, we have a lower bound. The natural strategy here is to choose the $q(x)$ that will maximise the lower bound. This is an optimisation problem. To make the notation more familiar, we let $q(x = i) = q_i$. Then, we arrive at the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(q, \\lambda) & = & \\sum_i q_i \\log p(y, x=i| \\theta) - \\sum_i q_i \\log q_i \\\\\n",
    "& & + \\lambda (1 - \\sum_i q_i)\n",
    "\\end{eqnarray}\n",
    "We take the derivative with respect to $q_i$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(q, \\lambda)}{\\partial q_k} & = & \\log p(y, x=k| \\theta) - (\\log q_k + 1) - \\lambda = 0\\\\\n",
    "\\log q_k  & = & \\log p(y, x=k| \\theta) -1 - \\lambda \\\\\n",
    "q_k  & = & p(y, x=k| \\theta) \\exp(-1 - \\lambda) \\\\\n",
    "\\sum_k q_k & = & \\exp(-1 - \\lambda) \\sum_k p(y, x=k| \\theta) = 1\\\\\n",
    "\\exp(1 + \\lambda) & = & p(y | \\theta) \\\\\n",
    "\\exp(-1 - \\lambda) & = & 1/p(y | \\theta) \\\\\n",
    "\\end{eqnarray}\n",
    "hence we have\n",
    "\\begin{eqnarray}\n",
    "q_k  & = & p(y, x=k| \\theta)/p(y | \\theta) = p(x=k| y \\theta)\n",
    "\\end{eqnarray}\n",
    "This result shows that the best we can do is to choose the posterior distribution\n",
    "\\begin{eqnarray}\n",
    "q(x) & = & p(x| y, \\theta)\n",
    "\\end{eqnarray}\n",
    "The EM algorithm is an iterative algorithm that exploits this bound result. Given a particular parameter setting $\\theta^{(\\tau)}$ at iteration $\\tau$, we can compute a lower bound of the true likelihood function.\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta) & \\geq & \\E{\\log {p(y, x| \\theta)}}_{p(x| y, \\theta^{(\\tau)})} -\\E{\\log p(x| y, \\theta^{(\\tau)}) }_{p(x| y, \\theta^{(\\tau)})}\\\\\n",
    "& \\equiv & {\\cal F}[\\theta; \\theta^{(\\tau)}] +  H[p(x| y, \\theta^{(\\tau)})]\n",
    "\\end{eqnarray}\n",
    "We need to show that\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\theta^{(\\tau)}) & = & \\E{\\log {p(y, x| \\theta)}}_{p(x| y, \\theta^{(\\tau)})} -\\E{\\log p(x| y, \\theta^{(\\tau)}) }_{p(x| y, \\theta^{(\\tau)})}\\\\\n",
    "& = & {\\cal F}[\\theta^{(\\tau)}; \\theta^{(\\tau)}] +  H[p(x| y, \\theta^{(\\tau)})]\n",
    "\\end{eqnarray}\n",
    "In other words, the bound is tight at $\\theta^{(\\tau)}$, hence maximizing the bound guarantees maximization of the true loglikelihoood.\n",
    "\n",
    "In most cases, where the EM algorithm can be applied, the joint distribution is from an {\\it exponential family}, i.e., it has the generic algebraic form\n",
    "\\begin{eqnarray}\n",
    "p(y, x| \\theta) & = & b(y, x)\\exp( \\sum_l \\phi_l(y, x) \\psi(\\theta_l) - A(\\theta)   )\n",
    "\\end{eqnarray}\n",
    "where $\\phi_l$ are the sufficient statistics and $\\psi(\\theta_l)$ are the {\\it canonical} parameters. The canonical parameters are in one to one relation with a conventional parametrisation. We will give several explicit examples when we cover the HMM's of the next section.\n",
    "\n",
    "In the case when the complete data likelihood is an exponential family, the computation of the bound requires the expectation\n",
    "\\begin{eqnarray}\n",
    "\\E{\\log p(y, x| \\theta)} & = & \\E{\\log b(x, y)} + \\sum_l \\E{\\phi_l(y, x)} \\psi(\\theta_l) - A(\\theta)\n",
    "\\end{eqnarray}\n",
    "where the expectation is taken with respect to the posterior $p(x|y, \\theta^{(\\tau)})$. In other words, we need to compute expectations of form $\\E{\\phi_l(y, x)}$. Once these are available, we have effectively an expression for ${\\cal F}(\\theta; \\theta^{(\\tau)})$. By maximisation of ${\\cal F}$ with respect to $\\theta$,  and arrive at $\\theta^{(\\tau + 1)}$ and complete the iteration.\n",
    "\n",
    "In a rather abstract sense, the EM algorithm proceeds as follows:\n",
    "\n",
    "##### The Expectation/Maximization (EM) algorithm.\n",
    "\n",
    "\\begin{algorithmic}\n",
    "\\STATE Initialise $\\theta^{(0)}$\n",
    "\\FOR{  $\\text{epoch} = 1 \\dots $  MAXITER}\n",
    "\\STATE E-step. Compute the sufficient statistics of the complete data likelihood\n",
    "\\STATE M-step. Maximize with respect to the parameters $\\theta$ to find $\\theta^{(\\tau + 1)}$\n",
    "\\ENDFOR\n",
    "\\end{algorithmic}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning HMM parameters by EM\n",
    "\n",
    "Suppose we have observed several sequences\n",
    "\\begin{eqnarray}\n",
    "Y = \\{y_1^{(n)}, y_2^{(n)}, \\dots, y_{T_n}^{(n)}   \\}\n",
    "\\end{eqnarray}\n",
    "for $n = 1\\dots N$. Here $T_n$ is the length of the $n$'th sequence.\n",
    "Let $Y \\in \\{1,\\dots, R\\}$ and $X \\in \\{1,\\dots, S \\}$.\n",
    "\n",
    "The discrete observation, discrete state space HMM has the following factors:\n",
    "\\begin{eqnarray}\n",
    "p(x_1 = i) & = & \\pi_i \\\\\n",
    "p(y_k = r| x_k = i) & = & B_{r,i} \\\\\n",
    "p(x_k = i| x_{k-1} = j) & = & A_{i,j} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The joint probability of all observed sequences and corresponding hidden sequences are\n",
    "\\begin{eqnarray}\n",
    "p(Y, X | \\pi, A, B) & = & \\prod_n \\left( p(x_1^{(n)}) p(y_1^{(n)} | x_1^{(n)})  \\prod_{t=2}^{T_n} p(y_t^{(n)} | x_t^{(n)}) p(x_t^{(n)}| x_{t-1}^{(n)} ) \\right)\n",
    "\\end{eqnarray}\n",
    "The expected complete data loglikelihood is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi, A, B) & = & \\E{\\sum_n \\left( \\log p(x_1^{(n)}) + \\sum_{t=1}^{T_n} \\log p(x_t^{(n)}| x_{t-1}^{(n)} ) + \\sum_{t=2}^{T_n} \\log p(y_t^{(n)} | x_t^{(n)}) \\right)} \\\\\n",
    "& = & \\E{\\sum_n \\left( \\sum_{s=1}^{S} {\\ind{s = x_1^{(n)}}} \\log \\pi_s   + \\log \\pi_s \\sum_{t=2}^{T_n}\\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S} {\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}} \\log A_{s,j} + \\sum_{t=1}^{T_n}\\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} {\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}} \\log B_{r,i}\\right)} \\\\\n",
    "& = & \\sum_n \\sum_{s=1}^{S}   \\E{{\\ind{s = x_1^{(n)}}}} \\log \\pi_s  + \\sum_n \\sum_{t=2}^{T_n}\n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S}   \\E{{\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\log A_{s,j} + \\sum_n \\sum_{t=1}^{T_n}\\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} \\E{{\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}}} \\log B_{r,i} \\\\\n",
    "& = & \\sum_{s=1}^{S} \\underbrace{\\left( \\sum_n \\E{ {\\ind{s = x_1^{(n)}}}} \\right)}_{\\equiv C_{\\pi}} \\log \\pi_s  + \n",
    "\\sum_{j=1}^{S} \\sum_{s=1}^{S} \\underbrace{\\left(  \\sum_n \\sum_{t=2}^{T_n} \\E{{\\ind{s = x_t^{(n)}}}\\ind{j = x_{t-1}^{(n)}}} \\right)}_{\\equiv C_{A}} \\log A_{s,j} + \\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} \\underbrace{\\left(  \\sum_n \\sum_{t=1}^{T_n} \\E{{\\ind{r = y_t^{(n)}}}\\ind{i = x_{t}^{(n)}}} \\right)}_{\\equiv C_{B}} \\log B_{r,i} \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The M-Step\n",
    "We write the Lagrangian to ensure that the columns of $A$ and $B$ are positive and normalized\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A, \\lambda^B)\n",
    "& = & \\sum_{s=1}^{S} C_{\\pi}(s) \\log \\pi_s + \\sum_{j=1}^{S}\n",
    "\\sum_{s=1}^{S}  C_{A}(s,j) \\log A_{s,j} + \\sum_{r=1}^{R}\n",
    "\\sum_{i=1}^{S} C_{B}(r,i) \\log B_{r,i} \\\\\n",
    "& & + \\lambda^\\pi \\left( 1 - \\sum_s \\pi_s \\right) + \\sum_j \\lambda^A_j \\left( 1 - \\sum_s A_{s,j} \\right)\n",
    "+ \\sum_i \\lambda^B_i \\left( 1 - \\sum_r B_{r,i} \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To find $\\pi$, $A$ and $B$ we take the derivative of the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\Lambda(\\pi, A,\\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial \\pi_i} & = & C_{\\pi}(i) \\frac{1}{\\pi_i} - \\lambda^\\pi = 0\\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, A, \\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial A_{i,j}} & = &  C_{A}(i,j) \\frac{1}{A_{i,j}} - \\lambda^A_j = 0 \\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, B, \\lambda^\\pi, \\lambda^A, \\lambda^B)}{\\partial B_{k,i}} & = &  C_{B}(k,i) \\frac{1}{B_{k,i}} - \\lambda^B_i = 0 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Prior\n",
    "We set the derivative to zero and solve for $\\pi$\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & C_{\\pi}(i) \\frac{1}{\\lambda^\\pi}\n",
    "\\end{eqnarray}\n",
    "As we have the normalization constraint for $\\pi$, we also have the following equality from which we can solve for the Lagrange multiplier:\n",
    "\\begin{eqnarray}\n",
    "\\sum_i \\pi_i & = & \\frac{1}{\\lambda^\\pi} \\sum_i C_{\\pi}(i) = 1 \\\\\n",
    "\\lambda^\\pi & = & \\sum_i  C_{\\pi}(i)  = N \n",
    "\\end{eqnarray}\n",
    "Substituting, we obtain the intuitive answer:\n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\frac{1}{N} C_{\\pi}(i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Transition Matrix\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A_{i,j} & = & C_A(i,j) \\frac{1}{\\lambda^A_j} \\\\\n",
    "\\sum_i A_{i,j} & = & \\sum_i C_A(i,j) \\frac{1}{\\lambda^A_j} = 1 \\\\\n",
    "\\lambda^A_j & = & \\sum_i C_A(i,j) \\\\\n",
    "A_{i,j} & = &  \\frac{C_A(i,j)}{\\sum_i C_A(i,j)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Observation Matrix\n",
    "\\begin{eqnarray}\n",
    "B_{k,i} & = & C_B(k,i) \\frac{1}{\\lambda^B_i} \\\\\n",
    "\\sum_k B_{k,i} & = & \\sum_k C_B(k,i) \\frac{1}{\\lambda^B_i} = 1 \\\\\n",
    "\\lambda^B_i & = & \\sum_k C_B(k,i) \\\\\n",
    "B_{k,i} & = &  \\frac{C_B(k,i)}{\\sum_k C_B(k,i)}\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "y = np.array([0, 1, 3, 2, 4])\n",
    "\n",
    "hm = HMM(p, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward smoothers\n",
    "\n",
    "The EM algorithm requires obtaining the sufficient statistics of an HMM.  \n",
    "\n",
    "The key observation is that the sufficient statistics are additive:\n",
    "\\begin{eqnarray}\n",
    "C_t & = & \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k}) \\right) p(x_{1:t}|y_{1:t}) dx_{1:t}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The derivation depends on the following decomposition. By chain rule, we write\n",
    "\\begin{eqnarray}\n",
    "p(x_{1:t}|y_{1:t}) & = & p(x_{1}|x_{2:t}, y_{1:t}) \\cdots p(x_{t-2}|x_{t-1:t}, y_{1:t}) p(x_{t-1}|x_t, y_{1:t}) p(x_{t}|y_{1:t}) \n",
    "\\end{eqnarray}\n",
    "The key observation is that this expression admits computation in the forward direction. By conditional independence\n",
    "\\begin{eqnarray}\n",
    "p(x_{1:t}|y_{1:t}) & = & p(x_{1}|x_{2}, y_{1}) \\cdots p(x_{t-2}|x_{t-1}, y_{1:t-2}) p(x_{t-1}|x_t, y_{1:t-1}) p(x_{t}|y_{1:t}) \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "We will use this observation as the basis of a forward recursion. First we decompose the posterior \n",
    "as a product of the filtering density at time $t$ and a conditional quantity familiar from the correction smoother \n",
    "\\begin{eqnarray}\n",
    "C_t & = & \\int \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k})\\right) p(x_{1:t-1}|y_{1:t},x_t) p(x_{t}|y_{1:t}) dx_{1:t-1} dx_t \\\\\n",
    "& = & \\int \\underbrace{\\left( \\int \\left(\\sum_{k=2}^t s_k(x_{k-1}, x_{k})\\right) p(x_{1:t-1}|y_{1:t-1},x_t) dx_{1:t-1} \\right)}_{=V_t(x_t)} p(x_{t}|y_{1:t})  dx_t\n",
    "\\end{eqnarray}\n",
    "\n",
    "Due to additivity, we can decompose further\n",
    "\\begin{eqnarray}\n",
    "V_t(x_t) & = & \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-1}|y_{1:t-1}, x_t) dx_{1:t-1} \\\\\n",
    "& = & \\int \\int \\left( s_t(x_{t-1}, x_{t}) + \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k})  \\right) p(x_{1:t-2}|y_{1:t-2}, x_{t-1}) p(x_{t-1}|y_{1:t-1}, x_t) dx_{1:t-2} dx_{t-1} \\\\\n",
    "& = & \\int \\left( s_t(x_{t-1}, x_{t}) + \\int \\sum_{k=2}^{t-1} s_k(x_{k-1}, x_{k}) p(x_{1:t-2}|y_{1:t-2}, x_{t-1}) dx_{1:t-2}  \\right)  p(x_{t-1}|y_{1:t-1}, x_t)  dx_{t-1} \\\\\n",
    "& = & \\int \\left( s_t(x_{t-1}, x_{t}) + V_{t-1}(x_{t-1})  \\right)  p(x_{t-1}|y_{1:t-1}, x_t)  dx_{t-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "V_1(x_1) & = & 0 \\\\\n",
    "V_2(x_2) & = & \\int  s_2(x_{1}, x_{2})   p(x_{1}|y_{1}, x_2)  dx_{1} \\\\\n",
    "V_3(x_3) & = & \\int \\left( s_3(x_{2}, x_{3}) + V_{2}(x_{2})  \\right)  p(x_{2}|y_{1:2}, x_3)  dx_{2}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example \n",
    "Suppose only $y_{1:4}$ are observed. The above recursion, when applied to the sufficient statistics of an HMM has the following specific form. Recall that, $C_{\\pi}, C_A, C_B$ for stand for the sufficient statistics for the estimation of the prior, state transitions  and observations, respectively. Here, we will denote the expected sufficient statistics explicitely as $C_{A,t}$ where $t$ is the number of observations. \n",
    "\n",
    "The state transition statistics are\n",
    "\\begin{eqnarray}\n",
    "C_{A,4}(a,b) &=& \\sum_{x_4} \\sum_{x_3} \\sum_{x_2} \\sum_{x_1} \\left( {\\ind{a = x_2}}\\ind{b = x_{1}} + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) p(x_{1:4}|y_{1,4})\\\\\n",
    "\\end{eqnarray}\n",
    "By introducing the forward decomposition\n",
    "\\begin{eqnarray}\n",
    "C_{A,4}(a,b) &=& \\sum_{x_4} \\sum_{x_3} \\sum_{x_2} \\left(\\sum_{x_1} {\\ind{a = x_2}}\\ind{b = x_{1}} p(x_1|y_1, x_2) + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_2|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4})\\\\\n",
    "&=& \\sum_{x_4}\\sum_{x_3} \\sum_{x_2} \\left( \\underbrace{{\\ind{a = x_2}} p(x_1=b|y_1, x_2)}_{V_{A,2}(x_2)} + {\\ind{a = x_3}}\\ind{b = x_2} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_2|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "&=&  \\sum_{x_4}\\sum_{x_3} \\left( \\underbrace{p(x_1=b|y_1, x_2=a) p(x_2=a|y_{1,2}, x_3) + {\\ind{a = x_3}} p(x_2=b|y_{1,2}, x_3)}_{V_{A,3}(x_3)} + {\\ind{a = x_4}}\\ind{b = x_3} \\right) \\\\\n",
    "& & p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "&=&  \\sum_{x_4} \\left(p(x_1=b|y_1, x_2=a)\\sum_{x_3} p(x_2=a|y_{1,2}, x_3) p(x_3|y_{1,3}, x_4) + p(x_2=b|y_{1,2}, x_3=a) p(x_3=a|y_{1,3}, x_4) \\\\\n",
    "+ {\\ind{a = x_4}} p(x_3=b |y_{1,3}, x_4) \\right)  p(x_4|y_{1,4}) \\\\\n",
    "&=& p(x_1=b|y_1, x_2=a)\\sum_{x_3} p(x_2=a|y_{1,2}, x_3) \\sum_{x_4} p(x_3|y_{1,3}, x_4) p(x_4|y_{1,4}) \\\\\n",
    "& & + p(x_2=b|y_{1,2}, x_3=a) \\sum_{x_4} p(x_3=a|y_{1,3}, x_4)p(x_4|y_{1,4}) \\\\\n",
    "& & + p(x_3=b |y_{1,3}, x_4=a) p(x_4=a|y_{1,4})  \n",
    "\\end{eqnarray}\n",
    "\n",
    "One can verify, that the last line is indeed equal to the required sufficient statistics given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_{A,T}(a,b) & = &  \\sum_{x_{1:T}} \\sum_{t=2}^T \\ind{a = x_t}\\ind{b = x_{t-1}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = & \\sum_{x_{2:T}} \\left(\\sum_{x_{1}} \\ind{a = x_2}\\ind{b = x_{1}} p(x_{1}|y_{1},x_{2}) + \\sum_{t=3}^T \\ind{a = x_t}\\ind{b = x_{t-1}}  \\right) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "V_{A,2}(a, b, x_2) & = & \\ind{a = x_2} p(x_{1} = b |y_{1},x_{2}) \\\\\n",
    "C_{A,T}(a,b) & = & \\sum_{x_{3:T}} \\left( \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\sum_{x_2} \\ind{a = x_3}\\ind{b = x_{2}} p(x_{2}|y_{1:2},x_{3})  + \\sum_{t=4}^T \\ind{a = x_t}\\ind{b = x_{t-1}}  \\right) \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "V_{A,3}(a, b, x_3) & = & \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\sum_{x_2} \\ind{a = x_3}\\ind{b = x_{2}} p(x_{2}|y_{1:2},x_{3}) \\\\\n",
    "& = & \\sum_{x_2} V_2(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = x_3} p(x_{2}=b|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update rule is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "V_{A,t}(a, b, x_t) & = & \\sum_{x_{t-1}} V_{A,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\sum_{x_{t-1}} \\ind{a = x_t}\\ind{b = x_{t-1}} p(x_{t-1}|y_{1:t-1},x_{t}) \\\\\n",
    "& = & \\sum_{x_{t-1}} V_{A,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\ind{a = x_t} p(x_{t-1}=b|y_{1:t-1},x_{t})\n",
    "\\end{eqnarray}\n",
    "\n",
    "The advantage of this algorithm is that it is entirely forward and has attractive space properties, requiring only $S^3 + 2S$ space. The other statistics are derived below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_{B,T}(a,b) & = &  \\sum_{x_{1:T}} \\sum_{t=1}^T \\ind{a = y_t}\\ind{b = x_{t}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = &  \\sum_{x_{2:T}} \\left(\\ind{a = y_1} p(x_{1}=b|y_{1},x_{2}) + \\sum_{t=2}^T \\ind{a = y_t}\\ind{b = x_{t}} \\right) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{B,2}(a, b, x_2)  & = & \\ind{a = y_1} p(x_{1}=b|y_{1},x_{2}) \\\\\n",
    "C_{B,T}(a,b) & = &  \\sum_{x_{3:T}} \\left(\\sum_{x_2} V_{B,2}(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = y_2} p(x_{2}=b|y_{1:2},x_{3}) + \\sum_{t=3}^T \\ind{a = y_t}\\ind{b = x_{t}} \\right) \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{B,3}(a, b, x_3)  & = & \\sum_{x_2} V_{B,2}(a, b, x_2) p(x_{2}|y_{1:2},x_{3}) + \\ind{a = y_2} p(x_{2}=b|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update rule is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "V_{B,t}(a, b, x_t) & = & \\sum_{x_{t-1}} V_{B,t-1}(a, b, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t}) + \\ind{a = y_t} p(x_{t-1}=b|y_{1:t-1},x_{t})\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "C_{\\pi,T}(a) & = &  \\sum_{x_{1:T}} \\ind{a = x_{1}} \\left( \\prod_{t=2:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right) p(x_T|y_{1:T}) \\\\\n",
    "& = &  \\sum_{x_{2:T}} p(x_{1}=a|y_{1},x_{2}) \\left( \\prod_{t=3:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{\\pi,2}(a, x_2)  & = & p(x_{1}=a|y_{1},x_{2}) \\\\\n",
    "C_{\\pi,T}(a) & = &  \\sum_{x_{3:T}} \\sum_{x_2} V_{1,2}(a, x_2) p(x_{2}|y_{1:2},x_{3})  \\left( \\prod_{t=4:T} p(x_{t-1}|y_{1:t-1},x_{t}) \\right)  p(x_T|y_{1:T}) \\\\\n",
    "V_{\\pi,3}(a, x_3)  & = & \\sum_{x_2} V_{\\pi,2}(a, x_2) p(x_{2}|y_{1:2},x_{3})\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $t$, the update equation is \n",
    "\\begin{eqnarray}\n",
    "V_{\\pi,t}(a, x_t)  & = & \\sum_{x_{t-1}} V_{\\pi,t-1}(a, x_{t-1}) p(x_{t-1}|y_{1:{t-1}},x_{t})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " An implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with the Forward Smoother\n",
      "[[ 0.0101037 ]\n",
      " [ 0.00591267]\n",
      " [ 0.98398362]]\n",
      "1.0\n",
      "[[ 3.28676254  0.3130152   1.30799436]\n",
      " [ 0.93787016  0.32224762  0.57777562]\n",
      " [ 0.15521416  1.17804298  4.92107736]]\n",
      "13.0\n",
      "[[ 1.04421231  0.65538811  1.30039958]\n",
      " [ 0.50871901  0.23250729  3.2587737 ]\n",
      " [ 1.52524615  0.27454668  1.20020716]\n",
      " [ 1.30166939  0.65086371  1.0474669 ]\n",
      " [ 0.53802894  0.03050028  0.43147078]]\n",
      "14.0\n",
      "Results with the Correction Smoother\n",
      "[ 0.0101037   0.00591267  0.98398362]\n",
      "1.0\n",
      "[[ 3.28676254  0.3130152   1.30799436]\n",
      " [ 0.93787016  0.32224762  0.57777562]\n",
      " [ 0.15521416  1.17804298  4.92107736]]\n",
      "13.0\n",
      "[[ 1.04421231  0.65538811  1.30039958]\n",
      " [ 0.50871901  0.23250729  3.2587737 ]\n",
      " [ 1.52524615  0.27454668  1.20020716]\n",
      " [ 1.30166939  0.65086371  1.0474669 ]\n",
      " [ 0.53802894  0.03050028  0.43147078]]\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "S = 3\n",
    "R = 5\n",
    "A = np.random.dirichlet(0.7*np.ones(S),S).T\n",
    "B = np.random.dirichlet(0.7*np.ones(R),S).T\n",
    "p = np.random.dirichlet(0.7*np.ones(S)).T\n",
    "\n",
    "logA = np.log(A)\n",
    "logB = np.log(B)\n",
    "\n",
    "hm = HMM(p, A, B)\n",
    "\n",
    "y = np.array([1, 1, 1, 3, 2, 0, 3, 2, 1, 0, 0, 3, 2, 4])\n",
    "T = y.shape[0]\n",
    "\n",
    "# Forward only estimation of sufficient statistics\n",
    "V_pi  = np.eye((S))\n",
    "V_A  = np.zeros((S,S,S))\n",
    "V_B  = np.zeros((R,S,S))\n",
    "I_S1S = np.eye(S).reshape((S,1,S))\n",
    "I_RR = np.eye(R)\n",
    "\n",
    "for k in range(T):\n",
    "    if k==0:\n",
    "        log_alpha_pred = np.log(p)\n",
    "    else:\n",
    "        log_alpha_pred = predict(A, log_alpha)\n",
    "    \n",
    "    if k>0:\n",
    "        # Calculate p(x_{k-1}|y_{1:k-1}, x_k) \n",
    "        lp = np.log(normalize_exp(log_alpha)).reshape(S,1) + logA.T    \n",
    "        P = normalize_exp(lp, axis=0)\n",
    "        \n",
    "        # Update\n",
    "        V_pi = np.dot(V_pi, P)             \n",
    "        V_A = np.dot(V_A, P) + I_S1S*P.reshape((1,S,S))    \n",
    "        V_B = np.dot(V_B, P) + I_RR[:,y[k-1]].reshape((R,1,1))*P.reshape((1,S,S))    \n",
    "        \n",
    "    log_alpha = update(y[k], logB, log_alpha_pred)    \n",
    "    p_xT = normalize_exp(log_alpha)    \n",
    "    \n",
    "C1 = np.dot(V_pi, p_xT.reshape(S,1))\n",
    "C2 = np.dot(V_A, p_xT.reshape(1,S,1)).reshape((S,S))\n",
    "C3 = np.dot(V_B, p_xT.reshape(1,S,1)).reshape((R,S))\n",
    "C3[y[-1],:] +=  p_xT\n",
    "    \n",
    "print(\"Results with the Forward Smoother\")\n",
    "print(C1)\n",
    "print(np.sum(C1))\n",
    "\n",
    "print(C2)\n",
    "print(np.sum(C2))\n",
    "\n",
    "print(C3)\n",
    "print(np.sum(C3))\n",
    "\n",
    "print(\"Results with the Correction Smoother\")\n",
    "lg, C1_corr, C2_corr, C3_corr = hm.correction_smoother(y)\n",
    "\n",
    "print(C1_corr)\n",
    "print(np.sum(C1_corr))\n",
    "\n",
    "print(C2_corr)\n",
    "print(np.sum(C2_corr))\n",
    "\n",
    "print(C3_corr)\n",
    "print(np.sum(C3_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"stdin_port\": 52422, \n",
      "  \"ip\": \"127.0.0.1\", \n",
      "  \"control_port\": 52423, \n",
      "  \"hb_port\": 52424, \n",
      "  \"signature_scheme\": \"hmac-sha256\", \n",
      "  \"key\": \"b381430f-be59-444c-beb0-e75c64b6c970\", \n",
      "  \"shell_port\": 52420, \n",
      "  \"transport\": \"tcp\", \n",
      "  \"iopub_port\": 52421\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> ipython <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> ipython <app> --existing kernel-8739c958-47db-4b42-a280-a39582b02bdd.json \n",
      "or even just:\n",
      "    $> ipython <app> --existing \n",
      "if this is the most recent IPython session you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
