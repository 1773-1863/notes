{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\ind}[1]{\\left[#1\\right]}$\n",
    "\n",
    "### Model\n",
    "\n",
    "- Data set\n",
    "$$ {\\cal D} = \\{ x_1, \\dots  x_N \\} $$\n",
    "- Model with parameter $\\theta$\n",
    "$$ p(\\cal D | \\theta) $$\n",
    "\n",
    "<img src=\"fig13b.png\" width='360' align='center'>\n",
    "\n",
    "\n",
    "### Maximum Likelihood\n",
    "\n",
    "- Maximum Likelihood (ML)\n",
    "$$ \\lambda^{\\text{ML}} = \\arg\\max_{\\theta} \\log p({\\cal D} | \\theta) $$\n",
    "- Predictive distribution\n",
    "$$ p(x_{N+1} |  {\\cal D} ) \\approx  p(x_{N+1} |  \\theta^{\\text{ML}})  $$\n",
    "\n",
    "### Maximum Aposteriori\n",
    "\n",
    "- Prior\n",
    "$$ p(\\theta) $$\n",
    "\n",
    "- Maximum a-posteriori (MAP) : Regularised Maximum Likelihood\n",
    "$$\n",
    "\\theta^{\\text{MAP}} = \\arg\\max_{\\theta} \\log p({\\cal D} | \\theta) p(\\theta)\n",
    "$$\n",
    "\n",
    "- Predictive distribution\n",
    "$$ p(x_{N+1} |  {\\cal D} ) \\approx  p(x_{N+1} |  \\theta^{\\text{MAP}})  $$\n",
    "\n",
    "### Bayesian Learning\n",
    "\n",
    "- We treat parameters on the same footing as all other variables\n",
    "- We integrate over unknown parameters rather than using point estimates (remember the many-dice example)\n",
    " - Self-regularisation, avoids overfitting\n",
    " - Natural setup for online adaptation\n",
    " - Model selection\n",
    "\n",
    "\n",
    "- Predictive distribution\n",
    "\\begin{eqnarray}\n",
    "p(x_{N+1} ,  {\\cal D} ) &=& \\int d\\theta \\;\\; p(x_{N+1} |  \\theta) p( {\\cal D}| \\theta) p(\\theta)  \\\\\n",
    " &=& \\int d\\theta \\;\\; p(x_{N+1}|  \\theta) p( {\\cal D}, \\theta)   \\\\\n",
    " &=& \\int d\\theta \\;\\; p(x_{N+1}|  \\theta) p(  \\theta| {\\cal D}) p({\\cal D})   \\\\\n",
    " &=&  p({\\cal D}) \\int d\\theta \\;\\; p(x_{N+1}|  \\theta) p(  \\theta| {\\cal D})  \\\\\n",
    "p(x_{N+1} |  {\\cal D} ) &=& \\int d\\lambda \\;\\; p(x_{N+1} |  \\theta) p(\\theta | {\\cal D}) \n",
    "\\end{eqnarray}\n",
    "\n",
    "The interpretation is that past data provides an 'update' to the recent prior to be used for the current prediction.\n",
    "\n",
    "- Bayesian learning is just inference ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Independent Coin Flips\n",
    "\n",
    "Suppose we have a coin, flipped several times independently. A vague question one can ask is if one can predict the outcome of the next flip.\n",
    "\n",
    "It depends. If we already know that the coin is fair, there is nothing that we can learn from past data and indeed the future flips are independent of the previous flips. However, if we don't know the probability of the coin, we could estimate the parameter from past data to create a better prediction. Mathematically, the model is identical to  \n",
    "\n",
    "<img src=\"fig13b.png\" width='220' align='center'>\n",
    "\n",
    "Here, $\\theta$ is the parameter of the coin.\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "We observe the outcome of $N$ coin flips $\\{x^{(n)}\\}_{n=1\\dots N}$ where $x^{(n)} \\in \\left\\{0,1\\right\\}$. The model is a Bernoulli distribution with parameter $\\pi = (\\pi_0, \\pi_1)$. We have $\\pi_0 = 1 - \\pi_1$ where $0 \\leq \\pi_1 \\leq 1$. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "x^{(n)} & \\sim & p(x|\\pi) = (1-\\pi_1)^{1-x^{(n)} } \\pi_1^{x^{(n)} }\n",
    "\\end{eqnarray}\n",
    "\n",
    "The loglikelihood is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi_1) & = & \\sum_{n=1}^N (1- x^{(n)}) \\log (1 - \\pi_1) + \\sum_{n=1}^N x^{(n)} \\log (\\pi_1)  \\\\\n",
    "& = & \\log (1 - \\pi_1) \\sum_{n=1}^N (1- x^{(n)})  + \\log (\\pi_1) \\sum_{n=1}^N x^{(n)}  \n",
    "\\end{eqnarray}\n",
    "\n",
    "We define the number of $0$'s  \n",
    "\\begin{eqnarray}\n",
    "c_0 = \\sum_{n=1}^N (1- x^{(n)})\n",
    "\\end{eqnarray}\n",
    "and $1$'s as\n",
    "\\begin{eqnarray}\n",
    "c_1 = \\sum_{n=1}^N  x^{(n)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi_1) & = & \\log (1 - \\pi_1) c_0  + \\log (\\pi_1) c_1  \n",
    "\\end{eqnarray}\n",
    "\n",
    "We compute the gradient \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial \\pi_1} {\\cal L}(\\pi_1) & = & - \\frac{c_0}{1 - \\pi_1} + \\frac{c_1}{\\pi_1}  = 0 \n",
    "\\end{eqnarray}\n",
    "\n",
    "The solution is quite predictable\n",
    "\\begin{eqnarray}\n",
    "\\pi_1 & = &\\frac{c_1}{c_0 + c_1}  = \\frac{c_1}{N}  \n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Maximum A-posteriori estimation\n",
    "\n",
    "We need a prior over the probability parameter. One choice is the beta distribution\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\pi_1) & = & \\mathcal{B}(\\pi_1; \\alpha, \\beta) =  \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta) } \\pi_1^{\\alpha-1} (1-\\pi_1)^{\\beta-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The log joint ditribution of data is\n",
    "\\begin{eqnarray}\n",
    "\\log p(X, \\pi_1) & = & \\log p(\\pi_1) + \\log \\sum_{n=1}^N \\log p(x^{(n)}|\\pi_1) \\\\\n",
    "& = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & + (\\alpha-1) \\log \\pi_1 + (\\beta-1) \\log(1-\\pi_1) \\\\\n",
    "& & + c_1 \\log (\\pi_1)  + c_0 \\log (1 - \\pi_1) \\\\\n",
    "& = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & + (\\alpha + c_1 -1) \\log \\pi_1 + (\\beta + c_0 -1) \\log(1-\\pi_1) \n",
    "\\end{eqnarray}\n",
    "\n",
    "The gradient is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial \\pi_1} \\log p(X, \\pi_1)  & = & - \\frac{\\beta + c_0 -1}{1 - \\pi_1} + \\frac{\\alpha + c_1 -1}{\\pi_1}  = 0 \n",
    "\\end{eqnarray}\n",
    "\n",
    "We can solve for the parameter.\n",
    "\\begin{eqnarray}\n",
    "\\pi_1 (\\beta + c_0 -1) & = & (1 - \\pi_1) (\\alpha + c_1 -1) \\\\ \n",
    "\\pi_1 \\beta + \\pi_1 c_0 - \\pi_1 & = & \\alpha  + c_1  - 1 - \\pi_1 \\alpha - \\pi_1 c_1 + \\pi_1 \\\\ \n",
    "\\pi_1  & = & \\frac{\\alpha - 1  + c_1}{\\alpha + \\beta  - 2 + c_0 + c_1}    \\\\ \n",
    "\\end{eqnarray}\n",
    "\n",
    "When the prior is flat, i.e., when $\\alpha = \\beta = 1$, MAP and ML solutions coincide.\n",
    "\n",
    "#### Full Bayesian inference\n",
    "\n",
    "We infer the posterior\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\pi_1| X) & = & \\frac{p(\\pi_1, X)}{p(X)} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The log joint density is \n",
    "\\begin{eqnarray}\n",
    "\\log p(X, \\pi_1) & = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & + (\\alpha + c_1 -1) \\log \\pi_1 + (\\beta + c_0 -1) \\log(1-\\pi_1) \n",
    "\\end{eqnarray}\n",
    "\n",
    "At this stage, we may try to evaluate the integral \n",
    "$$\n",
    "p(X)  =  \\int d\\pi_1 p(X, \\pi_1) \n",
    "$$\n",
    "\n",
    "Rather than trying to evaluate this integral directly, a simple approach is known as 'completing the square': we add an substract terms to obtain an expression that corresponds to a known, normalized density. This typically involves adding and substracting an expression that will make us identify a normalized density. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log p(X, \\pi_1) & = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & - \\log \\Gamma(\\alpha + \\beta + c_0 + c_1) + \\log \\Gamma(\\alpha + c_1) + \\log \\Gamma(\\beta + c_0) \\\\\n",
    "& & + \\log \\Gamma(\\alpha + \\beta + c_0 + c_1) - \\log \\Gamma(\\alpha + c_1) - \\log \\Gamma(\\beta + c_0) \\\\\n",
    "& & + (\\alpha + c_1 -1) \\log \\pi_1 + (\\beta + c_0 -1) \\log(1-\\pi_1) \\\\\n",
    "& = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & - \\log \\Gamma(\\alpha + \\beta + c_0 + c_1) + \\log \\Gamma(\\alpha + c_1) + \\log \\Gamma(\\beta + c_0) \\\\\n",
    "& & + \\log \\mathcal{B}(\\alpha + c_1, \\beta + c_0) \\\\\n",
    "& = & \\log p(X) + \\log p(\\pi_1| X)\n",
    "\\end{eqnarray}\n",
    "\n",
    "From the resulting expression, taking the exponent on both sides we see that \n",
    "\\begin{eqnarray}\n",
    "p(\\pi_1| X) & = & \\mathcal{B}(\\alpha + c_1, \\beta + c_0) \\\\\n",
    "p(X) & = & \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + c_1)\\Gamma(\\beta + c_0)}{\\Gamma(\\alpha + \\beta + c_0 + c_1)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "##### Alternative Derivation\n",
    "Alternatively, we may directly write\n",
    "\\begin{eqnarray}\n",
    "p(X, \\pi_1) & = & \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}   \\pi_1^{(\\alpha + c_1 -1)} (1-\\pi_1)^{(\\beta + c_0 -1)} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(X) &=& \\int d\\pi_1 p(X, \\pi_1)  =  \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int d\\pi_1  \\pi_1^{(\\alpha + c_1 -1)} (1-\\pi_1)^{(\\beta + c_0 -1)} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "From the definition of the beta distribution, we can arrive at the 'formula' for the integral \n",
    "\\begin{eqnarray}\n",
    "1 &=& \\int d\\pi \\mathcal{B}(\\pi; a, b) \\\\\n",
    "& = & \\int d\\pi \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\pi^{(a -1)} (1-\\pi)^{(b -1)} \\\\\n",
    "\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a + b)} & = & \\int d\\pi \\pi^{(a -1)} (1-\\pi)^{(b -1)}\n",
    "\\end{eqnarray}\n",
    "Just substitute $a = \\alpha + c_1$ and $b = \\beta + c_0$\n",
    "\n",
    "#### An Approximation\n",
    "For large $x$, we have the following approximation\n",
    "\\begin{eqnarray}\n",
    "\\log \\Gamma(x + a) - \\log \\Gamma(x) & \\approx & a \\log(x) \\\\\n",
    "\\Gamma(x + a)  & \\approx & \\Gamma(x) x^a \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "When $c_0$ and $c_1$ are large, we obtain:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(X) & \\approx & \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(c_1)\\Gamma(c_0)c_0^{\\beta}c_1^{\\alpha}}{\\Gamma(c_0 + c_1)(c_0+c_1)^{\\alpha + \\beta}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Let $\\hat{\\pi}_1 = c_1/(c_0+c_1)$ and $N = c_0 + c_1$, we have\n",
    "\\begin{eqnarray}\n",
    "p(X) & \\approx &  \\frac{\\Gamma(c_1)\\Gamma(c_0)}{\\Gamma(c_0 + c_1)} (1-\\hat{\\pi}_1) \\hat{\\pi}_1 \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} (1-\\hat{\\pi}_1)^{\\beta-1}\\hat{\\pi}_1^{\\alpha-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "### Estimating a Categorical distribution\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "\n",
    "We observe a dataset $\\{x^{(n)}\\}_{n=1\\dots N}$. The model for a single observation is a categorical distribution with parameter $\\pi = (\\pi_1, \\dots, \\pi_S)$ where \n",
    "\n",
    "\\begin{eqnarray}\n",
    "x^{(n)} & \\sim & p(x|\\pi) = \\prod_{s=1}^{S} \\pi_s^{\\ind{s = x^{(n)}}}\n",
    "\\end{eqnarray}\n",
    "where $\\sum_s \\pi_s  = 1$.\n",
    "\n",
    "The loglikelihood of the entire dataset is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi_1,\\dots,\\pi_S) & = & \\sum_{n=1}^N\\sum_{s=1}^S \\ind{s = x^{(n)}} \\log \\pi_s\n",
    "\\end{eqnarray}\n",
    "This is a constrained optimisation problem.\n",
    "Form the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, \\lambda) & = & \\sum_{n=1}^N\\sum_{s'=1}^S \\ind{s' = x^{(n)}} \\log \\pi_{s'}  + \\lambda \\left( 1 - \\sum_{s'} \\pi_{s'} \\right ) \\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, \\lambda)}{\\partial \\pi_s} & = & \\sum_{n=1}^N \\ind{s = x^{(n)}} \\frac{1}{\\pi_s} - \\lambda = 0 \\\\\n",
    "\\pi_s & = & \\frac{\\sum_{n=1}^N \\ind{s = x^{(n)}}}{\\lambda}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We solve for $\\lambda$\n",
    "\\begin{eqnarray}\n",
    "1 & = & \\sum_s \\pi_s = \\frac{\\sum_{s=1}^S \\sum_{n=1}^N \\ind{s = x^{(n)}}}{\\lambda} \\\\\n",
    "\\lambda & = & \\sum_{s=1}^S \\sum_{n=1}^N \\ind{s = x^{(n)}} =  \\sum_{n=1}^N 1 = N\n",
    "\\end{eqnarray}\n",
    "\n",
    "Hence\n",
    "\\begin{eqnarray}\n",
    "\\pi_s & = & \\frac{\\sum_{n=1}^N \\ind{s = x^{(n)}}}{N}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "### Fair/Fake Coin\n",
    "\n",
    "### Change point\n",
    "Coin switch\n",
    "\n",
    "Coal Mining Data\n",
    " Single Change Point\n",
    " Multiple Change Point\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
