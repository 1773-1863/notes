{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\ind}[1]{\\left[#1\\right]}$\n",
    "\n",
    "### Model\n",
    "\n",
    "- Data set\n",
    "$$ {\\cal D} = \\{ x_1, \\dots  x_N \\} $$\n",
    "- Model with parameter $\\theta$\n",
    "$$ p(\\cal D | \\theta) $$\n",
    "\n",
    "<img src=\"fig13b.png\" width='360' align='center'>\n",
    "\n",
    "\n",
    "### Maximum Likelihood\n",
    "\n",
    "- Maximum Likelihood (ML)\n",
    "$$ \\theta^{\\text{ML}} = \\arg\\max_{\\theta} \\log p({\\cal D} | \\theta) $$\n",
    "- Predictive distribution\n",
    "$$ p(x_{N+1} |  {\\cal D} ) \\approx  p(x_{N+1} |  \\theta^{\\text{ML}})  $$\n",
    "\n",
    "### Maximum Aposteriori\n",
    "\n",
    "- Prior\n",
    "$$ p(\\theta) $$\n",
    "\n",
    "- Maximum a-posteriori (MAP) : Regularised Maximum Likelihood\n",
    "$$\n",
    "\\theta^{\\text{MAP}} = \\arg\\max_{\\theta} \\log p({\\cal D} | \\theta) p(\\theta)\n",
    "$$\n",
    "\n",
    "- Predictive distribution\n",
    "$$ p(x_{N+1} |  {\\cal D} ) \\approx  p(x_{N+1} |  \\theta^{\\text{MAP}})  $$\n",
    "\n",
    "### Bayesian Learning\n",
    "\n",
    "- We treat parameters on the same footing as all other variables\n",
    "- We integrate over unknown parameters rather than using point estimates (remember the many-dice example)\n",
    " - Self-regularisation, avoids overfitting\n",
    " - Natural setup for online adaptation\n",
    " - Model selection\n",
    "\n",
    "\n",
    "- Predictive distribution\n",
    "\\begin{eqnarray}\n",
    "p(x_{N+1} ,  {\\cal D} ) &=& \\int d\\theta \\;\\; p(x_{N+1} |  \\theta) p( {\\cal D}| \\theta) p(\\theta)  \\\\\n",
    " &=& \\int d\\theta \\;\\; p(x_{N+1}|  \\theta) p( {\\cal D}, \\theta)   \\\\\n",
    " &=& \\int d\\theta \\;\\; p(x_{N+1}|  \\theta) p(  \\theta| {\\cal D}) p({\\cal D})   \\\\\n",
    " &=&  p({\\cal D}) \\int d\\theta \\;\\; p(x_{N+1}|  \\theta) p(  \\theta| {\\cal D})  \\\\\n",
    "p(x_{N+1} |  {\\cal D} ) &=& \\int d\\theta \\;\\; p(x_{N+1} |  \\theta) p(\\theta | {\\cal D}) \n",
    "\\end{eqnarray}\n",
    "\n",
    "The interpretation is that past data provides an 'update' to the recent prior to be used for the current prediction.\n",
    "\n",
    "- Bayesian learning is just inference ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Independent Coin Flips\n",
    "\n",
    "Suppose we have a coin, flipped several times independently. A vague question one can ask is if one can predict the outcome of the next flip.\n",
    "\n",
    "It depends. If we already know that the coin is fair, there is nothing that we can learn from past data and indeed the future flips are independent of the previous flips. However, if we don't know the probability of the coin, we could estimate the parameter from past data to create a better prediction. Mathematically, the model is identical to  \n",
    "\n",
    "<img src=\"fig13b.png\" width='220' align='center'>\n",
    "\n",
    "Here, $\\theta$ is the parameter of the coin.\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "We observe the outcome of $N$ coin flips $\\{x^{(n)}\\}_{n=1\\dots N}$ where $x^{(n)} \\in \\left\\{0,1\\right\\}$. The model is a Bernoulli distribution with parameter $\\pi = (\\pi_0, \\pi_1)$. We have $\\pi_0 = 1 - \\pi_1$ where $0 \\leq \\pi_1 \\leq 1$. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "x^{(n)} & \\sim & p(x|\\pi) = (1-\\pi_1)^{1-x^{(n)} } \\pi_1^{x^{(n)} }\n",
    "\\end{eqnarray}\n",
    "\n",
    "The loglikelihood is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi_1) & = & \\sum_{n=1}^N (1- x^{(n)}) \\log (1 - \\pi_1) + \\sum_{n=1}^N x^{(n)} \\log (\\pi_1)  \\\\\n",
    "& = & \\log (1 - \\pi_1) \\sum_{n=1}^N (1- x^{(n)})  + \\log (\\pi_1) \\sum_{n=1}^N x^{(n)}  \n",
    "\\end{eqnarray}\n",
    "\n",
    "We define the number of $0$'s  \n",
    "\\begin{eqnarray}\n",
    "c_0 = \\sum_{n=1}^N (1- x^{(n)})\n",
    "\\end{eqnarray}\n",
    "and $1$'s as\n",
    "\\begin{eqnarray}\n",
    "c_1 = \\sum_{n=1}^N  x^{(n)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi_1) & = & \\log (1 - \\pi_1) c_0  + \\log (\\pi_1) c_1  \n",
    "\\end{eqnarray}\n",
    "\n",
    "We compute the gradient \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial \\pi_1} {\\cal L}(\\pi_1) & = & - \\frac{c_0}{1 - \\pi_1} + \\frac{c_1}{\\pi_1}  = 0 \n",
    "\\end{eqnarray}\n",
    "\n",
    "The solution is quite predictable\n",
    "\\begin{eqnarray}\n",
    "\\pi_1 & = &\\frac{c_1}{c_0 + c_1}  = \\frac{c_1}{N}  \n",
    "\\end{eqnarray}\n",
    "\n",
    "#### Maximum A-posteriori estimation\n",
    "\n",
    "We need a prior over the probability parameter. One choice is the beta distribution\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\pi_1) & = & \\mathcal{B}(\\pi_1; \\alpha, \\beta) =  \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta) } \\pi_1^{\\alpha-1} (1-\\pi_1)^{\\beta-1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The log joint ditribution of data is\n",
    "\\begin{eqnarray}\n",
    "\\log p(X, \\pi_1) & = & \\log p(\\pi_1) + \\log \\sum_{n=1}^N \\log p(x^{(n)}|\\pi_1) \\\\\n",
    "& = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & + (\\alpha-1) \\log \\pi_1 + (\\beta-1) \\log(1-\\pi_1) \\\\\n",
    "& & + c_1 \\log (\\pi_1)  + c_0 \\log (1 - \\pi_1) \\\\\n",
    "& = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & + (\\alpha + c_1 -1) \\log \\pi_1 + (\\beta + c_0 -1) \\log(1-\\pi_1) \n",
    "\\end{eqnarray}\n",
    "\n",
    "The gradient is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial \\pi_1} \\log p(X, \\pi_1)  & = & - \\frac{\\beta + c_0 -1}{1 - \\pi_1} + \\frac{\\alpha + c_1 -1}{\\pi_1}  = 0 \n",
    "\\end{eqnarray}\n",
    "\n",
    "We can solve for the parameter.\n",
    "\\begin{eqnarray}\n",
    "\\pi_1 (\\beta + c_0 -1) & = & (1 - \\pi_1) (\\alpha + c_1 -1) \\\\ \n",
    "\\pi_1 \\beta + \\pi_1 c_0 - \\pi_1 & = & \\alpha  + c_1  - 1 - \\pi_1 \\alpha - \\pi_1 c_1 + \\pi_1 \\\\ \n",
    "\\pi_1  & = & \\frac{\\alpha - 1  + c_1}{\\alpha + \\beta  - 2 + c_0 + c_1}    \\\\ \n",
    "\\end{eqnarray}\n",
    "\n",
    "When the prior is flat, i.e., when $\\alpha = \\beta = 1$, MAP and ML solutions coincide.\n",
    "\n",
    "#### Full Bayesian inference\n",
    "\n",
    "We infer the posterior\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\pi_1| X) & = & \\frac{p(\\pi_1, X)}{p(X)} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The log joint density is \n",
    "\\begin{eqnarray}\n",
    "\\log p(X, \\pi_1) & = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & + (\\alpha + c_1 -1) \\log \\pi_1 + (\\beta + c_0 -1) \\log(1-\\pi_1) \n",
    "\\end{eqnarray}\n",
    "\n",
    "At this stage, we may try to evaluate the integral \n",
    "$$\n",
    "p(X)  =  \\int d\\pi_1 p(X, \\pi_1) \n",
    "$$\n",
    "\n",
    "Rather than trying to evaluate this integral directly, a simple approach is known as 'completing the square': we add an substract terms to obtain an expression that corresponds to a known, normalized density. This typically involves adding and substracting an expression that will make us identify a normalized density. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log p(X, \\pi_1) & = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & - \\log \\Gamma(\\alpha + \\beta + c_0 + c_1) + \\log \\Gamma(\\alpha + c_1) + \\log \\Gamma(\\beta + c_0) \\\\\n",
    "& & + \\log \\Gamma(\\alpha + \\beta + c_0 + c_1) - \\log \\Gamma(\\alpha + c_1) - \\log \\Gamma(\\beta + c_0) \\\\\n",
    "& & + (\\alpha + c_1 -1) \\log \\pi_1 + (\\beta + c_0 -1) \\log(1-\\pi_1) \\\\\n",
    "& = & \\log \\Gamma(\\alpha + \\beta) -\\log \\Gamma(\\alpha) - \\log \\Gamma(\\beta) \\\\\n",
    "& & - \\log \\Gamma(\\alpha + \\beta + c_0 + c_1) + \\log \\Gamma(\\alpha + c_1) + \\log \\Gamma(\\beta + c_0) \\\\\n",
    "& & + \\log \\mathcal{B}(\\alpha + c_1, \\beta + c_0) \\\\\n",
    "& = & \\log p(X) + \\log p(\\pi_1| X)\n",
    "\\end{eqnarray}\n",
    "\n",
    "From the resulting expression, taking the exponent on both sides we see that \n",
    "\\begin{eqnarray}\n",
    "p(\\pi_1| X) & = & \\mathcal{B}(\\alpha + c_1, \\beta + c_0) \\\\\n",
    "p(X) & = & \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + c_1)\\Gamma(\\beta + c_0)}{\\Gamma(\\alpha + \\beta + c_0 + c_1)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "##### Alternative Derivation\n",
    "Alternatively, we may directly write\n",
    "\\begin{eqnarray}\n",
    "p(X, \\pi_1) & = & \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}   \\pi_1^{(\\alpha + c_1 -1)} (1-\\pi_1)^{(\\beta + c_0 -1)} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(X) &=& \\int d\\pi_1 p(X, \\pi_1)  =  \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int d\\pi_1  \\pi_1^{(\\alpha + c_1 -1)} (1-\\pi_1)^{(\\beta + c_0 -1)} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "From the definition of the beta distribution, we can arrive at the 'formula' for the integral \n",
    "\\begin{eqnarray}\n",
    "1 &=& \\int d\\pi \\mathcal{B}(\\pi; a, b) \\\\\n",
    "& = & \\int d\\pi \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\pi^{(a -1)} (1-\\pi)^{(b -1)} \\\\\n",
    "\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a + b)} & = & \\int d\\pi \\pi^{(a -1)} (1-\\pi)^{(b -1)}\n",
    "\\end{eqnarray}\n",
    "Just substitute $a = \\alpha + c_1$ and $b = \\beta + c_0$\n",
    "\n",
    "#### An Approximation\n",
    "For large $x$, we have the following approximation\n",
    "\\begin{eqnarray}\n",
    "\\log \\Gamma(x + a) - \\log \\Gamma(x) & \\approx & a \\log(x) \\\\\n",
    "\\Gamma(x + a)  & \\approx & \\Gamma(x) x^a \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "When $c_0$ and $c_1$ are large, we obtain:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(X) & \\approx & \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(c_1)\\Gamma(c_0)c_0^{\\beta}c_1^{\\alpha}}{\\Gamma(c_0 + c_1)(c_0+c_1)^{\\alpha + \\beta}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Let $\\hat{\\pi}_1 = c_1/(c_0+c_1)$ and $N = c_0 + c_1$, we have\n",
    "\\begin{eqnarray}\n",
    "p(X) & \\approx &  \\frac{\\Gamma(c_1)\\Gamma(c_0)}{\\Gamma(c_0 + c_1)} (1-\\hat{\\pi}_1) \\hat{\\pi}_1 \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} (1-\\hat{\\pi}_1)^{\\beta-1}\\hat{\\pi}_1^{\\alpha-1}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating a Categorical distribution\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "\n",
    "We observe a dataset $\\{x^{(n)}\\}_{n=1\\dots N}$. The model for a single observation is a categorical distribution with parameter $\\pi = (\\pi_1, \\dots, \\pi_S)$ where \n",
    "\n",
    "\\begin{eqnarray}\n",
    "x^{(n)} & \\sim & p(x|\\pi) = \\prod_{s=1}^{S} \\pi_s^{\\ind{s = x^{(n)}}}\n",
    "\\end{eqnarray}\n",
    "where $\\sum_s \\pi_s  = 1$.\n",
    "\n",
    "The loglikelihood of the entire dataset is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\pi_1,\\dots,\\pi_S) & = & \\sum_{n=1}^N\\sum_{s=1}^S \\ind{s = x^{(n)}} \\log \\pi_s\n",
    "\\end{eqnarray}\n",
    "This is a constrained optimisation problem.\n",
    "Form the Lagrangian\n",
    "\\begin{eqnarray}\n",
    "\\Lambda(\\pi, \\lambda) & = & \\sum_{n=1}^N\\sum_{s'=1}^S \\ind{s' = x^{(n)}} \\log \\pi_{s'}  + \\lambda \\left( 1 - \\sum_{s'} \\pi_{s'} \\right ) \\\\\n",
    "\\frac{\\partial \\Lambda(\\pi, \\lambda)}{\\partial \\pi_s} & = & \\sum_{n=1}^N \\ind{s = x^{(n)}} \\frac{1}{\\pi_s} - \\lambda = 0 \\\\\n",
    "\\pi_s & = & \\frac{\\sum_{n=1}^N \\ind{s = x^{(n)}}}{\\lambda}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We solve for $\\lambda$\n",
    "\\begin{eqnarray}\n",
    "1 & = & \\sum_s \\pi_s = \\frac{\\sum_{s=1}^S \\sum_{n=1}^N \\ind{s = x^{(n)}}}{\\lambda} \\\\\n",
    "\\lambda & = & \\sum_{s=1}^S \\sum_{n=1}^N \\ind{s = x^{(n)}} =  \\sum_{n=1}^N 1 = N\n",
    "\\end{eqnarray}\n",
    "\n",
    "Hence\n",
    "\\begin{eqnarray}\n",
    "\\pi_s & = & \\frac{\\sum_{n=1}^N \\ind{s = x^{(n)}}}{N}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair/Fake Coin\n",
    "\n",
    "We consider the folowing problem: Given a sequence of coin tosses $\\{x^{(n)}\\}_{n=1\\dots N}$, determine if the coin is fair or fake.\n",
    "\n",
    "This can be cast as a model selection problem:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\pi_1|m & \\sim & \\left\\{ \\begin{array}{cc} \\delta(\\pi_1 - 0.5) & m = 0\\\\ \\mathcal{B}(\\pi; a, b) & m = 1 \\end{array} \\right.\n",
    "\\end{eqnarray}\n",
    "For $n = 1\\dots N$\n",
    "\\begin{eqnarray}\n",
    "x^{(n)}| \\pi_1 & \\sim & \\mathcal{BE}(\\pi_1; a, b)\n",
    "\\end{eqnarray}\n",
    "\n",
    "This model defines the following:\n",
    "- The indicator $m$, that denotes if the coin is fake,\n",
    "- What a fake coin is: a fake coin is one that has an arbitrary probability $\\pi_1$ between $0$ and $1$. \n",
    "- What a fair coin is: a fair coin has $\\pi_1 = 0.5$\n",
    "\n",
    "We need to calculate the marginal likelihoods for $m=0$ and $m=1$\n",
    "\\begin{eqnarray}\n",
    "p(x^{(n)}| m) & = & \\int d\\pi_1 p(x^{(n)}| \\pi_1) p(\\pi_1|m)\n",
    "\\end{eqnarray}\n",
    "\n",
    "###### Not Fake\n",
    "\\begin{eqnarray}\n",
    "p(x^{(n)}| m) & = & \\int d\\pi_1 p(x^{(n)}| \\pi_1) \\delta(\\pi_1 - 0.5) \\\\\n",
    "& = & \\prod_{n=1}^N \\left(\\frac{1}{2}\\right)^{x^{(n)}} \\left(\\frac{1}{2}\\right)^{1-x^{(n)}} = \\frac{1}{2^N}\n",
    "\\end{eqnarray}\n",
    "\n",
    "###### Fake\n",
    "\n",
    "\\begin{eqnarray}\n",
    "1 &=& \\int d\\pi \\mathcal{B}(\\pi; a, b) \\\\\n",
    "& = & \\int d\\pi \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\pi^{(a -1)} (1-\\pi)^{(b -1)} \\\\\n",
    "\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a + b)} & = & \\int d\\pi \\pi^{(a -1)} (1-\\pi)^{(b -1)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(x^{(n)}| m) & = & \\int d\\pi_1 p(x^{(n)}| \\pi_1) p(\\pi_1; a, b) \\\\\n",
    "& = &  \\int d\\pi_1 \\left(\\prod_{n=1}^N \\left(1-\\pi_1\\right)^{1-x^{(n)}} \\pi_1^{x^{(n)}} \\right) \\mathcal{B}(\\pi; a, b) \\\\\n",
    "& = & \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\int d\\pi_1 \\left(1-\\pi_1\\right)^{c_0+a-1} \\pi_1^{c_1+b-1} \\\\\n",
    "& = & \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} \\frac{\\Gamma(c_0+a)\\Gamma(c_1+b)}{\\Gamma(c_0 + c_1 +a + b)}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_odds =  -1.3881464778\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.special as sps\n",
    "\n",
    "# Number of Zeros observed\n",
    "c_0 = 30\n",
    "# Number of Ones\n",
    "c_1 = 60\n",
    "\n",
    "# Total number of tosses\n",
    "N = c_0 + c_1\n",
    "\n",
    "# Prior\n",
    "a = 0.1\n",
    "b = 0.1\n",
    "\n",
    "M_fair = N*np.log(0.5)\n",
    "M_fake = sps.gammaln(a+b) - sps.gammaln(a) - sps.gammaln(b) +  sps.gammaln(c_0+a) + sps.gammaln(c_1+b) - sps.gammaln(N+a + b) \n",
    "\n",
    "print('log_odds = ', M_fair - M_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEACAYAAAB4ayemAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VeW1+PHvCgE0GgQkEOYwIwQitTIUh+NQRQgq9bZK\nxV6xqLdXq1zEiWsh+vCrlQetpVgrtOglBfHiRIGiGOEoegUZVSISxhAIKgGiEGby/v7YSQwhydln\n2HufYX2e5zwkOfvsvbI5WVlZ+93vK8YYlFJKxYYkrwNQSillnyZtpZSKIZq0lVIqhmjSVkqpGKJJ\nWymlYogmbaWUiiEBk7aIPCgiX4jIRhF50I2glFJK1a7epC0imcAY4FIgC8gWkS5uBKaUUupsgSrt\nnsAqY8wxY8xp4APgZ86HpZRSqjaBkvZG4HIRaS4iKcAwoJ3zYSmllKpNcn1PGmO+EpFngKVAGbAe\nKHcjMKWUUmeTYOYeEZHfA7uMMX+t9jWdvEQppUJgjJFgX2Nn9EjLin87ACOAubUcWB/GMGnSJM9j\niJaHngs9F3ou6n+Eys447ddFJB/4J/CfxpjvQz6aUiqicnJyvA5BuazenjaAMeYKNwJRSikVmN4R\nGUE+n8/rEKKGnosfOHkuYq3S1vdF+IK6EFnrDkRMuPtQSqlEIyKYEC5EBmyPhEok6FiUilleFS45\nOTkxV22r8DiWtMG7N7JSbtICRbnJsfZIRekf1r6VigX6XlehCLU9ohcilVIqhmjSViqGaT878WjS\nVkqpGKI9baXCpO91FQrtaSeIHTt2eB1CxO3du5cjR454HYZSMUGTdhAyMzP58MMPPTv+9u3bWbly\npWfHd0paWhpTpkzxOoyYpD3txJPQSTsjI4OUlBRSU1NJT09n9OjRlJWV1bn9xo0bueKKyEzFsnfv\nXkaNGsWMGTPIy8tj/PjxtGnThrvuuqvO17z00kuMHDkyIsevacOGDYwfP96RfQc6VnJyMsOGDWP2\n7NmuHF+pWJbQSVtEWLRoEYcOHWLdunWsWbOGyZMnn7XdqVOnQj5GXa9t3LgxJSUlvPPOO5SXlzN1\n6lRee+01FixYUOv2n332Ge3aObNo0HPPPcdTTz3F/v3769ymvLyc8ePHc9VVVzlyrEsvvZS8vLyw\n9p2ItNJOPAmdtKtr06YNQ4YMIT8/H7Cq8ClTptC3b19SU1M5ffo0GRkZvP/++wBs2rQJn89Hs2bN\nyMzMZOHChVX7qvna8vKzF/tp3rw527ZtIy0tjeuuuw6A/Px8Jk2aVGt8ixYt4uqrr470tw3AuHHj\nuOmmm+rdJikpiV69eoUdQ33HSktLY+vWrWHtX6l45+ht7LGg8qp/UVERS5Ys4ZZbbql6bt68eSxZ\nsoQWLVrQoEEDRAQR4eTJkwwfPpwxY8aQl5fHihUruOmmm1i7di3dunU767VJSWf/bjx69CiFhYWM\nHTuWN954g1mzZnHfffcxdOjQWuNcvXo1EyZMsP19bd++nZkzZ9b5/MCBA89InnZGPyxfvpy7777b\nsWNlZWWxdu1aunbtGjAWZdG5RxKPp0k7UlM2hDrayhjDzTffTHJyMhdccAHZ2dlViVFEeOCBB2jb\ntu1Zr1u5ciVlZWU89thjAFx11VVkZ2czd+5cJk2aVO9rK33++edkZWWRmppKYWEhR44cYfTo0Sxf\nvpxevXqdtf2RI0fOmOOioKCA3NxcBg0axNy5c7ntttvIzs6uer5z5848/fTTts+FnfkzPvjgA66/\n/nrmzJnDvn37GDt2bESP1axZMwoKCmzvR6lEZGe5scdFJF9EvhCRuSLSOFIHNyYyj1CJCAsWLODg\nwYPs3LmT6dOn07jxD99e+/bta31dcXHxWc917NiR4uLigK+ttHLlSoYNG0a7du0YN24cy5Yto1+/\nfsyYMaPW7U+fPl31cVlZGb/4xS946KGHGDp0KMXFxfTv3z/g91ufQJX2li1b6NKlC6NGjeL222/n\n2Wefjfixzj33XE6cOBHyfhORVtmJp95KW0QygLuBi4wxx0XkNeA24H+cD817dVWEbdu2paioCGNM\n1TaFhYX07Nkz4Gsr5eXlndG/FhGysrJo0aJFrdsnJ//wX/Xmm2/Sp08fmjZtyrFjxzh8+DAtW7Y8\nY/tgWxaB4v3oo48YNmwYAJs3b6ZJkyYRP9Z3331H8+bN641DqUQXqD3yPXASSBGR00AKsMfxqKLc\ngAEDSElJYcqUKYwbN46PP/6YRYsW2a56SktL2bx5MwUFBVx88cUkJydTWFjIli1bmDhxYq2vSU9P\n5/Dhw5x//vmUlJSQlZUFWMl/4MCBvPPOOwwZMqRq+2BbFrVVv5XVdVJSEgcPHiQzMxOA3NxcHn74\n4YgeC6xhkBdddJHt/SS0AwcgN5ecP/yBnBkzYPhwryNSLqm3PWKMOQA8C+wCioFSY0zCj8tq2LAh\nCxcuZMmSJaSlpXH//feTm5tL9+7dbb1+/vz5jBkzhhdffJHevXtzzz33sHjxYubMmcN5551X62uu\nvPJKPv30UwBGjhzJ7t27WbJkCfv27SMpKYnS0tKQv5/p06cza9Ys/H4/Tz75JN9/b63dfOONN7J0\n6VIAbr31VlatWsUrr7xC69atufPOOyN6LLDGbw8ePDjk7yMhnDoFv/41dO4Mn34KvXvDhAlQywgl\nFZ/qnXtERLoAC4HLge+A+cDrxpg51bYx1f/M9/l8+Hw+nY8hwkpLS5k6dWqt48idcuLECVavXu1K\nIj127BgTJkzgueeec/xYkebqe/1f/4InnoD33oMLL7Qu6vTvD489BtVGPqno4/f78fv9VZ8/+eST\nIc09Eihp3wr81BgzpuLzO4CBxpj7qm2jE0a55Pnnn2fUqFF19r0jbfHixdxwww21DlmMtFdeeYVB\ngwbRo0cPx48Vaa6+12+9FXw++M1vfvjawoVWIl+/Hlz4v1KR4dSEUV8BA0XkXLGuHl0LfBlKgCp8\nDz74IG+99ZZrxxs2bJgrCbuoqIhmzZrFZMJ21cGD8O67VuKukJOTA9nZkJwMddxNq+JLoJ72Z8Bs\nYA3wecWXax+TphwnIrXe3BLr2rdvH/COTAX87//CT38KNUfYiMDEifDUU+GNgVUxQefTVipMrr3X\nf/IT66JjtZuoqhgDP/oR5OSA/gKMCTqftlLxrKAAtm+H66+v/XkReOQR+Otf3Y1LuU6TtlKxYPZs\n+OUvoWHDM758xr0B114Ln3wC1e6eVfFHk7ZS0a68HHJz4d//vf7t0tIgPR02bnQnLuUJTdpKRbsP\nPoBmzaDiLtjqzroL97LL4KOP3IlLeUKTtlLRLi8PbrzR3raatOOeJm2lot2qVTBwYK1P1Vppr1ih\nQ//imCZtpaLZ6dOwerV1q7odXbpYryksdDYu5RlN2kpFs6++si4w1jF1wVmVtoi2SOKcJm11lh07\ndngdQlTYu3cvR44c8TaIelojddKkHdc0aUeZzMxMPvzwQ8+Ov337dlauXOnZ8aNJWloaU6ZM8TaI\nVatgwIA6n651DndN2nEtoZN2RkYGKSkppKamkp6ezujRoykrKwtrf8uWLQsrpo0bN3LFFVeEtY/q\n9u7dy6hRo5gxYwZ5eXmMHz+eNm3acNddd9W6/UsvvcTIkSMjdvzqDh8+zMSJE5k5cybPPvvsGbd+\nb9iwgfHjxzty3OoWLFjAnDlzeOqpp/jLX/5yxnM1Y0hOTmbYsGHMnj3b8bjqtHJlvUm7VllZsGuX\ntVCCij/GmLAe1i7OVtfXo0lGRoZ5//33jTHG7Nmzx2RmZprHHnssrP3l5eWF9NqTJ0+GfNz6Xr9/\n/35z/fXXmxEjRph3333XGGPMhx9+aJo3b37Wths2bDDTpk0LK476jB492uzcudMYY0yvXr2qPn72\n2WfNiBEjzJ133lnna0+fPm0eeugh4/P5Qj7+wYMHTePGjc3Ro0dNeXm5ad68ua0Y7rjjjnr369h7\n/dAhY1JSjDl2LPjXXnutMQsXRj4mFTEV75ugc25CV9rVtWnThiFDhrCx4m6yTZs24fP5aNasGZmZ\nmSxcuLBq22eeeYZ27drRpEkTevbsybJly7jjjjvYtWsXw4cPJzU1lalTpwLWIsC33HILLVu2pHPn\nzvz5z3+u2k9GRgZTpkyhb9++pKamcvr0aTIyMnj//fertqkvjpqvL69l9ZLmzZuzbds20tLSuO66\n6wDIz88/Y33KSosWLeLqq68O80zWbvv27RQXF9OxY0cAli5dWvXxuHHjAs7yl5SURK9evcKKr2nT\npqxdu5ZzzjkHEeHUqVNV1X59MaSlpbF169aQjxuytWuhb19oHMJa2pdfri2SOBVojci4V/lDW1RU\nxJIlS7jllls4deoUw4cPZ8yYMeTl5bFixQpuuukm1qxZgzGGF154gTVr1pCens6uXbs4deoUubm5\nfPTRR/z973+vSizl5eUMHz6cESNG8Nprr1FUVMS1115Ljx49qhLovHnzWLJkCS1atKBBgwaISNXC\ntydPnqw1jrVr19KtW7ezXl/b3NdHjx6lsLCQsWPH8sYbbzBr1izuu+8+hg4deta2q1evZsKECUGd\nP7uL+i5btoymTZuSm5tLaWkpqampZyxZVvn/UJ/ly5fXOjVtMAsL9+7dG7AWKvb5fGRkZASMISsr\ni7Vr19K1a9eAMUaUjdZITk5O3X3t3/3OmbiUp7xN2gFWALctxBsJjDHcfPPNJCcnc8EFF5Cdnc2E\nCRP45JNPKCsr47HHHgPgqquuIjs7m1dffZVRo0Zx/Phx8vPzufDCC+nQoUOd+1+9ejUlJSU88cQT\nAHTq1IkxY8Ywb948rrvuOkSEBx54gLZt29b6+pUrV9Yax9y5c5k0aVLA1wN8/vnnZGVlkZqaSmFh\nIUeOHGH06NEsX76cXr16nbHtkSNHzlopvaCggNzcXAYNGsTcuXO57bbbyK42NajdRX2/+eYbNm7c\nyLx58wC4/PLLGTx4cNUvn0CrwQN88MEHXH/99cyZM4d9+/YxduzYoGKo9OabbzJ//nyeffbZM75e\nVwzNmjWjoKDA9v4jZtUq+PnPQ3vtgAHWSjYnTkCjRpGNS3nK2/aIMZF5hEhEWLBgAQcPHmTnzp1M\nnz6dxo0bU1xcTPv27c/YtmPHjuzZs4cuXbrw/PPPk5OTQ6tWrRg5ciR79+6tdf+FhYUUFxfTrFmz\nqsfTTz/Nt99+W7VNzeNUV1ccxcXFtl4PVuIfNmwY7dq1Y9y4cSxbtox+/foxY8bZa1mcrjE7XFlZ\nGb/4xS946KGHGDp0KMXFxfS3e5NHDU2aNKFPnz5Vn3fo0KFq0WAIXGlXrgw/atQobr/99rMSbjB+\n9rOfMXPmTG644QZ27twZMIZzzz2XEydOhHy8kBhjVdoBhvvVWmUDnHcetG8PW7ZEPjblqYCVtoj0\nAOZV+1Jn4HfGmGmOReWxNm3aUFRUhDGmqvoqLCykZ8+egLUa+siRIzl06BD33nsvjz76KLNnzz6r\nUuvQoQOdOnWqt0qrr8IMFEeg1wPk5eWd0b8WEbKysmpdZzI5+cy3w5tvvkmfPn1o2rQpx44d4/Dh\nw7Rs2fKMbey2Jnr37s2KFSuqvp6UlHRGDz7Q9/HRRx8xbNgwADZv3kyTJk2CjmHx4sX8/ve/5+OP\nP+b888+nZcuWvP7661UjRuqK4bvvvqN5zdVinLZ7t7XyerX2TdB69YIvv7RWbFdxI2DSNsZsBvoB\niEgSsAdwb6FCDwwcOJCUlBSmTJnCuHHj+Pjjj1m0aBE5OTkUFBSwe/duBg8eTOPGjTnnnHOqKrRW\nrVqxbdu2qp52//79SU1NZcqUKfz2t7+lUaNGbNq0iWPHjvHjH/84YBwDBgyoMw47SktL2bx5MwUF\nBVx88cUkJydTWFjIli1bmDhx4lnbp6enc/jwYc4//3wASkpKyKqYWS4vL4+BAwfyzjvvMGTIkKrX\n2G1NDB48+Ix++bZt2874Pmqrciur66SkJA4ePEhmZiYAubm5PPzww0HH0KBBA3w+X9XxioqK6Nu3\nb70xgDVs8qKLLgq4/4iqHJ8d4JdZnT1tsJL1l7qka7wJtj1yLbDNGFPkRDDRomHDhixcuJAlS5aQ\nlpbG/fffT25uLt27d+f48eM8/vjjpKWl0bp1a0pKSqoSxuOPP87kyZNp1qwZzz33HElJSSxatIgN\nGzbQuXNn0tLSuOeee/j+++9txdGoUaM647Bj/vz5jBkzhhdffJHevXtzzz33sHjxYubMmcN55513\n1vZXXnkln376adXnI0eOZPfu3SxZsoR9+/aRlJREaWmprWPX1LhxY3Jycpg4cSJPPPEE9913H126\ndAFg+vTpzJo1C7/fz5NPPll1fm688caqFsqtt97KqlWreOWVV2jduvUZFzHtGjJkCG3btuXPf/4z\nDz/8MP/93/9ddUG4rhjAGr89ePDgkL7vkAW4qcaWykpbxZWg1ogUkVnAGmPMX6p9zdS2D10jMvaU\nlpYydepUJk+e7HUoAJw4cYLVq1e7nzCrOXbsGBMmTOC5556rcxtH3us+n7UeZMUvlZBs2AB33AFf\nfBGxsFTkhLpGpO3RIyLSCBgOPFrzuep/nvl8vqo/QVVsadq0KS1atKCkpKTWnrfb3nvvPW644QZP\nY5g3bx733nuv+wfOz4dqF25D0qMHbN1q9caTE350r+f8fj9+vz/s/diutEXkJuA3xpghNb6ulXYc\nMcbwt7/9rdbx0ImmqKiIdevWBbzxJ+Lv9W+/tRLugQPh9bQBunaFRYug2sVrFR0cr7SBkcCrwR5A\nxRYR0YRdoX379gGHVDqicsRHJO5jqOxra9KOG7YuRIrIeVgXId90NhylFPn5VrK1IeBIIr0YGXds\nJW1jTJkxpoUx5pDTASmV8CI5tlqH/cUdnTBKqWijlbaqhyZtpaJNJCvtnj2hoMBaN1LFhaDGade6\nAx09ohJcRN/r+/ZBt25w8GDkJlTr1AmWLrX2q6KGG6NHgmZn5jalVDWRHDlSqbJFokk7LjiWtLXK\nVioEQfSzwcY4bfghaQcYb65ig/a0lYomTszKpyNI4oombaWiSQiVdkA6giSuOHYhUikVglatYN06\nqGc1oqAdOgTp6da/tSxJp7wR6oVI/R9UKlqUlMDx49Cmje2X2Kq0U1Phwguh2io9KnZp0lYqWnz5\npdXKcGLUVa9eVutFxTxN2kpFiyD72WCz0gZr1kBdLzIuaNJWKlo4uZ5jly6wbZsz+1au0qStVLTI\nzw86aduutDVpxw1N2kpFi8qethM0accNHfKnVDQ4cMCaI6S01JkLkcePQ5MmUFamS49FCR3yp1Qs\nKyiA7t2dSdgAjRtbY7V37XJm/8o1AZO2iDQVkddFZJOIfCkiA90ITKmEUpm0g2S7pw3aIokTdirt\nPwH/MsZcBPQFNjkbklIJqKDA+Vn4unbVpB0H6k3aInIBcLkxZhaAMeaUMeY7VyJTKpFs2eJOpb11\na9DHUNElUKXdCdgnIi+LyDoRmSkiKW4EplRCCbE9EhRtj8SFQJeRk4EfAfcbY1aLyPPAY8DE6htV\n/23v8/nw+XyRjVKpeGaMVWmH0B6xNZ92JU3anvL7/fj9/rD3U++QPxFJBz4xxnSq+Pwy4DFjTHa1\nbXTIn1LhKC6Gfv3gm2+CfmlQSfv7763JqA4dcm6UirLNkSF/xpivgSIRqfy77VpAZ51RKpLCuAgZ\nVE+7SRNISYGvvw7pWCo62Bll/1tgjog0ArYBo50NSakEE+JFyJBUtkhat3bneCriAg75M8Z8Zoy5\n1BiTZYz5mY4eUSrCwrgIGVSlDdrXjgN6R6RSXgvxImRIdKx2zNOkrZTX3K60dax2TNOkrZSXTp+G\nHTusZOoGbY/EPE3aSnlp1y5IS7NGdYQg6Epb2yMxT5O2Ul5y407I6tLSrGlaS0vdO6aKKE3aSnkp\nzIuQQVfaItoiiXGatJXyktuVNmjSjnGatJXyktuVNmhfO8Zp0lbKS1ppqyDpGpFKeeXECWs+kEOH\noGFD9477/vsweTIsX+7eMdVZdI1IpWLN9u3Qvr27CRusBYR37HD3mCpiNGkr5ZUItEZC6mm3bw97\n98LJk2EdW3lDk7ZSXnFzzpHqGja0ZvkrKnL/2CpsmrSV8koEknZIlTZoiySGadJWyitbt3pTaYOV\ntLdv9+bYKiy2kraI7BSRz0VkvYh86nRQSiWELVusMdNh0Eo78dhZuQbAAD5jzAEng1EqYRw7Zq0J\n2aGDN8fv1AkWL/bm2CoswbRHdCVQpSJl+3bo2BGS7dZNtQu50u7cWSvtGGU3aRsgT0TWiMjdTgak\nVELwauRIJW2PxCy7SXuwMaYfcANwn4hc7mBMSsW/rVvD7mdDGJV2erp1J2ZZWdgxKHfZ+tvMGLO3\n4t99IvIW0B9YUfl89TeOz+fD5/NFNEil4s6WLdCnj3fHF7HaMzt3Qu/e3sWRQPx+P36/P+z9BJx7\nRERSgAbGmEMich6wFHjSGLO04nmde0SpYF17LTzyCFx3nXcxDB0K//mfkJ3tXQwJLNS5R+xU2q2A\nt0Skcvs5lQlbKRWiCAz3C5v2tWNSwJ62MWaHMebiikemMeZpNwJTKm5FcLhfyD1t0BtsYpTeEamU\n2yI03C9sWmnHJE3aSrktgsP9wq60NWnHHE3aSrktQsP9wlZ5g40OJIgpmrSVclu0VNpNm0KDBnBA\nZ6eIJZq0lXKbl7P71aQtkpijSVspt0VwuF9YlTZo0o5BmrSVcpPXs/vVpEk75mjSVspNER7up5V2\n4tGkrZSbvJ7drya9wSbmaNJWyk0RHu6nlXbi0aStlJuirdLOyIBdu6C83OtIlE2atJVyU4Qnigq7\n0j73XGjeHPbsiUg8ynmatJVy05Yt0L2711GcSZceiykB59MOuAOdT1spe8rKoEULOHzYuhMxWvzq\nV3D11XDnnV5HklBCnU9bK22l3LJ1K3TpEl0JG6xKW0eQxAxN2kq5paAg4q2RsHvaoEk7xthK2iLS\nQETWi8hCpwNSKm5t3gw9engdxdk0accUu5X2g8CXgDavlQpVtFbaXbpo0o4hAZO2iLQDhgJ/A4Ju\nmiulKjiQtCMiPR2+/966UKqinp1K+4/Aw4COvlcqVMZY7ZForLRF9M7IGFJv0haRbOBbY8x6tMpW\nKnQlJVZybNHC60hqp33tmBFoqrGfADeKyFDgHKCJiMw2xvyq+kbVf9v7fD58Pl+Ew1QqxlW2RiSy\ntU9EKm3QpO0Cv9+P3+8Pez+2b64RkSuB8caY4TW+rjfXKBXIyy/D8uUwe7bXkdTuT3+Cbdtg2jSv\nI0kYbt1co9lZqVA4NNxPK+3EYztpG2M+MMbc6GQwSsWtaB05UkmTdszQuUeUckNmJsyZA1lZXkdS\nuyNHfpgXJUlvlHaDzj2iVLQ6fdrqF0fTPNo1paTABRfA1197HYkKQJO2Uk7btQvS0qzEGGER62mD\n1SLZti1y+1OO0KStlNOivZ9dSfvaMUGTtlJOc+BOyEoRr7Q1aUc9TdpKOa2gIDpn96tJk3ZM0KSt\nlNMcbI9opZ14NGkr5TQH2yMRpUk7Jug4baWcdPQoNGtmTXsabcuM1VReDuedB/v3OzLSRZ1Jx2kr\nFY0KCqwKNtoTNlg31WRk6BStUU6TtlJOys+H3r0d231Ee9qgLZIYoElbKSd9+aWjSTviunSxVo1X\nUUuTtlJOirVKu3t32LIlsvtUEaVJWykn5edDr15eR2Ff9+5WH15FLU3aSjnl2DEoKnJ0oqiIV9rd\numnSjnKatJVyyubN1oW9Ro28jsS+Dh1g3z5rqlYVlQImbRE5R0RWicgGEflSRJ52IzClYp4LrZGI\nV9oNGli/aPRiZNQKmLSNMceAq4wxFwN9gatE5DLHI1Mq1sXayJFKejEyqtlqjxhjKv9WagQ0AA44\nFpFS8cLhkSPgQKUNejEyytlK2iKSJCIbgG+A5caYL50NS6k4EGsjRypp0o5qyXY2MsaUAxeLyAXA\nuyLiM8b4K5+v/tve5/Ph8/kiG6VSscaFkSPgYKU9a1bk95vg/H4/fr8/7P0EPWGUiPwOOGqMmVrx\nuU4YpVRNn30Gv/ylVW3Hmq+/hj59rFEkyjGOTRglIi1EpGnFx+cCPwXWBx+iUgnEpdaII5V2q1Zw\n/Dgc0EtX0chOT7s1sKyip70KWGiMed/ZsJSKcbE6cgRAREeQRLGAPW1jzBfAj1yIRan4kZ9vtUcc\n5kilDT9cjBwwwJn9q5DpHZFKOSFWR45U0hEkUUuTtlKR5tLIEXCh0lZRR5O2UpEWi3OO1KQ97ail\na0QqFWlz58Jbb8H8+V5HErrvvoO2beHQIevCpIo4XSNSqWixcSNkZnodRXguuMBa5HfvXq8jUTVo\n0lYq0tatgx+5M+DKsZ42aF87SmnSViqSjHE1aTtKk3ZU0qStVCTt2WP1gNu0ceVwWmknHk3aSkVS\nZZUdDxfvune3RsKoqKJJW6lIcrk14milfdFF1u34Kqpo0lYqktatg0su8TqKyOja1Ro9cuiQ15Go\nanSctlKR1LYtfPwxZGR4HUlkXHIJvPACDBzodSRxR8dpK+W1r7+Go0ehY0evI4mcPn3giy+8jkJV\no0lbqUhZv971i5CO9rQB+vaFzz939hgqKJq0lYqUeBmfXZ1W2lFHe9pKRcott8DPfw633eZ1JJHz\nzTfWFLMlJfExjDGKOLncWHsRWS4i+SKyUUQeCC1EpeJcPFbarVpBgwZQXOx1JKqCnfbISeC/jDG9\ngYHAfSJykbNhKRVjDhyA/futYXIucrynDdoiiTIBk7Yx5mtjzIaKjw8DmwB37tFVKlasXw8XXwxJ\ncXiZqG9fTdpRJKh3mIhkAP2wFvhVSlXyqDXiWqWtI0iiRsCFfSuJyPnA68CDFRV3lepvHJ/Ph8/n\ni1B4SsWIdetg6FCvo3BGnz4wbZrXUcQ8v9+P3+8Pez+2Ro+ISENgEbDEGPN8jed09IhSXbvCggXQ\nu7erh83JyXG+2j5yBC68EL7/Hho2dPZYCcTJ0SMC/B34smbCVkphjaw4eNCaYCkepaRA+/Y6TWuU\nsNPTHgyTF6I4AAAKHElEQVSMAq4SkfUVjyEOx6VU7FixAi6/3JOLkK70tEEvRkaRgD1tY8xH6J2T\nStXtww/hiiu8jsJZlRcj4+nGoRilyVipcH34oVVpe8C1SlvHakcNTdpKhWP/figshH79vI7EWdoe\niRo694hS4fjnP635pt991+tInFVeDk2awO7d0LSp19HEBZ1PWykvJEI/G6yLrJdcAp9+6nUkCU+T\ntlLh8Dhpu9bTBvjJT+D//s+946laadJWKlSHDlkL3156qdeRuEOTdlTQnrZSoVq6FCZPtqrtRFBS\nAl26WDMaNmjgdTQxT3vaSrltxYrE6GdXatECWreGjRu9jiShadJWKlRRcBHS1Z42WC2Sjz9295jq\nDJq0lQrFsWOwdi0MGuR1JO4aPFj72h7TnrZSocjLgyeegJUrvY7EXZs2WVPQ7tjhdSQxT3vaSrlp\nwQK46Savo3Bfjx7WFK26ZqRnNGkrFSxj4O234eabvY7E/Z52UpLVEvrkE3ePq6po0lYqWOvWWXNM\n9+zpdSTe0IuRntKkrVSwKqtsCbodGXGuV9qgFyM9pklbqWC9/XZi9rMrXXqpNePf0aNeR5KQ7Cw3\nNktEvhERnZdRqW3bYN8+GDDA60gAjyrtlBTo1QvWrHH/2MpWpf0yoMuLKQXWqJEbb9TbuC+/HCKw\nsrgKXsCkbYxZARx0IRalol+UjBqp5EmlDZCdbf0CU67TnrZSdn37rbVO4tVXex2J9664AnbuhF27\nvI4k4QRc2NeO6r/tn3zSB/gisVulospoFjGE67j13HO8DqWanIqH25J5mWzWdlzAdH7rwfFjkR/w\nM2lSeHuxdRu7iGQAC40xfWp5Tm9jV4nhssvgoYdgxAivI6mSk5PjXYvk7bdh2jRYtsyb48e4UG9j\n16StlB3r11vD/LZvh+SI/IEa+44cgfR0ax6SCy/0OpqY49jcIyLyKvB/QHcRKRKR0aEEqFRMe+EF\n+I//0IRdXUoKXHMNLFrkdSQJxc7okZHGmDbGmMbGmPbGmJfdCEypqHHgALzxBowZ43UkZ/GsNVJp\nxAh46y1vY0gwOnpEqUBmzYLhw6FlS68jiT7Z2VZP+8gRryNJGDqftlL1OX0aunWDV1+Nmrsgo841\n18D990fVBdpYoPNpK+WEd96xLrL17+91JNFLWySu0qStVH2mTbOqyCiY0a82nve0Af7t36yLkSUl\nXkeSEDRpK1WX996DLVvg1lu9jiS6pafDLbfA9OleR5IQtKetVG1OnICsLHjmGWuCKFW/ggJrnu0d\nO+D8872OJiZoT1upSJo2DTIyrFEjKrDu3cHng7/9zetI4p4mbaVq2rsX/vAH+NOforaXXSkqetqV\nHn0UnnvO+itFOUaTtlI1PfII3H23VT0q+378Y+ucvfqq15HENe1pK1Xd4sXW7eqbNmlvNhR5efDA\nA7Bxo7Vyu6qT9rSVCtf69TB6NMyfrwk7VNdcY41r/+MfvY4kbmnSVgqgqMi66PjiizBwoNfR2BZV\nPW2wrgH84x8wZQp88onX0cQlTdpKffcdDB0K48ZZ441VeDp2hJkzYeRIa7ItFVHa01aJbccO+PnP\nYdAga5hflI8WiSkPPWTdnLRggZ7XWmhPW6lgvf22NQnUHXdownbC009b62o+/LA18ZaKCDuLIAwR\nka9EZIuIPOpGUEo5av9+ePBBGDsWFi60Po7RhB11Pe3qGjWyzu/69XD99VYCV2GrN2mLSANgOjAE\n6AWMFJGL3AgsFvn9fq9DiBpReS6Ki60/2bt1g6NHYd06V6Zbjcpz4Za0NFi61Lq4e8kl+KdN8zqi\nmBeo0u4PbDXG7DTGnATmATc5H1ZsSugfzhqi5lzs3Al/+Ys1WX9mJhgDn38OM2ZA8+auhODkuYjq\nSrtSgwYweTL89a/4n3jCWiD5H/+AY8e8jiwmBVrwri1QVO3z3YDOBK+ix6lTUFoKX39t3X6+Z491\nY0x+vvUoK4MbbrD61v/4BzRt6nXEiWvYMKsl1a+fNbTyv/7LSuCXXGI9evaEVq2stSdVnQIl7eCG\nhUyZAitWhB5NrNu8Gdau9TqK6LB5M6xZU/tz1UcbGVP7o7zcepw+bSXm06fh+PEfHmVlcOiQ9XGT\nJtC69Q+Pnj3h17+G3r2ha9e4vjMvJycnNqrtSklJ1qIJI0ZAYSGsXGn9zEydCtu2wTffWJV5ixZW\n8j7nHDj3XGtB5QYNfniInPmoFO3XJu66K+wVfuod8iciA4EcY8yQis8fB8qNMc9U20bH+ymlVAhC\nGfIXKGknA5uBa4Bi4FNgpDFmU6hBKqWUCl297RFjzCkRuR94F2gA/F0TtlJKeSfsOyKVUkq5x/YV\nGjs32YjItIrnPxORfpELM7oEOhcicnvFOfhcRD4Wkb5exOkGuzdficilInJKRH7mZnxusvkz4hOR\n9SKyUUT8LofoGhs/Iy1E5B0R2VBxLu70IEzHicgsEflGRL6oZ5vg8qYxJuADqzWyFcgAGgIbgItq\nbDMU+FfFxwOAlXb2HWsPm+diEHBBxcdDEvlcVNtuGbAIuMXruD18XzQF8oF2FZ+38DpuD89FDvB0\n5XkA9gPJXsfuwLm4HOgHfFHH80HnTbuVtp2bbG4E/gfAGLMKaCoirWzuP5YEPBfGmE+MMd9VfLoK\naOdyjG6xe/PVb4HXgX1uBucyO+fil8AbxpjdAMaYEpdjdIudc7EXaFLxcRNgvzHmlIsxusIYswI4\nWM8mQedNu0m7tpts2trYJh6TlZ1zUd2vgX85GpF3Ap4LEWmL9QP7YsWX4vUiip33RTeguYgsF5E1\nInKHa9G5y865mAn0FpFi4DPgQZdiizZB581AN9dUsvuDVnPMYTz+gNr+nkTkKuAuYLBz4XjKzrl4\nHnjMGGNERDj7PRIv7JyLhsCPsIbQpgCfiMhKY8wWRyNzn51zMQHYYIzxiUgX4D0RyTLGHHI4tmgU\nVN60m7T3AO2rfd4e6zdCfdu0q/havLFzLqi4+DgTGGKMqe/Po1hm51xcAsyz8jUtgBtE5KQx5p/u\nhOgaO+eiCCgxxhwFjorIh0AWEG9J2865+Anw/wCMMdtEZAfQA6jjNtq4FXTetNseWQN0E5EMEWkE\n3ArU/KH7J/ArqLqTstQY843N/ceSgOdCRDoAbwKjjDFbPYjRLQHPhTGmszGmkzGmE1Zf+zdxmLDB\n3s/IAuAyEWkgIilYF56+dDlON9g5F18B1wJU9HB7ANtdjTI6BJ03bVXapo6bbETk3ornXzLG/EtE\nhorIVqAMGB3GNxK17JwLYCLQDHixosI8aYzp71XMTrF5LhKCzZ+Rr0TkHeBzoByYaYyJu6Rt833x\ne+BlEfkMq3h8xBgTd2uTicirwJVACxEpAiZhtclCzpt6c41SSsWQ+J3+TCml4pAmbaWUiiGatJVS\nKoZo0lZKqRiiSVsppWKIJm2llIohmrSVUiqGaNJWSqkY8v8B7qRrTBuAXeoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b82bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def log_beta_pdf(x, a, b):\n",
    "    return - gammaln(a) - gammaln(b) + gammaln(a+b) + np.log(x)*(a-1) + np.log(1-x)*(b-1) \n",
    "\n",
    "x = np.arange(0.01,1,0.01)\n",
    "\n",
    "pi_ML = c_1/N\n",
    "\n",
    "plt.plot(x, np.exp(log_beta_pdf(x, a, b)), 'b')\n",
    "plt.plot(x, np.exp(log_beta_pdf(x, a+c_1, b+c_0)), 'r')\n",
    "yl = plt.gca().get_ylim()\n",
    "plt.plot([pi_ML, pi_ML], yl , 'k:')\n",
    "plt.legend([\"Prior $\\cal B$ $(a={}, b={})$\".format(a,b), \"Posterior $\\cal B$ $(a={}, b={})$\".format(a+c_1, b+c_0)], loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change point\n",
    "Coin switch\n",
    "\n",
    "Coal Mining Data\n",
    " Single Change Point\n",
    " Multiple Change Point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
