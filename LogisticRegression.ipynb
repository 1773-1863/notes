{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a binary classification method. The key idea is learning a mapping from a feature vector to a probability, a number between $0$ and $1$. It is similar to least-squares in the sense that (apart from some extreme cases) it has a unique solution.\n",
    "\n",
    "Suppose, for a set of objects $X$, each denoted by the feature vector $x_i \\in \\mathbb{R}^D$, we are given the answer to some true-false question, such as 'is object $i$ of class $c$?'. This answer is denoted by $y_i \\in \\{0, 1\\}$. We are given a dataset of feature vectors $x_i$ along with the corresponding 'labels' $y_i$. For $i=1\\dots N$\n",
    "\n",
    "$$(y_i, x_i)$$\n",
    "\n",
    "The model is \n",
    "$$\n",
    "\\Pr\\{y_i = 1\\} = \\sigma(x_i^\\top w)\n",
    "$$\n",
    "Here,\n",
    "$\\sigma(x)$ is the sigmoid function defined as\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{1}{1+e^{-x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "This is a generative model. To understand logistic regression as a generative model, consider the following metaphor: assume that for each data instance $x_i$, we select a biased coin with probability $p(y_i = 1| w, x) = \\pi_i = \\sigma(w^\\top x_i)$, throw the coin and label the data item with class $y_i$ accordingly. \n",
    "\n",
    "\n",
    "Mathematically, we assume that each label $y_i$ is drawn from a Bernoulli distribution. That is: \n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\sigma(x_i^\\top w) \\\\\n",
    "y_i & \\sim &\\mathcal{BE}(\\pi)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here, we think of a biased coin with two sides denoted as $1$ and $0$ with probability of side $1$ as $\\pi$, and consequently the probability of side $0$ with $1-\\pi$. We denote the outcome of the coin toss with the random variable $y$. We write the probability as $p(y = 1) = \\pi$ and probability of heads is $p(y = 0) = 1-\\pi$. More compactly, the probability of the outcome of a toss, provided we know $\\pi$, is written as\n",
    "\\begin{eqnarray}\n",
    "p(y|\\pi) = \\pi^y(1-\\pi)^y\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we are given a dataset of form\n",
    "\\begin{eqnarray}\n",
    "X & = &  \\begin{pmatrix}\n",
    "  x_{1,1} & x_{1,2} & \\dots & x_{1,D} \\\\\n",
    "  x_{2,1} & x_{2,2} & \\dots & x_{2,D} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots  \\\\\n",
    "  x_{i,1} & x_{i,2} & \\dots & x_{i,D}  \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots  \\\\\n",
    "  x_{N,1} & x_{N,2} & \\dots & x_{N,D} \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "x_1^\\top \\\\\n",
    "x_2^\\top \\\\\n",
    "\\dots \\\\\n",
    "x_i^\\top \\\\\n",
    "\\dots \\\\\n",
    "x_N^\\top\n",
    "\\end{pmatrix} \n",
    "\\\\\n",
    "{y} & = & \\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_i \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "where $x_{i,j}$ denotes the $j$'th feature of the $i$'th data point. It is customary, to set a column entirely to $1$, for example $x_{i,D}=1$ for all $i$. This 'feature' is artificially added to the dataset to allow a slightly more flexible model. The $y_i$ denote the target class label of the\n",
    "$i$'th object. In logistic regression, we consider the case of binary classification where $y_i \\in \\{0,1\\}$. It is possible to use other encodings such as $y_i \\in \\{-1,1\\}$; the derivations are similar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the sigmoid function\n",
    "Note that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{e^x}{(1+e^{-x})e^x} = \\frac{e^x}{1+e^{x}} \\\\\n",
    "1 - \\sigma(x) & = & 1 - \\frac{e^x}{1+e^{x}} = \\frac{1+e^{x} - e^x}{1+e^{x}} = \\frac{1}{1+e^{x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma'(x) & = & \\frac{e^x(1+e^{x}) - e^{x} e^x}{(1+e^{x})^2} = \\frac{e^x}{1+e^{x}}\\frac{1}{1+e^{x}} = \\sigma(x) (1-\\sigma(x))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log \\sigma(x) & = & -\\log(1+e^{-x}) = x - \\log(1+e^{x}) \\\\\n",
    "\\log(1 - \\sigma(x)) & = &  -\\log({1+e^{x}})\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Exercise: Plot the sigmoid function and its derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the parameters\n",
    "\n",
    "The likelihood of the observations, that is the probability of observing the class sequence is\n",
    "\\begin{eqnarray}\n",
    "p(y_1, y_2, \\dots, y_N|w, x_1, x_2, \\dots, x_N ) &=& \\left(\\prod_{i : y_i=1} \\sigma(w^\\top x_i) \\right) \\left(\\prod_{i : y_i=0}(1- \\sigma(w^\\top x_i)) \\right)\n",
    "\\end{eqnarray}\n",
    "Here, the left product is the expression for examples from class $1$ and the right product is for examples from class $0$.\n",
    "We will look for the particular setting of the weight vector, the so called maximum likelihood solution, denoted by $w^*$.\n",
    "\\begin{eqnarray}\n",
    "w^* & = & \\arg\\max_{w} {\\cal L}(w)\n",
    "\\end{eqnarray}\n",
    "where the loglikelihood function\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(w) & = & \\log p(y_1, y_2, \\dots, y_N|w, x_1, x_2, \\dots, x_N ) \\\\\n",
    "& = & \\sum_{i : y_i=1} \\log \\sigma(w^\\top x_i) + \\sum_{i : y_i=0} \\log (1- \\sigma(w^\\top x_i)) \\\\\n",
    "& = & \\sum_{i : y_i=1} w^\\top x_i - \\sum_{i : y_i=1} \\log(1+e^{w^\\top x_i}) - \\sum_{i : y_i=0}\\log({1+e^{w^\\top x_i}}) \\\\\n",
    "& = & \\sum_i y_i w^\\top x_i - \\sum_{i} \\log(1+e^{w^\\top x_i}) \\\\\n",
    "& = & y^\\top X w - \\mathbf{1}^\\top logsumexp(0, X w)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Unlike the least-squares problem, an expression for direct evaluation of $w^*$ is not known so we need to resort to numerical optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization via gradient ascent\n",
    "\n",
    "One way for\n",
    "optimization is gradient ascent\n",
    "\\begin{eqnarray}\n",
    "w^{(\\tau)} & \\leftarrow & w^{(\\tau-1)} + \\eta \\nabla_w {\\cal L}\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\nabla_w {\\cal L} & = &\n",
    "\\begin{pmatrix}\n",
    "{\\partial {\\cal L}}/{\\partial w_1} \\\\\n",
    "{\\partial {\\cal L}}/{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "{\\partial {\\cal L}}/{\\partial w_{D}}\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "is the gradient vector.\n",
    "\n",
    "#### Evaluating the gradient\n",
    "The partial derivative of the loglikelihood with respect to the $k$'th entry of the weight vector is given by the chain rule as\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\frac{\\partial{\\cal L}}{\\partial \\sigma(u)} \\frac{\\partial \\sigma(u)}{\\partial u} \\frac{\\partial u}{\\partial w_k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(w) & = & \\sum_{i : y_i=1} \\log \\sigma(w^\\top x_i) + \\sum_{i : y_i=0} \\log (1- \\sigma(w^\\top x_i))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}(\\sigma)}{\\partial \\sigma} & = &  \\sum_{i : y_i=1} \\frac{1}{\\sigma(w^\\top x_i)} - \\sum_{i : y_i=0} \\frac{1}{1- \\sigma(w^\\top x_i)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\sigma(u)}{\\partial u} & = & \\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial w^\\top x_i }{\\partial w_k} & = & x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "So the gradient is\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\sum_{i : y_i=1} \\frac{\\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))}{\\sigma(w^\\top x_i)} x_{i,k} - \\sum_{i : y_i=0} \\frac{\\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))}{1- \\sigma(w^\\top x_i)} x_{i,k} \\\\\n",
    "& = & \\sum_{i : y_i=1} {(1-\\sigma(w^\\top x_i))} x_{i,k} - \\sum_{i : y_i=0} {\\sigma(w^\\top x_i)} x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can write this expression more compactly by noting\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\sum_{i : y_i=1} {(\\underbrace{1}_{y_i}-\\sigma(w^\\top x_i))} x_{i,k} + \\sum_{i : y_i=0} {(\\underbrace{0}_{y_i} - \\sigma(w^\\top x_i))} x_{i,k} \\\\\n",
    "& = & \\sum_i (y_i - \\sigma(w^\\top x_i)) x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The update rule is\n",
    "\\begin{eqnarray}\n",
    "w^{(\\tau)} = w^{(\\tau-1)} + \\eta X^\\top (y-\\sigma(X w))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cemgil/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from cvxpy import *\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "[[-1.  1.  1.]\n",
      " [ 2. -1.  1.]\n",
      " [-1. -1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 2.  1.  1.]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "[[ 0.98201379]\n",
      " [ 0.95257413]\n",
      " [ 0.5       ]\n",
      " [ 0.99752738]\n",
      " [ 0.99908895]]\n",
      "[[ 0.          0.63793365]\n",
      " [ 0.          0.52882577]\n",
      " [ 1.          0.47117423]\n",
      " [ 1.          0.67267636]\n",
      " [ 1.          0.68938999]]\n"
     ]
    }
   ],
   "source": [
    "#x = np.matrix('[-2,1; -1,2; 1,5; -1,1; -3,-2; 1,1] ')\n",
    "x = np.matrix('[-1,1;2,-1;-1,-1;1,1;2,1]')\n",
    "#y = np.matrix('[0,0,1,0,0,1]').T\n",
    "y = np.matrix('[0,0,1,1,1]').T\n",
    "N = x.shape[0]\n",
    "#A = np.hstack((np.power(x,0), np.power(x,1), np.power(x,2)))\n",
    "X = np.hstack((x, np.ones((N,1)) ))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "print(y)\n",
    "print(X)\n",
    "\n",
    "#w = np.random.randn(3,1)\n",
    "\n",
    "w = np.mat('[1;2;3]')\n",
    "\n",
    "print(w)\n",
    "\n",
    "print(sigmoid(X*w))\n",
    "eta = 0.1\n",
    "\n",
    "for i in range(10000):\n",
    "    pr = sigmoid(X*w)\n",
    "    w = w + eta*X.T*(y-pr)\n",
    "    \n",
    "print(np.hstack((y,pr)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "K = 10\n",
    "Ke = 40-K\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x = np.matrix(np.random.randn(N, K))\n",
    "w_true = np.random.randn(K,1)\n",
    "\n",
    "p = sigmoid(x*w_true)\n",
    "u = np.random.rand(N,1)\n",
    "y = (u < p)\n",
    "y = y.astype(np.float64)\n",
    "\n",
    "#A = np.hstack((np.power(x,0), np.power(x,1), np.power(x,2)))\n",
    "X = np.hstack((x, np.random.randn(N, Ke )))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.61873692e-01]\n",
      " [ -3.45888873e-11]\n",
      " [ -1.15100442e+00]\n",
      " [  2.96918652e-01]\n",
      " [  6.06625465e-01]\n",
      " [  6.42161156e-01]\n",
      " [ -5.66000985e-01]\n",
      " [ -1.22920576e-10]\n",
      " [ -7.62241790e-01]\n",
      " [  2.26527800e-10]\n",
      " [  5.59296917e-11]\n",
      " [ -1.06575891e-10]\n",
      " [ -3.80442713e-11]\n",
      " [  7.35221115e-10]\n",
      " [  9.11113530e-13]\n",
      " [  7.95399232e-13]\n",
      " [  3.46032187e-11]\n",
      " [ -5.92069678e-13]\n",
      " [ -3.68158703e-11]\n",
      " [ -1.28123391e-11]\n",
      " [ -1.18309432e-11]\n",
      " [ -5.20654701e-12]\n",
      " [ -6.03313857e-11]\n",
      " [ -5.89164342e-12]\n",
      " [ -3.35170186e-10]\n",
      " [  2.63155232e-11]\n",
      " [ -1.37202788e-11]\n",
      " [  3.38273484e-11]\n",
      " [  2.84843711e-11]\n",
      " [ -2.88087419e-11]\n",
      " [ -2.03696923e-11]\n",
      " [ -1.55134247e-11]\n",
      " [  7.06091201e-12]\n",
      " [  3.72090301e-11]\n",
      " [ -3.33548890e-12]\n",
      " [  6.84441048e-12]\n",
      " [ -5.19792493e-10]\n",
      " [ -2.36637916e-11]\n",
      " [ -6.33682911e-12]\n",
      " [  7.44414549e-04]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFw9JREFUeJzt3X+MHPd53/H3wwgyahrpKWlMWzRMMnRS10IDRUZVpVdU\n67iMqCixndqt7Rzb0gUUIZbkorVZKaWZO4I91AqFtLZ+wFEsR3Ys10EL1JZ0UCMK9jUQCyWCJcay\nS/0odcfIpKQGtTeJ5MAQxKd/7B65d7d7t3u73Jndeb+Ag3Zm52aeGUmfnXvmO7ORmUiSqmVT0QVI\nkobP8JekCjL8JamCDH9JqiDDX5IqyPCXpAoaSPhHxN0R8VJEfKvD+1dGRD0iHm/+fHIQ25UkbcwF\nA1rP7wG3AV9cY5k/ysz3DGh7kqQ+DOTMPzMfAb6/zmIxiG1Jkvo3zJ7/z0XEsYiYi4h3DHG7kqQV\nBtX2Wc83gbdm5g8i4mrgq8BPD2nbkqQVhhL+mflyy+sHI+LOiPixzPzeymUjwocNSVKPMrOn1vog\n2z5Bh75+RGxpeX05EO2Cf0lmdvyZnp5e8/0y/JS9xrLXZ43WWLafste4EQM584+ILwM14Mcj4s+A\naeDCRo7nXcAHIuLXgVeBvwY+OIjtSpI2ZiDhn5m/us77dwB3DGJbkqT+jdwdvrVaregS1lX2Gste\nH1jjoFjjYIxCjb2KjfaLzpeIyLLVJEllFhFkgRd8JUkjwvCXpAoy/CWpggx/Saogw1+SKsjwl6QK\nMvwlqYIMf0mqIMNfkirI8JekCjL8JamCDH9JqiDDX5IqyPCXpAoy/CWpggx/Saogw39Q5uagXl8+\nr15vzJekkjH8B2VyEvbvP/cBUK83picni61LktrwaxwHaSnw9+2Dw4dhdhYmJoquStKY28jXOBr+\ng7a4SOzYTi4swvbtxdYiqRL8Dt+i1euNM35o/HPlNQBJKomBhH9E3B0RL0XEt9ZY5jMR8WxEHIuI\nSwex3VJZavnMzjamZ2eXXwOQpBIZ1Jn/7wFXdXozIq4GdmbmTwHXAZ8d0HbL4+jR5T3+iYnG9NGj\nxdYlSW0MrOcfEduA+zPzZ9q891ngG5n5B83p40AtM19qs+xo9/yBCBjxXZA0Qsrc898KPN8yfao5\nT5JUgAuKLqCdmZmZs69rtRq1Wq2wWiSpbObn55mfn+9rHUW1fZ4CrhyZts/cXONmrdYx+/V6o59/\nzTWrFrftI2mYim77RPOnnfuAfwEQEVcA9XbBX1revStpzAzkzD8ivgzUgB8HXgKmgQuBzMy7msvc\nDuwGXgE+kpmPd1hX+c78oae7dz3zlzRM3uF7vnV5967hL2mYim77jDfv3pU0Rgz/bnj3rqQxY9un\nGy2jfc62dBztI6kk7PlvVA9DObsJdsNf0jDZ898oh3JKqhjP/Jd0OZTTM39JZWPbp19dDOU0/CWV\njW2ffjiUU1KFGP7gUE5JlWPbB3oaymnbR1LZ2PMfyPbXDm7DX1LZ2POXJHXF8JekCjL8JamCDH9J\nqiDDf4AWFk6yZ89BAPbsOcjCwsmCK5Kk9hzts2r7Gxvts7Bwkl27buPEiYPAZuAVdu6c5siRG9mx\nY9v5KleSHO1TpAMH7mkJfoDNnDhxkAMH7imwKklqz/Dv0notnVOnznAu+Jds5vTpM8MpUJJ6YPh3\nYamlc++9nwDg3ns/wa5dty37ANi6dRON76Zv9QoXX+whllQ+JlMXumnpHDq0l507pzn3AdDo+R86\ntHeIlUpSdwz/LnTT0tmxYxtHjtzI1NStAExN3erFXkmlZfh3oduWzo4d2/jSl6YB+NKXpg1+SaU1\nkPCPiN0R8VREPBMRN7V5/8qIqEfE482fTw5iu8NiS0fSuOk7/CNiE3A7cBVwCfDhiHh7m0X/KDMv\na/78h363O0y2dCSNm75v8oqIK4DpzLy6OX0zkJl5S8syVwKfyMxf7mJ9I3mTV6/LSNKgFHWT11bg\n+Zbp7zbnrfRzEXEsIuYi4h0D2K4kaYMuGNJ2vgm8NTN/EBFXA18FfrrTwjMzM2df12o1arXa+a5P\nkkbG/Pw88/Pzfa1jUG2fmczc3Zxe1fZp8zsLwDsz83tt3rPtI0k9KKrt8xjwtojYFhEXAh8C7ltR\n2JaW15fT+NBZFfySpOHou+2Tma9FxA3AQzQ+TO7OzOMRcV3j7bwL+EBE/DrwKvDXwAf73a4kaeN8\npPOq7dv2kTRafKSzJKkrhr8kVZDhL0kVZPhLUgUZ/pJUQYZ/03pf0yhJ48Shnpz7msZz39bVeGRz\nuyd3OtRTUtk41HODuvmaRkkaJ4Y/3X1NoySNE8Of7r+mUZLGhemGX9MoqXoMf/yaRknV42ifVdv3\nwW6SRoujfSRJXTH8JamCDH9JqiDDX5IqyPCXpAoy/MfF3BzU68vn1euN+ZK0guE/LiYnYf/+cx8A\n9XpjenKy2LoklZLj/Fdtf4TH+S8F/r59cPgwzM7CxEQBhUgapo2M8zf8V21/hMMfYHGR2LGdXFiE\n7dsLKkLSMHmTV9XV640zfmj8c+U1AElqGkj4R8TuiHgqIp6JiJs6LPOZiHg2Io5FxKWD2K5aLLV8\nZmcb07Ozy68BSFKLC/pdQURsAm4H3g2cBh6LiK9l5lMty1wN7MzMn4qIvw98Frii3223Wlg4yYED\n93Dq1Bm2bt3EoUN7R+7BbH3tw9GjnPy169h/w6eBafbc8Glm913HtqNH4Zprul7/esuU/f0y1OA+\nlKOGKuxDXzKzrx8aIf5gy/TNwE0rlvks8MGW6ePAlg7ry14999xi7tz58YSXs9Ftfzl37vx4Pvfc\nYs/rWm/z3ZS3gV3oex/W+/1u1t/vOop+vww1uA/uwzBrXNLMzd6yu9dfWLUCeD9wV8v0HuAzK5a5\nH/gHLdMPA5d1WN+qHVvP1NRMywHKswdqamqm53UVFf797sN6v9/N+vtdR9Hvl6EG98F9GGaNSzYS\n/n2P9omI9wNXZeavNaf3AJdn5sdalrkf+I+Z+b+a0w8D/y4zH2+zvpyenj47XavVqNVqrQv0Va8k\njaKgkdXvetc0v/mb72J+fv7sewcPHiR7HO3T0ydFux8abZ//0TLdTdvnKTbY9mn3dm+fkGuufijO\nxz5U4UzHfShHje5DeWpcwgbO/HtauO0K4EeA/wNsAy4EjgF/Z8UyvwjMNV9fATy6xvpW7djynVw9\n74XPfT4v3X59y4F6OS/dfn2+8LnPd/X7w9auht76e73//jj0ON2HctToPpSnxiUbCf+B3OQVEbuB\nT9MYOnp3Zn4qIq5rFnRXc5nbgd00vij3I9mm5dNcLteqqe0NVPU6f3njx/jEDy/md//rp7j2n97M\nra87zY/e9plVd7iW4Vu2OtWwdGX/3nunmZo62PHK/nq/f/r0GS6+uPPIgU7vD2IdRb9fhhrch3LU\nUIV9WFKJO3w7hndznHvceQf50es7PtqgzOE/qPclVUu1wx+6erRBGYLT8Jc0SNV+vEPz0Qa5sOij\nDSRpHeMR/q2PNti+3UcbSNI6xqPtMzfXeG59a4+/XoeWRxus+ftDZttH0iDZ8+9q/cUHp+EvaZCq\n3fOXJHXN8JekCjL8JamCDH9JqiDDvyzm5lYPTa3XG/MlacAM/7KYnFx+b8LSvQuTk8XWJWksOdSz\nAOPwfCJJ5eE4/67WX3xwjsPziSSVh+P8R53PJ5I0JIZ/Wfh8IklDZNunAOPwfCJJ5WHPv6v1Fx+c\n47APksrDnr8kqSuGvyRVkOEvSRVk+EtSBRn+klRBIxP+Cwsn2bPnIAB79hxkYeFkwRVJ0ujqa6hn\nRFwE/AGwDVgE/llm/kWb5RaBvwDOAK9m5uVrrHPVUM+FhZPs2nUbJ04cBDYDr7Bz5zRHjtzIjh3b\neqy5+GGSDvWUNEhFDPW8GXg4M/828HXgNzosdwaoZebPrhX8nRw4cE9L8ANs5sSJgxw4cM9Gapak\nyus3/N8LfKH5+gvA+zosF/1s69SpM5wL/iWbOX36zEZXKUmV1m/4vzEzXwLIzBeBN3ZYLoEjEfFY\nRFzb60a2bt0EvLJi7itcfPHIXLKQpFK5YL0FIuIIsKV1Fo0w/2SbxTt1oicz84WI+AkaHwLHM/OR\nTtucmZk5+7pWq3Ho0F4efXR6Vc//0KEb1ytfksbO/Pw88/Pz/a0kMzf8AxwHtjRfvwk43sXvTAP/\ndo33s53nnlvMqamZhMypqZl87rnFtsutp8Pqh2Ic9kFS+TRzs6f87ne0zy3A9zLzloi4CbgoM29e\nsczrgU2Z+XJEbAYeAg5m5kMd1plr1TSqI2XGbcSSpPIoYrTPLcCuiHgaeDfwqWYhb46IB5rLbAEe\niYgngEeB+zsF/zhzxJKkMlm357+WzPwe8I/bzH8B+KXm6wXg0n62Mw4csSSpTBwuMySOWJJUJibP\nkBw6tJedO6c59wGwNGJpb2E1Saouw39IduzYxpEjNzI1dSsAU1O3buhiryQNgl/jWIBx2AdJ5eHX\nOEqSumL4S1IFGf6SVEGGvyRVkOEvSRVk+I8Qv8pS0qA41LMAG6lhkA+GkzReHOo5xnwwnKRBMvxH\nhA+GkzRIhv+I8MFwkgbJ5BgRPhhO0iAZ/iPCB8NJGiRH+xRgHPZBUnk42keS1BXDX5IqyPCXpAoy\n/Idlbg7q9eXz6vXGfEkaMsN/WCYnYf/+cx8A9XpjenKy2LokVZKjfYapGfhx5x3kR6+H2VmYmOh5\nNY72kdRq6KN9IuIDEfHtiHgtIi5bY7ndEfFURDwTETf1s82RNjEB+/Y1Xu/bt6Hgl6RB6Lft8yTw\nK8D/7LRARGwCbgeuAi4BPhwRb+9zu6OpXofDh8mFRTh8ePU1AEkakr7CPzOfzsxngbX+3LgceDYz\nT2bmq8BXgPf2s92RtNTjn52F7dsb/2y9BiBJQzSMC75bgedbpr/bnNe9cRgpc/To8h7/xERj+ujR\nYuuSVEkXrLdARBwBtrTOAhLYn5n3n4+iZmZmzr6u1WrUlkbKzM4CE8vPokfFNdesnjcx0X6+JK1h\nfn6e+fn5vtYxkNE+EfEN4OOZ+Xib964AZjJzd3P6ZiAz85YO62o/2seRMmeNwz5IGpyin+3TacOP\nAW+LiG0RcSHwIeC+ntfuSBlJGph+h3q+LyKeB64AHoiIB5vz3xwRDwBk5mvADcBDwHeAr2Tm8Z43\n5kgZSRqY0bjJq7XHPzGxerqn9Y9+y2Qc9kHS4Gyk7TMa4T8313gMQmvQ1+uNkTI9XjAdh+Ach32Q\nNDjjG/4DXf/oB+c47IOkwSn6gm+pLSycZM+egwDs2XOQhYWTBVckScWpxJn/wsJJdu26jRMnDgKb\nWfry81H9DlzP/CW18sy/gwMH7mkJfoDNnDhxkAMH7imwKkkqTiXC/9SpM5wL/iWbOX36TBHlSFLh\nKhH+W7duAl5ZMfcVLr64ErsvSatUIv0OHdrLzp3TnPsAaPT8Dx3aW1hNklSkSoT/jh3bOHLkRqam\nbgVgaurWkb3YK0mDUInRPsvXP/ojZcZhHyQNjqN9JEldMfwlqYIMf0mqIMNfkirI8JekCjL8JamC\nDH9JqiDDX5IqyPCXpAoy/CWpggx/Saogw1+SKsjwl6QK6iv8I+IDEfHtiHgtIi5bY7nFiPjTiHgi\nIv6kn21Kkvp3QZ+//yTwK8DvrLPcGaCWmd/vc3uSpAHoK/wz82mAiFjvOdKBLSZJKo1hBXICRyLi\nsYi4dkjblCR1sO6Zf0QcAba0zqIR5vsz8/4utzOZmS9ExE/Q+BA4npmPdFp4Zmbm7OtarUatVuty\nM5I0/ubn55mfn+9rHQP5GseI+Abw8cx8vItlp4G/yszf7vC+X+O4jnHYB0mDU/TXOLbdcES8PiLe\n0Hy9GfgF4NsD3K4kqUf9DvV8X0Q8D1wBPBARDzbnvzkiHmgutgV4JCKeAB4F7s/Mh/rZriSpPwNp\n+wySbZ/1jcM+SBqcots+Op/m5qBeXz6vXm/Ml6QeGf6jYnIS9u8/9wFQrzemJyeLrUvSSLLtM0qa\ngR933kF+9HqYnYWJiaKrklSwjbR9DP9Rs7hI7NhOLizC9u3F1iKpFOz5j7t6HQ4fbgT/4cOrrwFI\nUpcM/1Gx1OOfnW2c8c/OLr8GIEk9sO0zKubmGhd3W3v89TocPQrXXFNcXZIKZ8+/q/WPaPhLUgf2\n/CVJXTH8JamCDH9JqiDDX5IqyPCXpAoy/CWpggx/SaqgaoS/j0OWpGWqEf4+DlmSlqnOHb4+DlnS\nmPLxDuvxcciSxpCPd1iLj0OWpLOqEf4+DlmSlqlG28fHIUsaY0Pv+UfEbwG/DPwQOAF8JDP/ss1y\nu4H/TOMvjbsz85Y11nleH+ksSeOmiJ7/Q8AlmXkp8CzwG22K2gTcDlwFXAJ8OCLevtENzs/Pb/RX\nh6bsNZa9PrDGQbHGwRiFGnvVV/hn5sOZeaY5+SjwljaLXQ48m5knM/NV4CvAeze6zVH4l1D2Gste\nH1jjoFjjYIxCjb0a5AXffwU82Gb+VuD5lunvNudJkgpywXoLRMQRYEvrLCCB/Zl5f3OZ/cCrmfnl\n81KlJGmg+h7tExF7gWuBn8/MH7Z5/wpgJjN3N6dvBrLTRd+I8GqvJPWo1wu+6575r6U5imcf8I/a\nBX/TY8DbImIb8ALwIeDDndbZ6w5IknrXb8//NuANwJGIeDwi7gSIiDdHxAMAmfkacAONkUHfAb6S\nmcf73K4kqQ+lu8lLknT+jczjHSJid0Q8FRHPRMRNRdfTTkQsRsSfRsQTEfEnRdcDEBF3R8RLEfGt\nlnkXRcRDEfF0RPxhRPzNEtY4HRHfbf5F+XizxVhUfW+JiK9HxHci4smI+FhzfmmOY5sab2zOL9Nx\nfF1E/HHz/48nI2K6Ob9Mx7FTjaU5js16NjXruK853fMxHIkz/+aNYs8A7wZO07iO8KHMfKrQwlaI\niOeAd2bm94uuZUlE/EPgZeCLmfkzzXm3AP8vM3+r+UF6UWbeXLIap4G/yszfLqquJRHxJuBNmXks\nIt4AfJPGvSofoSTHcY0aP0hJjiNARLw+M38QET8CHAU+BryfkhzHNWq8mnIdx38DvBP40cx8z0b+\nnx6VM/+B3ih2HgUlO6aZ+Qiw8sPovcAXmq+/ALxvqEWt0KFGaBzPwmXmi5l5rPn6ZeA4jRsaS3Mc\nO9S4dD9NKY4jQGb+oPnydTQGnCQlOo7QsUYoyXGMiLcAvwh8rmV2z8ewVEG1hlG5USxpXPx+LCKu\nLbqYNbwxM1+CRmgAbyy4nk5uiIhjEfG5oltTSyJiO3ApjTvat5TxOLbU+MfNWaU5js12xRPAi8CR\nzHyMkh3HDjVCeY7jf6IxyrK1bdPzMRyV8B8Vk5l5GY1P5eub7YxRUMbe353ATzafG/UiUPif2812\nyn8D/nXz7HrlcSv8OLapsVTHMTPPZObP0vjL6fKIuISSHcc2Nb6DkhzHiLgGeKn5V95af4msewxH\nJfxPAW9tmX5Lc16pZOYLzX/+OfDfabSryuiliNgCZ3vF/7fgelbJzD9vebzr7wJ/r8h6IuICGqH6\n+5n5tebsUh3HdjWW7TguaT79dx7YTcmO45LWGkt0HCeB9zSvL/4X4Ocj4veBF3s9hqMS/mdvFIuI\nC2ncKHZfwTUtExGvb551ERGbgV8Avl1sVWcFy88S7gP2Nl//S+BrK3+hAMtqbP4HvOSfUPyx/Dzw\nvzPz0y3zynYcV9VYpuMYEX9rqV0SEX8D2EXj2kRpjmOHGp8qy3HMzH+fmW/NzJ+kkYNfz8x/DtxP\nr8cwM0fih8YZwtM0Hh19c9H1tKlvB3AMeAJ4siw1Al+mMULqh8Cf0RihchHwcPN4PgRMlLDGLwLf\nah7Tr9LoaRZV3yTwWsu/38eb/z3+WFmO4xo1luk4/t1mXceaNe1vzi/TcexUY2mOY0utVwL3bfQY\njsRQT0nSYI1K20eSNECGvyRVkOEvSRVk+EtSBRn+klRBhr8kVZDhL0kVZPhLUgX9f61m47TFMbbs\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d404d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.zeros((N,1))\n",
    "# Construct the problem.\n",
    "w = Variable(K+Ke)\n",
    "objective = Minimize(25.5*norm(w, 1) -y.T*X*w + sum_entries(log_sum_exp(hstack(z, X*w),axis=1)))\n",
    "#constraints = [0 <= x, x <= 10]\n",
    "#prob = Problem(objective, constraints)\n",
    "prob = Problem(objective)\n",
    "\n",
    "# The optimal objective is returned by prob.solve().\n",
    "result = prob.solve()\n",
    "# The optimal value for x is stored in x.value.\n",
    "print(w.value)\n",
    "# The optimal Lagrange multiplier for a constraint\n",
    "# is stored in constraint.dual_value.\n",
    "#print(constraints[0].dual_value)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.stem(w.value)\n",
    "plt.stem(w_true,markerfmt='xr')\n",
    "\n",
    "plt.gca().set_xlim((-1, K+Ke))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"stdin_port\": 52252, \n",
      "  \"ip\": \"127.0.0.1\", \n",
      "  \"control_port\": 52253, \n",
      "  \"hb_port\": 52254, \n",
      "  \"signature_scheme\": \"hmac-sha256\", \n",
      "  \"key\": \"ea79abbc-0894-4390-90e4-df49b34619e0\", \n",
      "  \"shell_port\": 52250, \n",
      "  \"transport\": \"tcp\", \n",
      "  \"iopub_port\": 52251\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> ipython <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> ipython <app> --existing /Users/cemgil/Library/Jupyter/runtime/kernel-0e1f423d-e753-4e58-a8b9-87b5530794df.json \n",
      "or even just:\n",
      "    $> ipython <app> --existing \n",
      "if this is the most recent IPython session you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Expression(CONVEX, POSITIVE, (1, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(w,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
