{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a binary classification method. The key idea is learning a mapping from a feature vector to a probability, a number between $0$ and $1$. It is similar to least-squares in the sense that (apart from some extreme cases) it has a unique solution.\n",
    "\n",
    "Suppose, for a set of objects $X$, each denoted by the feature vector $x_i \\in \\mathbb{R}^D$, we are given the answer to some true-false question, such as 'is object $i$ of class $c$?'. This answer is denoted by $y_i \\in \\{0, 1\\}$. We are given a dataset of feature vectors $x_i$ along with the corresponding 'labels' $y_i$. For $i=1\\dots N$\n",
    "\n",
    "$$(y_i, x_i)$$\n",
    "\n",
    "The model is \n",
    "$$\n",
    "\\Pr\\{y_i = 1\\} = \\sigma(x_i^\\top w)\n",
    "$$\n",
    "Here,\n",
    "$\\sigma(x)$ is the sigmoid function defined as\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{1}{1+e^{-x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "This is a generative model. To understand logistic regression as a generative model, consider the following metaphor: assume that for each data instance $x_i$, we select a biased coin with probability $p(y_i = 1| w, x) = \\pi_i = \\sigma(w^\\top x_i)$, throw the coin and label the data item with class $y_i$ accordingly. \n",
    "\n",
    "\n",
    "Mathematically, we assume that each label $y_i$ is drawn from a Bernoulli distribution. That is: \n",
    "\\begin{eqnarray}\n",
    "\\pi_i & = & \\sigma(x_i^\\top w) \\\\\n",
    "y_i & \\sim &\\mathcal{BE}(\\pi)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here, we think of a biased coin with two sides denoted as $1$ and $0$ with probability of side $1$ as $\\pi$, and consequently the probability of side $0$ with $1-\\pi$. We denote the outcome of the coin toss with the random variable $y$. We write the probability as $p(y = 1) = \\pi$ and probability of heads is $p(y = 0) = 1-\\pi$. More compactly, the probability of the outcome of a toss, provided we know $\\pi$, is written as\n",
    "\\begin{eqnarray}\n",
    "p(y|\\pi) = \\pi^y(1-\\pi)^y\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we are given a dataset of form\n",
    "\\begin{eqnarray}\n",
    "X & = &  \\begin{pmatrix}\n",
    "  x_{1,1} & x_{1,2} & \\dots & x_{1,D} \\\\\n",
    "  x_{2,1} & x_{2,2} & \\dots & x_{2,D} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots  \\\\\n",
    "  x_{i,1} & x_{i,2} & \\dots & x_{i,D}  \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots  \\\\\n",
    "  x_{N,1} & x_{N,2} & \\dots & x_{N,D} \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "x_1^\\top \\\\\n",
    "x_2^\\top \\\\\n",
    "\\dots \\\\\n",
    "x_i^\\top \\\\\n",
    "\\dots \\\\\n",
    "x_N^\\top\n",
    "\\end{pmatrix} \n",
    "\\\\\n",
    "{y} & = & \\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_i \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "where $x_{i,j}$ denotes the $j$'th feature of the $i$'th data point. It is customary, to set a column entirely to $1$, for example $x_{i,D}=1$ for all $i$. This 'feature' is artificially added to the dataset to allow a slightly more flexible model. The $y_i$ denote the target class label of the\n",
    "$i$'th object. In logistic regression, we consider the case of binary classification where $y_i \\in \\{0,1\\}$. It is possible to use other encodings such as $y_i \\in \\{-1,1\\}$; the derivations are similar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the sigmoid function\n",
    "Note that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{e^x}{(1+e^{-x})e^x} = \\frac{e^x}{1+e^{x}} \\\\\n",
    "1 - \\sigma(x) & = & 1 - \\frac{e^x}{1+e^{x}} = \\frac{1+e^{x} - e^x}{1+e^{x}} = \\frac{1}{1+e^{x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma'(x) & = & \\frac{e^x(1+e^{x}) - e^{x} e^x}{(1+e^{x})^2} = \\frac{e^x}{1+e^{x}}\\frac{1}{1+e^{x}} = \\sigma(x) (1-\\sigma(x))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log \\sigma(x) & = & -\\log(1+e^{-x}) = x - \\log(1+e^{x}) \\\\\n",
    "\\log(1 - \\sigma(x)) & = &  -\\log({1+e^{x}})\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Exercise: Plot the sigmoid function and its derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the parameters\n",
    "\n",
    "The likelihood of the observations, that is the probability of observing the class sequence is\n",
    "\\begin{eqnarray}\n",
    "p(y_1, y_2, \\dots, y_N|w, x_1, x_2, \\dots, x_N ) &=& \\left(\\prod_{i : y_i=1} \\sigma(w^\\top x_i) \\right) \\left(\\prod_{i : y_i=0}(1- \\sigma(w^\\top x_i)) \\right)\n",
    "\\end{eqnarray}\n",
    "Here, the left product is the expression for examples from class $1$ and the right product is for examples from class $0$.\n",
    "We will look for the particular setting of the weight vector, the so called maximum likelihood solution, denoted by $w^*$.\n",
    "\\begin{eqnarray}\n",
    "w^* & = & \\arg\\max_{w} {\\cal L}(w)\n",
    "\\end{eqnarray}\n",
    "where the loglikelihood function\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(w) & = & \\log p(y_1, y_2, \\dots, y_N|w, x_1, x_2, \\dots, x_N ) \\\\\n",
    "& = & \\sum_{i : y_i=1} \\log \\sigma(w^\\top x_i) + \\sum_{i : y_i=0} \\log (1- \\sigma(w^\\top x_i)) \\\\\n",
    "& = & \\sum_{i : y_i=1} w^\\top x_i - \\sum_{i : y_i=1} \\log(1+e^{w^\\top x_i}) - \\sum_{i : y_i=0}\\log({1+e^{w^\\top x_i}}) \\\\\n",
    "& = & \\sum_i y_i w^\\top x_i - \\sum_{i} \\log(1+e^{w^\\top x_i}) \\\\\n",
    "& = & y^\\top X w - \\mathbf{1}^\\top logsumexp(0, X w)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Unlike the least-squares problem, an expression for direct evaluation of $w^*$ is not known so we need to resort to numerical optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization via gradient ascent\n",
    "\n",
    "One way for\n",
    "optimization is gradient ascent\n",
    "\\begin{eqnarray}\n",
    "w^{(\\tau)} & \\leftarrow & w^{(\\tau-1)} + \\eta \\nabla_w {\\cal L}\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\nabla_w {\\cal L} & = &\n",
    "\\begin{pmatrix}\n",
    "{\\partial {\\cal L}}/{\\partial w_1} \\\\\n",
    "{\\partial {\\cal L}}/{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "{\\partial {\\cal L}}/{\\partial w_{D}}\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "is the gradient vector.\n",
    "\n",
    "#### Evaluating the gradient\n",
    "The partial derivative of the loglikelihood with respect to the $k$'th entry of the weight vector is given by the chain rule as\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\frac{\\partial{\\cal L}}{\\partial \\sigma(u)} \\frac{\\partial \\sigma(u)}{\\partial u} \\frac{\\partial u}{\\partial w_k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(w) & = & \\sum_{i : y_i=1} \\log \\sigma(w^\\top x_i) + \\sum_{i : y_i=0} \\log (1- \\sigma(w^\\top x_i))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}(\\sigma)}{\\partial \\sigma} & = &  \\sum_{i : y_i=1} \\frac{1}{\\sigma(w^\\top x_i)} - \\sum_{i : y_i=0} \\frac{1}{1- \\sigma(w^\\top x_i)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\sigma(u)}{\\partial u} & = & \\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial w^\\top x_i }{\\partial w_k} & = & x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "So the gradient is\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\sum_{i : y_i=1} \\frac{\\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))}{\\sigma(w^\\top x_i)} x_{i,k} - \\sum_{i : y_i=0} \\frac{\\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))}{1- \\sigma(w^\\top x_i)} x_{i,k} \\\\\n",
    "& = & \\sum_{i : y_i=1} {(1-\\sigma(w^\\top x_i))} x_{i,k} - \\sum_{i : y_i=0} {\\sigma(w^\\top x_i)} x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can write this expression more compactly by noting\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\sum_{i : y_i=1} {(\\underbrace{1}_{y_i}-\\sigma(w^\\top x_i))} x_{i,k} + \\sum_{i : y_i=0} {(\\underbrace{0}_{y_i} - \\sigma(w^\\top x_i))} x_{i,k} \\\\\n",
    "& = & \\sum_i (y_i - \\sigma(w^\\top x_i)) x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The update rule is\n",
    "\\begin{eqnarray}\n",
    "w^{(\\tau)} = w^{(\\tau-1)} + \\eta X^\\top (y-\\sigma(X w))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cemgil/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from cvxpy import *\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[-2.  1.  1.]\n",
      " [-1.  2.  1.]\n",
      " [ 1.  5.  1.]\n",
      " [-1.  1.  1.]\n",
      " [-3. -2.  1.]\n",
      " [ 1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.matrix('[-2,1; -1,2; 1,5; -1,1; -3,-2; 1,1] ')\n",
    "y = np.matrix('[0,0,1,0,0,1]').T\n",
    "\n",
    "\n",
    "N = x.shape[0]\n",
    "#A = np.hstack((np.power(x,0), np.power(x,1), np.power(x,2)))\n",
    "X = np.hstack((x, np.ones((N,1)) ))\n",
    "\n",
    "K = X.shape[1]\n",
    "Ke = 0\n",
    "\n",
    "z = np.zeros((N,1))\n",
    "\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "K = 10\n",
    "Ke = 40-K\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x = np.matrix(np.random.randn(N, K))\n",
    "w_true = np.random.randn(K,1)\n",
    "\n",
    "p = sigmoid(x*w_true)\n",
    "u = np.random.rand(N,1)\n",
    "y = (u < p)\n",
    "y = y.astype(np.float64)\n",
    "\n",
    "#A = np.hstack((np.power(x,0), np.power(x,1), np.power(x,2)))\n",
    "X = np.hstack((x, np.random.randn(N, Ke )))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.40983335e-02]\n",
      " [ -3.99878834e-11]\n",
      " [ -4.89025643e-11]\n",
      " [  2.80208513e-12]\n",
      " [  8.99384909e-02]\n",
      " [  2.48151990e-01]\n",
      " [  1.00411325e+00]\n",
      " [ -6.49613096e-02]\n",
      " [  1.14700040e+00]\n",
      " [ -6.10505750e-01]\n",
      " [  1.09795546e-10]\n",
      " [ -1.01054452e-11]\n",
      " [  6.10728067e-11]\n",
      " [ -2.78667789e-11]\n",
      " [  3.11769935e-11]\n",
      " [  1.54248866e-12]\n",
      " [ -1.64127375e-10]\n",
      " [  6.07470106e-11]\n",
      " [ -7.33071236e-11]\n",
      " [ -2.65350325e-12]\n",
      " [  6.54192363e-11]\n",
      " [ -3.76599877e-10]\n",
      " [  1.60127872e-11]\n",
      " [  1.21984759e-10]\n",
      " [ -3.28280038e-11]\n",
      " [ -5.44375293e-12]\n",
      " [ -2.35710693e-11]\n",
      " [ -1.26861576e-11]\n",
      " [  1.26534640e-11]\n",
      " [ -5.25187409e-11]\n",
      " [ -1.33941329e-11]\n",
      " [ -3.14596819e-09]\n",
      " [ -4.26032415e-10]\n",
      " [  3.51397512e-11]\n",
      " [ -1.38935273e-10]\n",
      " [ -9.18761500e-13]\n",
      " [ -6.34084551e-11]\n",
      " [ -1.41931589e-10]\n",
      " [  4.54740315e-12]\n",
      " [  1.54700892e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114832210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.zeros((N,1))\n",
    "# Construct the problem.\n",
    "w = Variable(K+Ke)\n",
    "objective = Minimize(25.5*norm(w, 1) -y.T*X*w + sum_entries(log_sum_exp(hstack(z, X*w),axis=1)))\n",
    "#constraints = [0 <= x, x <= 10]\n",
    "#prob = Problem(objective, constraints)\n",
    "prob = Problem(objective)\n",
    "\n",
    "# The optimal objective is returned by prob.solve().\n",
    "result = prob.solve()\n",
    "# The optimal value for x is stored in x.value.\n",
    "print(w.value)\n",
    "# The optimal Lagrange multiplier for a constraint\n",
    "# is stored in constraint.dual_value.\n",
    "#print(constraints[0].dual_value)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.stem(w.value)\n",
    "plt.stem(w_true,markerfmt='xr')\n",
    "\n",
    "plt.gca().set_xlim((-1, K+Ke))\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
