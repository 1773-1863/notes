{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Consider a coin with probability of tails $\\pi$ and the probability of heads $1-\\pi$. We denote the outcome of the coin toss as $y$. Denoting the tail with a $1$ and head with $0$, we write the probability of tails with $p(y = 1) = \\pi$ and probability of heads is $p(y = 0) = 1-\\pi$. More compactly, the probability of the outcome of a toss, provided we know $\\pi$ is written as\n",
    "\\begin{eqnarray}\n",
    "p(y|\\pi) = \\pi^y(1-\\pi)^y\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we are given a dataset of form\n",
    "\\begin{eqnarray}\n",
    "X & = &  \\begin{pmatrix}\n",
    "  x_{1,1} & x_{1,2} & \\dots & x_{1,D} & x_{1,D+1}\\\\\n",
    "  x_{2,1} & x_{2,2} & \\dots & x_{2,D} & x_{2,D+1}\\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots  &  \\vdots\\\\\n",
    "  x_{i,1} & x_{i,2} & \\dots & x_{i,D}  & x_{i,D+1}\\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "  x_{N,1} & x_{N,2} & \\dots & x_{N,D} & x_{N,D+1}\\\\\n",
    "\\end{pmatrix} \\\\\n",
    "\\mathbf{y} & = & \\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_i \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "where $x_{i,j}$ denotes the $j$'th feature of the $i$'th data point. The $D+1$ column, where $x_{i,D+1}=1$ for all $i$, is artificially added to the dataset to allow for a bias. The $y_i$ denote the target class label of the\n",
    "$i$'th object. In logistic regression, we consider the case of binary classification where $y_i \\in \\{0,1\\}$ (or $y_i \\in \\{-1,1\\}$).\n",
    "\n",
    "To understand logistic regression, consider the following metaphor: assume that for each data instance $x_i$, we select a biased coin with probability $p(y_i = 1| w, x) = \\pi_i = \\sigma(w^\\top x_i)$, throw the coin and label the data item with class $y_i$ accordingly. Here,\n",
    "$\\sigma(x)$ is the sigmoid function defined as\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{1}{1+e^{-x}}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the sigmoid function\n",
    "Note that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) & = & \\frac{e^x}{(1+e^{-x})e^x} = \\frac{e^x}{1+e^{x}} \\\\\n",
    "1 - \\sigma(x) & = & 1 - \\frac{e^x}{1+e^{x}} = \\frac{1+e^{x} - e^x}{1+e^{x}} = \\frac{1}{1+e^{x}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sigma'(x) & = & \\frac{e^x(1+e^{x}) - e^{x} e^x}{(1+e^{x})^2} = \\frac{e^x}{1+e^{x}}\\frac{1}{1+e^{x}} = \\sigma(x) (1-\\sigma(x))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log \\sigma(x) & = & -\\log(1+e^{-x}) = x - \\log(1+e^{x}) \\\\\n",
    "\\log(1 - \\sigma(x)) & = &  -\\log({1+e^{x}})\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Exercise: Plot the sigmoid function and its derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of the observations, that is the probability of observing the class sequence is\n",
    "\\begin{eqnarray}\n",
    "p(y_1, y_2, \\dots, y_N|w, x_1, x_2, \\dots, x_N ) &=& \\left(\\prod_{i : y_i=1} \\sigma(w^\\top x_i) \\right) \\left(\\prod_{i : y_i=0}(1- \\sigma(w^\\top x_i)) \\right)\n",
    "\\end{eqnarray}\n",
    "Here, the left product is the expression for examples from class $1$ and the right product is for examples from class $0$.\n",
    "We will look for the particular setting of the weight vector, the so called maximum likelihood solution, denoted by $w^*$.\n",
    "\\begin{eqnarray}\n",
    "w^* & = & \\arg\\max_{w} {\\cal L}(w)\n",
    "\\end{eqnarray}\n",
    "where the loglikelihood function\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(w) & = & \\log p(y_1, y_2, \\dots, y_N|w, x_1, x_2, \\dots, x_N ) \\\\\n",
    "& = & \\sum_{i : y_i=1} \\log \\sigma(w^\\top x_i) + \\sum_{i : y_i=0} \\log (1- \\sigma(w^\\top x_i)) \\\\\n",
    "& = & \\sum_{i : y_i=1} w^\\top x_i - \\sum_{i : y_i=1} \\log(1+e^{w^\\top x_i}) - \\sum_{i : y_i=0}\\log({1+e^{w^\\top x_i}}) \\\\\n",
    "& = & \\sum_i y_i w^\\top x_i - \\sum_{i} \\log(1+e^{w^\\top x_i}) \\\\\n",
    "& = & y^\\top X w - \\mathbf{1}^\\top logsumexp(0, X w)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Unlike the least-squares problem, an expression for direct evaluation of $w^*$ is not known so we need to resort to numerical optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for\n",
    "optimization is gradient ascent\n",
    "\\begin{eqnarray}\n",
    "w^{(\\tau)} & \\leftarrow & w^{(\\tau-1)} + \\eta \\nabla_w {\\cal L}\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\nabla_w {\\cal L} & = &\n",
    "\\begin{pmatrix}\n",
    "{\\partial {\\cal L}}/{\\partial w_1} \\\\\n",
    "{\\partial {\\cal L}}/{\\partial w_2} \\\\\n",
    "\\vdots \\\\\n",
    "{\\partial {\\cal L}}/{\\partial w_{D+1}}\n",
    "\\end{pmatrix}\n",
    "\\end{eqnarray}\n",
    "is the gradient vector.\n",
    "\n",
    "\\subsection{Evaluating the gradient}\n",
    "The partial derivative of the loglikelihood with respect to the $k$'th entry of the weight vector is given by the chain rule as\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\frac{\\partial{\\cal L}}{\\partial \\sigma(u)} \\frac{\\partial \\sigma(u)}{\\partial u} \\frac{\\partial u}{\\partial w_k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(w) & = & \\sum_{i : y_i=1} \\log \\sigma(w^\\top x_i) + \\sum_{i : y_i=0} \\log (1- \\sigma(w^\\top x_i))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}(\\sigma)}{\\partial \\sigma} & = &  \\sum_{i : y_i=1} \\frac{1}{\\sigma(w^\\top x_i)} - \\sum_{i : y_i=0} \\frac{1}{1- \\sigma(w^\\top x_i)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\sigma(u)}{\\partial u} & = & \\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial w^\\top x_i }{\\partial w_k} & = & x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "So the gradient is\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\sum_{i : y_i=1} \\frac{\\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))}{\\sigma(w^\\top x_i)} x_{i,k} - \\sum_{i : y_i=0} \\frac{\\sigma(w^\\top x_i) (1-\\sigma(w^\\top x_i))}{1- \\sigma(w^\\top x_i)} x_{i,k} \\\\\n",
    "& = & \\sum_{i : y_i=1} {(1-\\sigma(w^\\top x_i))} x_{i,k} - \\sum_{i : y_i=0} {\\sigma(w^\\top x_i)} x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can write this expression more compactly by noting\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial{\\cal L}}{\\partial w_k} & = & \\sum_{i : y_i=1} {(\\underbrace{1}_{y_i}-\\sigma(w^\\top x_i))} x_{i,k} + \\sum_{i : y_i=0} {(\\underbrace{0}_{y_i} - \\sigma(w^\\top x_i))} x_{i,k} \\\\\n",
    "& = & \\sum_i (y_i - \\sigma(w^\\top x_i)) x_{i,k}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The update rule is\n",
    "\\begin{eqnarray}\n",
    "w^{(\\tau)} = w^{(\\tau-1)} + \\eta X^\\top (y-\\sigma(X w))\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cemgil/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from cvxpy import *\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[-2.  1.  1.]\n",
      " [-1.  2.  1.]\n",
      " [ 1.  5.  1.]\n",
      " [-1.  1.  1.]\n",
      " [-3. -2.  1.]\n",
      " [ 1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.matrix('[-2,1; -1,2; 1,5; -1,1; -3,-2; 1,1] ')\n",
    "y = np.matrix('[0,0,1,0,0,1]').T\n",
    "\n",
    "\n",
    "N = x.shape[0]\n",
    "#A = np.hstack((np.power(x,0), np.power(x,1), np.power(x,2)))\n",
    "X = np.hstack((x, np.ones((N,1)) ))\n",
    "\n",
    "K = X.shape[1]\n",
    "z = np.zeros((N,1))\n",
    "\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.35076889e+00]\n",
      " [  1.68577115e-09]\n",
      " [ -1.61815400e-10]]\n"
     ]
    }
   ],
   "source": [
    "# Construct the problem.\n",
    "w = Variable(K)\n",
    "objective = Minimize(norm(w, 1) -y.T*X*w + sum_entries(log_sum_exp(hstack(z, X*w),axis=1)))\n",
    "#constraints = [0 <= x, x <= 10]\n",
    "#prob = Problem(objective, constraints)\n",
    "prob = Problem(objective)\n",
    "\n",
    "# The optimal objective is returned by prob.solve().\n",
    "result = prob.solve()\n",
    "# The optimal value for x is stored in x.value.\n",
    "print(w.value)\n",
    "# The optimal Lagrange multiplier for a constraint\n",
    "# is stored in constraint.dual_value.\n",
    "#print(constraints[0].dual_value)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Expression(CONVEX, UNKNOWN, (1, 1))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "log_sum_exp(w) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
